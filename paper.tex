% compile with knitr::knit2pdf("paper.rnw")

\documentclass[bimj,fleqn]{w-art}\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{times}
\usepackage{w-thm}
\usepackage[authoryear, round]{natbib}
\usepackage{booktabs}
\setlength{\bibsep}{2pt}
\setlength{\bibhang}{2em}
\newcommand{\J}{J\"{o}reskog}
\newcommand{\So}{S\"{o}rbom}
\newcommand{\bcx}{{\bf X}}
\newcommand{\bcy}{{\bf Y}}
\newcommand{\bcz}{{\bf Z}}
\newcommand{\bcu}{{\bf U}}
\newcommand{\bcv}{{\bf V}}
\newcommand{\bcw}{{\bf W}}
\newcommand{\bci}{{\bf I}}
\newcommand{\bch}{{\bf H}}
\newcommand{\bcb}{{\bf B}}
\newcommand{\bcr}{{\bf R}}
\newcommand{\bcm}{{\bf M}}
\newcommand{\bcp}{{\bf P}}
\newcommand{\bcf}{{\bf F}}
\newcommand{\bcg}{{\bf G}}
\newcommand{\bcs}{{\bf S}}
\newcommand{\bct}{{\bf T}}
\newcommand{\bca}{{\bf A}}
\newcommand{\bcd}{{\bf D}}
\newcommand{\bcc}{{\bf C}}
\newcommand{\bce}{{\bf E}}
\newcommand{\ba}{{\bf a}}
\newcommand{\bb}{{\bf b}}
\newcommand{\bc}{{\bf c}}
\newcommand{\bd}{{\bf d}}
\newcommand{\bx}{\mbox{\boldmath x}}
\newcommand{\by}{\mbox{\boldmath y}}
\newcommand{\bz}{{\bf z}}
\newcommand{\bu}{{\bf u}}
\newcommand{\bv}{{\bf v}}
\newcommand{\bh}{{\bf h}}
\newcommand{\bl}{{\bf l}}
\newcommand{\be}{{\bf e}}
\newcommand{\br}{{\bf r}}
\newcommand{\bw}{{\bf w}}
\newcommand{\de}{\stackrel{D}{=}}
\newcommand{\bt}{\bigtriangleup}
\newcommand{\bfequiv}{\mbox{\boldmath $\equiv$}}
\newcommand{\bmu}{\mbox{\boldmath $\mu$}}
\newcommand{\bnu}{\mbox{\boldmath $\nu$}}
\newcommand{\bxi}{\mbox{\boldmath $\xi$}}
\newcommand{\btau}{\mbox{\boldmath $\tau$}}
\newcommand{\bgamma}{\mbox{\boldmath $\Gamma$}}
\newcommand{\bphi}{\mbox{\boldmath $\Phi$}}
\newcommand{\bfphi}{\mbox{\boldmath $\varphi$}}
\newcommand{\bfeta}{\mbox{\boldmath $\eta$}}
\newcommand{\bpi}{\mbox{\boldmath $\Pi$}}
\newcommand{\bequiv}{\mbox{\boldmath $\equiv$}}
\newcommand{\bvarepsilon}{\mbox{\boldmath $\varepsilon$}}
\newcommand{\btriangle}{\mbox{\boldmath $\triangle$}}
\newcommand{\bdelta}{\mbox{\boldmath $\Delta$}}
\newcommand{\beps}{\mbox{\boldmath $\epsilon$}}
\newcommand{\btheta}{\mbox{\boldmath $\theta$}}
\newcommand{\balpha}{\mbox{\boldmath $\alpha$}}
\newcommand{\bfbeta}{\mbox{\boldmath $\beta$}}
\newcommand{\bsphi}{\mbox{\boldmath $\varphi$}}
\newcommand{\bsig}{\mbox{\boldmath $\sigma$}}
\newcommand{\bfpsi}{\mbox{\boldmath $\psi$}}
\newcommand{\bfdelta}{\mbox{\boldmath $\delta$}}
\newcommand{\bsigma}{{\bf \Sigma}}
\newcommand{\bzero}{{\bf 0}}
\newcommand{\bone}{{\bf 1}}
\newcommand{\bpsi}{\mbox{\boldmath $\Psi$}}
\newcommand{\bep}{\mbox{\boldmath $\epsilon$}}
\newcommand{\bomega}{\mbox{\boldmath $\Omega$}}
\newcommand{\bfomega}{\mbox{\boldmath $\omega$}}
\newcommand{\blambda}{\mbox{\boldmath $\Lambda$}}
\newcommand{\bflambda}{\mbox{\boldmath $\lambda$}}
\newcommand{\bfsigma}{\mbox{\boldmath $\sigma$}}
\newcommand{\bfpi}{{\mbox{\boldmath $\pi$}}}
\newcommand{\bupsilon}{\mbox{\boldmath $\upsilon$}}
\newcommand{\obs}{{\rm obs}}
\newcommand{\mis}{{\rm mis}}
\theoremstyle{plain}
\newtheorem{criterion}{Criterion}
\theoremstyle{definition}
\newtheorem{condition}[theorem]{Condition}
\usepackage[]{graphicx}
\chardef\bslash=`\\ % p. 424, TeXbook
\newcommand{\ntt}{\normalfont\ttfamily}
\newcommand{\cn}[1]{{\protect\ntt\bslash#1}}
\newcommand{\pkg}[1]{{\protect\ntt#1}}
\let\fn\pkg
\let\env\pkg
\let\opt\pkg
\hfuzz1pc % Don't bother to report overfull boxes if overage is < 1pc
\newcommand{\envert}[1]{\left\lvert#1\right\rvert}
\let\abs=\envert



% Linebreaks in xtable
\usepackage{makecell}
% \usepackage[utf8x]{inputenc}
\usepackage{bm}
\usepackage{amsthm,amsmath,amssymb,amsfonts,amssymb}
\usepackage{listings}
% \usepackage{booktabs}
\usepackage{enumerate}
% \usepackage{subfigure}
% \usepackage{caption}
% \usepackage{subcaption}
% \usepackage{capt-of}
\usepackage{enumitem}
\setlist[enumerate,1]{label={(\roman*)}}
\usepackage[figuresright]{rotating}
% \usepackage{lmodern}
% \usepackage[a4paper, left=1in,right=1in,top=1.5in,bottom=1.5in]{geometry}
% \usepackage{babel}
% \usepackage{setspace}
% \usepackage[draft]{fixme}
\usepackage{mathtools}
%\usepackage[nolists,heads,tablesfirst]{endfloat}
% \usepackage{hyphenat}
% \usepackage[section]{placeins}
% \usepackage{xcolor}
\usepackage[dvipsnames]{xcolor}
\usepackage{xurl}
\usepackage{hyperref}
\hypersetup{
colorlinks,
citecolor=black,
filecolor=black,
linkcolor=black,
urlcolor=black
}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\def\subsubsectionautorefname{Section}

\definecolor{backgroundColour}{rgb}{0.97,0.96,0.97}
\lstset {
  language={C++},
  numbers=none,
  frame=none,
  includerangemarker=false,
  basicstyle=\small\ttfamily, % basic font setting
  backgroundcolor=\color{backgroundColour},
% https://tex.stackexchange.com/questions/68091/how-do-i-add-syntax-coloring-to-my-c-source-code-in-beamer
  keywordstyle=\color{BrickRed}\textbf,
  emph={DATA_VECTOR, DATA_INTEGER, PARAMETER, PARAMETER_VECTOR, sum, exp, dnorm, Type, dpois, Gamma_w2n, Stat_dist, setOnes, size,
        ADREPORT, objective_function,
        lambda, tlambda, gamma, tgamma, vector, matrix},
  emphstyle=\color{BrickRed}\textbf,
  emph={[2]a, b, x, y, m, n, i, j, sigma, tsigma, nll},
  emphstyle={[2]\color{Green}},
  commentstyle=\color{Gray}\textit,
  % morecomment=[l][\color{magenta}]{\#}
}

% \onehalfspacing
% \newcommand{\dnorm}[3][x]{\frac{1}{\sqrt{2\pi}#3}\exp\biggl[-\frac{1}{2}\biggl(\frac{#1-#2}{#3}\biggr)^2\biggr]}
% \newcommand{\dnormvar}[3][x]{\frac{1}{\sqrt{2\pi(#3)}}\exp\biggl[-\frac{1}{2}\frac{(#1-#2)^2}{#3}\biggr]}
% 
% \newcommand{\ndist}[1]{\mathcal{N}(#1)} % Normal distribution
% \newcommand{\mn}[1]{\overline{#1}} % Mean: bar or overline?
% \newcommand{\est}[1]{\widehat{#1}} % Estimator/estimate: hat?
% \newcommand{\altest}[1]{\widetilde{#1}} % Estimator/estimate: hat?
% % \mkern2mu\overline{\mkern-2mu F}_i  \overline{F}_i
% 
% \DeclareMathOperator{\LW}{LW}
% \DeclareMathOperator{\Cov}{Cov}         % Covariance
% \DeclareMathOperator{\corr}{corr}       % Correlation
% \DeclareMathOperator{\Var}{Var}         % Variance
% \DeclareMathOperator{\E}{\mathbb{E}}    % Expected value
% \DeclareMathOperator{\diag}{diag}       % Diagonal matrix
% \DeclareMathOperator*{\argmax}{argmax}

% \newcommand*\diff{
% \mathop{}\nobreak
% \mskip-\thinmuskip\nobreak
% \mathrm{d}
% }
% 
% \newcommand*\pdiff{
% \mathop{}\nobreak
% \mskip-\thinmuskip\nobreak
% \partial
% }
% 
% % \newcommand{\abbr}[1]{\textsc{\lowercase{#1}}}
% \newcommand{\abbr}[1]{#1}
% \newcommand{\kort}[1]{\abbr{#1}}
% 
% \providecommand*\ifrac[2]{
% \begingroup #1 \endgroup
% % A Close / so no space before it (unless a punctuation atom is the
% % last item in #1 but unlikely).
% \mathclose{/}%
% % Followed by an empty Open. Thus no space is inserted before the
% % denominator.
% \mathopen{}%
% \begingroup #2 \endgroup
% }
% 
% % Absolute value and norm
% \newcommand{\abs}[1]{\lvert #1 \rvert}
% \newcommand{\eqdef}{\ensuremath{\stackrel{\mathrm{def}}{=}}}


%\bibliographystyle{jf}
%\bibpunct{(}{)}{;}{a}{}{,}

% Bold letters
% \def\balpha{\bm{\alpha}}
% \def\bbeta{\bm{\beta}}
% \def\bdelta{\bm{\delta}}
% \def\bGamma{\bm{\Gamma}}
% \def\btheta{\bm{\theta}}
% \def\bpsi{\bm{\psi}}
% \def\bphi{\bm{\phi}}
% \def\bI{\bm{I}}
% \def\bU{\bm{U}}
% \def\bT{\bm{T}}
% \def\b1{\bm{1}}
% \def\bP{\bm{P}}
% \def\bx{\bm{x}}
% \def\blambda{\bm{\lambda}}

% % Matrices and transpose symbol:
% \newcommand{\matr}[1]{\boldsymbol{#1}}
% \newcommand{\tr}{^{\mathrm{T}}} % Transponert
% \newcommand{\startr}{^{\star{}\mathrm{T}}} % Transponert
% \newcommand{\eps}{\varepsilon}
% 
% 
% \newcommand{\argmin}{\operatornamewithlimits{arg\,min}}
\newcommand{\argmax}{\operatornamewithlimits{arg\,max}}
% \newcommand{\dcon}{\ensuremath{\stackrel{\mathrm{d}}{\to}}}


% \usepackage{url}

% \newenvironment{subfigures}{}{}
% \newenvironment{subtables}{}{}


% \newcommand{\dataset}[1]{\nohyphens{\texttt{#1}}}
% \newcommand{\package}[1]{\dataset{#1}}
% \newcommand{\program}[1]{\dataset{#1}}
% 
% \makeatletter
% \def\maxwidth{.7\textwidth}
% % {%
% % \ifdim\Gin@nat@width>\linewidth
% % \linewidth
% % \else
% % \Gin@nat@width
% % \fi
% % }
% \makeatother

%\usepackage[center]{titlesec}
%\renewcommand{\thetable}{\Roman{table}}
%\renewcommand{\thesection}{\Roman{section}.}
%\renewcommand\thesubsection{\Alph{subsection}.} 

%\hyphenation{Huft-hammer}

% \usepackage{dcolumn}
% \newcolumntype{e}{D{.}{.}{9}}

% \numberwithin{equation}{section}

% \theoremstyle{plain}
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{proposition}{Proposition}[section]
% \newtheorem{lemma}{Lemma}[section]
% 
% \theoremstyle{definition}
% \newtheorem{definition}{Definition}[section]
% \newtheorem{example}{Example}[section]
% 
% \theoremstyle{remark}
% \newtheorem*{remark}{Remark}

% \providecommand{\abs}[1]{\lvert#1\rvert}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}              
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% for compilation knitr::knit2pdf("paper.rnw")



% Import R files here with proper labels
% include = FALSE removes any output from this chunk


% \author{Timothee Bacri$^{1,*}$\email{timothee.bacri@uib.no} \\
% University of Bergen, 5007 Bergen, Norway
% \and
% Jan Bulla$^{2,**}$\email{jan.bulla@uib.no}\\
% University of Bergen, 5007 Bergen, Norway
% \and
% Geir Drage Berentsen$^{3,***}$\email{geir.berentsen@nhh.no}\\
% Norwegian School of Economics, Helleveien 30, 5045 Bergen, Norway
% }

%\DOIsuffix{bimj.DOIsuffix}
\DOIsuffix{bimj.200100000}
\Volume{52}
\Issue{61}
\Year{2020}
\pagespan{1}{}
\keywords{Hidden Markov Model; TMB; Confidence intervals;\\
\noindent \hspace*{-4pc}
% {\small\it (Up to five keywords are allowed and should be given in alphabetical order. Please capitalize the key}\\
\hspace*{-4pc} %{\small\it words)}
\\[1pc]
\noindent\hspace*{-4.2pc} Supporting Information for this article is available from the author or on the WWW under\break \hspace*{-4pc}
\underline{\url{https://github.com/timothee-bacri/hmm-tmb}} % (please delete if not
% applicable)
}  %%% semicolon and fullpoint added here for keyword style

\title[Running title]{Fast parameter and confidence interval estimation for Hidden Markov Models using Template Model Builder}
%% Information for the first author.
\author[First Author{\it{et al.}}]{Timothee Bacri\footnote{Corresponding author: {\sf{e-mail: timothee.bacri@uib.no}}, Phone: +33-636-775-063}\inst{1}}
\address[\inst{1}]{Department of Statistics, University of Bergen, 5007 Bergen, Norway}
%%%%    Information for the second author
\author[dd]{Jan Bulla\inst{1}}
%%%%    Information for the third author
\author[]{Geir D. Berentsen\inst{2}}
\address[\inst{2}]{Department of Business and Management Science, Norwegian School of Economics, Helleveien 30, 5045 Bergen, Norway}
%%%%    \dedicatory{This is a dedicatory.}
\Receiveddate{zzz} \Reviseddate{zzz} \Accepteddate{zzz}

\begin{abstract}
Hidden Markov Models (HMMs) are a class of models widely used in speech recognition \citep[See e.g.][]{gales, fredkin}.
There are straightforward ways to compute Maximum Likelihood (ML) estimates of their parameters, but getting confidence intervals can be more difficult \citep[See e.g.][]{zucchini, lystig}.
In addition, computing ML estimates can be time-consuming when the dataset and the complexity become very large.
We show in this paper a way to speed up computation by up to 50 times in the language R when compared to usual optimization approaches, and at the same time retrieve standard errors easily.
In a first part, we see how to optimize HMMs with the {\tt{TMB}} package in R and how to retrieve confidence intervals.
In a second part, we compare different optimizers such as {\tt{nlminb}}, and minimize the negative log-likelihood directly on different datasets: a small one (240 data points) from \citet{leroux}, a medium sized simulated one (2000 data points), and a large one (87648 data points) from a hospital.
% 
% Why we did it
% What problem/question did we address
% What did we find
% What is new (compare with best approaches)
% How did we do it
% 
% 
% 1-2 sentences Basic intro, audience = anyone
% 2-3 intro to audience in related discipline
% 1 general problem in this paper
% 1 "here we show" summarize result
% 2-3 explain what result reveals compared to what was thought/how it adds to previous knowledge
% 1-2 put result in more general context
% 2-3 broader perspective (discussion points)
\end{abstract}

\maketitle

% \tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short paragraph about HMMs and problems with speed and uncertainty evaluation.

TODO: -define max. l. estimation (MLE)\\

Hidden Markov models (HMMs) are a versatile type of model that have been employed in many different situations since their introduction by \citet{baum}.
As an example, \citet{fredkin} applied them in speech recognition, \citet{lystig} to rainfall occurence data, and \citet{schadt} to phylogenetic trees.
\citet{fredkin} and \citet{zucchini} are references in the theory of HMMs.
Evaluating uncertainty and getting confidence intervals in HMMs is uneasy and not necessarily reliable.
Although \citet[][Ch.~12]{cappe} showed it can be achieved using asymptotic normality of the ML estimates of the parameters under certain conditions, \citet[p. ~53]{fruhwirth-schnatter} points out that in independent mixture models, ``The regularity conditions are often violated''.
\citet[p. ~68]{mclachlan} adds that ``In particular for mixture models, it is well known that the sample size $n$ has to be very large before the asymptotic theory of maximum likelihood applies.''
\citet{lystig} shows a way to compute the exact Hessian, and \citet{zucchini} shows another way to compute the approximate Hessian and thus confidence intervals but admits that ``the use of the Hessian to compute standard errors (and thence confidence intervals) is unreliable if some of the parameters are on or near the boundary of their parameter space''.

{\tt{TMB}} (Template Model Builder) is an R package for efficient fitting of complex statistical random effect models to data, as described by \citet{kristensen}.
It provides exact calculations of first and second order derivatives of the likelihood of a model by automatic differentiation, which allows for efficient gradient and/or Hessian based optimization of the likelihood as well as uncertainty estimates by means of the Hessian.
The Hessian is not necessarily directly applicable for evaluating parameter uncertainty in HMMs as there are several parameter constraints in these models.
This can be adressed by constraint optimization, and subsequently combining the Hessian with the Jacobian of the constraints to obtain the covariance matrix as shown by \citet{visser}.
Alternatively, \citet{zucchini} shows how the constraints can be imposed by suitable transformations of the parameters.
The covariance matrix of the untransformed (original) parameters can then be retrieved by the delta method, a feature implemented in {\tt{TMB}}.

In this paper, we first show how to optimize Poisson HMMs with the help of {\tt{TMB}} in the language R.
Afterwards, we explain how to make a nested model through an example, how to effortlessly compute confidence intervals, how to fetch interesting probabilities, and we apply a Poisson HMM on a real hospital dataset.
Eventually, we see that {\tt{TMB}} can accelerate traditional optimizers in R by up to approximately 50 times on a fairly large dataset, and can easily output confidence intervals similar to bootstrap and profile methods via its Hessian based approach.

Maximum likelihood estimation can be achieved either by a direct numerical maximization as introduced by \citet{turner} and later detailed with R code by \citet{zucchini}, by an Expectation-Maximization (EM) based algorithm as described by \citet{bauma}, or by a mixture of the 2 as shown by \citet{bulla}.
We decide to use a direct maximization approach instead of the EM algorithm.
The reason is that the direct maximization approach is easier to adapt if one wants to fit different and more complex models.
It also deals easily with missing observations, whereas the EM approach is more complex.

% Selling points to include:
% \begin{enumerate}
% \item Two aspects of the paper: 1) Speedup of estimation 2) Esier and faster evaluation of parameter uncertainty
% \item Speed important when $T$ and $m$ large?
% \item Hessian based uncertainty earlier in the traditional sense not feasable \citep{visser}, and finite-differences must be employed since the computation of the Hessian not feasible.
% \item Bootstrap methods requires speed
% \item Profile methods requires speed
% \end{enumerate}
% 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Principles of using {\tt{TMB}} for Maximum Likelihood Estimation}
\label{sec:principles}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


In order to keep this tutorial at acceptable length, all sections follow the same concept.
That is, the reader is encouraged to consult our GitHub repository in parallel (https://github.com/(TIMO:MAKE A REPOSITORY)).
This permits to copy-paste or download all the scripts presented in this tutorial for each section.
Moreover, the repository also contains additional explanations, comments, and scripts.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Setup}
\label{sec:setup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Execution of our routines requires the installation of the R-package {\tt{TMB}} and the software {\tt{Rtools}}, where the latter serves for compiling C++ code.
In order to ensure reproducibility of all results involving the generation of random numbers, the \texttt{set.seed} function requires R version number 3.6.0 or greater.
Our scripts were tested on an Intel(R) Core(TM) i7-8700 processor running under Windows 10 Enterprise version 1809.

In particular for beginners, those parts of scripts involving C++ code can be difficult to debug because the code operates using a specific template.
Therefore it is helpful to know that {\tt{TMB}} provides a debugging feature, which can be useful to retrieve diagnostic error messages, in RStudio.
Enabling this feature is optional and can be achieved by the command \texttt{TMB:::setupRStudio()} (requires manual confirmation and re-starting RStudio).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Linear regression example}
\label{sec:linreg}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We begin by demonstrating the principles of {\tt{TMB}}, which we illustrate through the fitting procedure for a simple linear model.
This permits, among other things, to show how to handle parameters subject to constraints, an aspect particularly relevant for HMMs.
A more comprehensive tutorial on {\tt{TMB}} presenting many technical details in more depths is available at\\
\underline{\url{https://kaskr.github.io/adcomp/\_book/Tutorial.html}}.

Let $\bm{x}$ and $\bm{y}$ denote the predictor and response vector, respectively, both of length $n$.
For a simple linear regression model with intercept $a$ and slope $b$, the negative log-likelihood equals
\begin{equation*}
- \log L(a, b, \sigma^2) = - \sum_{i=1}^n \log(\phi(y_i; a + bx_i, \sigma^2))),
\end{equation*}
where $\phi(\cdot; \mu, \sigma^2)$ corresponds to the density function of the univariate normal distribution with mean $\mu$ and variance $\sigma^2$.

The use of {\tt{TMB}} requires the (negative) log-likelihood function to be coded in C++ under a specific template, which is then loaded into R.
The minimization of this function and other post-processing procedures are all carried out in R.
Therefore, we require two files.
The first file, named \textit{linreg.cpp}, is written in C++ and defines the objective function, i.e.~the negative log-likelihood (nll) function of the linear model, as follows.\\

\lstinputlisting{code/linreg.cpp}

Note that we define data inputs $x$ and $y$ using the \texttt{DATA\_VECTOR()} declaration in the above code.
Furthermore, we declare the nll as a function of the three parameters a, b and $\log(\sigma)$ using the \texttt{PARAMETER()} declaration.
In order to be able to carry out unconstrained optimization procedures in the following, the nll function is parameterized in terms of $\log(\sigma)$.
While the parameter $\sigma$ is constrained to be non-negative, $\log(\sigma)$ can be freely estimated.
Alternatively, constraint optimization methods could be carried out, but we do not investigate such procedures.
The \texttt{ADREPORT()} function is optional but useful for parameter inference at the postprocessing stage.
%This approach avoids the need of using constraint optimization methods.

The second file needed is written in R and serves for compiling the nll function defined above and carrying out the estimation procedure by numerical optimization of the nll function.
The .R file (shown below) carries out the compilation of the C++ file and minimization of the nll function:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Loading TMB package}
\hlkwd{library}\hlstd{(TMB)}
\hlcom{# Compilation. The compiler returns 0 if the compilation of}
\hlcom{# the cpp file was successful}
\hlstd{TMB}\hlopt{::}\hlkwd{compile}\hlstd{(}\hlstr{"code/linreg.cpp"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0
\end{verbatim}
\begin{alltt}
\hlcom{# Dynamic loading of the compiled cpp file}
\hlkwd{dyn.load}\hlstd{(}\hlkwd{dynlib}\hlstd{(}\hlstr{"code/linreg"}\hlstd{))}
\hlcom{# Generate the data for our test sample}
\hlkwd{set.seed}\hlstd{(}\hlnum{123}\hlstd{)}
\hlstd{data} \hlkwb{<-} \hlkwd{list}\hlstd{(}\hlkwc{y} \hlstd{=} \hlkwd{rnorm}\hlstd{(}\hlnum{20}\hlstd{)} \hlopt{+} \hlnum{1}\hlopt{:}\hlnum{20}\hlstd{,} \hlkwc{x} \hlstd{=} \hlnum{1}\hlopt{:}\hlnum{20}\hlstd{)}
\hlstd{parameters} \hlkwb{<-} \hlkwd{list}\hlstd{(}\hlkwc{a} \hlstd{=} \hlnum{0}\hlstd{,} \hlkwc{b} \hlstd{=} \hlnum{0}\hlstd{,} \hlkwc{tsigma} \hlstd{=} \hlnum{0}\hlstd{)}
\hlcom{# Instruct TMB to create the likelihood function}
\hlstd{obj_linreg} \hlkwb{<-} \hlkwd{MakeADFun}\hlstd{(data, parameters,} \hlkwc{DLL} \hlstd{=} \hlstr{"linreg"}\hlstd{,}
                        \hlkwc{silent} \hlstd{=} \hlnum{TRUE}\hlstd{)}
\hlcom{# Optimization of the objective function with nlminb}
\hlstd{mod_linreg} \hlkwb{<-} \hlkwd{nlminb}\hlstd{(obj_linreg}\hlopt{$}\hlstd{par, obj_linreg}\hlopt{$}\hlstd{fn)}
\hlstd{mod_linreg}\hlopt{$}\hlstd{par}
\end{alltt}
\begin{verbatim}
##           a           b      tsigma 
##  0.31009240  0.98395535 -0.05814659
\end{verbatim}
\end{kframe}
\end{knitrout}

In addition to the core functionality presented above, different types of post-processing of the results are possible as well. For example, the function \texttt{sdreport} returns the ML estimates and standard errors of the parameters in terms of which the nll is parameterized:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{sdreport}\hlstd{(obj_linreg,} \hlkwc{par.fixed} \hlstd{= mod_linreg}\hlopt{$}\hlstd{par)}
\end{alltt}
\begin{verbatim}
## sdreport(.) result
##           Estimate Std. Error
## a       0.31009240 0.43829083
## b       0.98395535 0.03658781
## tsigma -0.05814659 0.15811381
## Maximum gradient component: 6.931683e-05
\end{verbatim}
\end{kframe}
\end{knitrout}
In principle, the argument \texttt{par.fixed = mod\_linreg\$par} is optional but recommended, because it ensures that the \texttt{sdreport} function is carried out at the minimum found by \texttt{nlminb}. Note that the standard errors above are based on the Hessian matrix of the nll.\\
From a practical perspective, it is usually desirable to obtain standard errors for the constrained variables, in this case $\sigma$. To achieve this, one can run the \texttt{summary} function with argument \texttt{select = "report"}:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{summary}\hlstd{(}\hlkwd{sdreport}\hlstd{(obj_linreg,} \hlkwc{par.fixed} \hlstd{= mod_linreg}\hlopt{$}\hlstd{par),}
        \hlkwc{select} \hlstd{=} \hlstr{"report"}\hlstd{)}
\end{alltt}
\begin{verbatim}
##        Estimate Std. Error
## sigma 0.9435116  0.1491822
\end{verbatim}
\end{kframe}
\end{knitrout}
These standard errors result from the generalized delta method described by \citet{kass}, which is implemented within {\tt{TMB}}. Note that full functionality of the \texttt{sdreport} function requires calling the function \texttt{ADREPORT} on the additional parameters of interest (i.e. those including transformed parameters, in our example $\sigma$) in the C++ part.
The \texttt{select} argument restricts the output to variables passed by \texttt{ADREPORT}.
This feature is particularly useful when the likelihood has been reparameterized as above, and is especially relevant for HMMs.
Following \citet{zucchini}, we refer to the original parameters as natural parameters, and to their transformed version as the working parameters.\\
Last, we display the estimation results from the \texttt{lm} function for comparison.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{lm}\hlstd{(y} \hlopt{~} \hlstd{x,} \hlkwc{data} \hlstd{= data)}\hlopt{$}\hlstd{coefficients}
\end{alltt}
\begin{verbatim}
## (Intercept)           x 
##   0.3100925   0.9839554
\end{verbatim}
\end{kframe}
\end{knitrout}
Note that minor deviations from the results of \texttt{lm} originate in the numerical methods involved in the selected optimization procedure, in our case \texttt{nlminb}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Parameter estimation techniques for HMMs}
\label{sec:estimation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section we recall basic concepts underlying parameter estimation for HMMs via direct numerical optization of the likelihood. In terms of notation, we stay as close as possible to \citet{zucchini}, where a more detailed presentation is available.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Basic notation and model setup}
\label{sec:notation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A large variety of modeling approaches is possible with HMMs, ranging from rather simple to highly complex setups.
In a basic HMM, one assumes that the data-generating process corresponds to a time-dependent mixture of the so-called conditional distributions.
More specifically, the mixing process is driven by an unobserved (hidden) homogeneous Markov chain.
In this paper we focus on a Poisson HMM, but only small changes are necessary to adapt our scripts to models with other conditional distributions.
Let $\{X_t: t = 1, \ldots, T\}$ and $\{C_t : t = 1, \ldots, T\}$ denote the observed and hidden process, respectively.
For an $m$-state Poisson HMM, the conditional distributions with parameter $\lambda_i$ are then specified through
\begin{equation*}
p_i(x) = P(X_t = x \vert C_t = i) = \frac{e^{-\lambda_i} \lambda_i^x}{x!},
\end{equation*}
where $i = 1, \ldots, m$. Furthermore, we let $\bgamma = \{\gamma_{ij}\}$ and $\bfdelta$ denote the transition probability matrix (TPM) of the Markov chain and the corresponding stationary distribution, respectively.
It is noteworthy that Markov chains in the context of HMMs are often assumed irreducible and aperiodic.
For example,
\citet[Lemma 6.3.5 on p. ~225 and Theorem 6.4.3 on p. ~227]{grimmett} show that irreducibility ensures the existence of the stationary distribution, and \citet[p. ~394]{feller} describe that aperiodicity implies that a unique limiting distribution exists and corresponds to the stationary distribution.
These results are, however, of limited relevance for most estimation algorithms, because the elements of $\bgamma$ are in general strictly positive. Nevertheless, one should be careful when manually setting selected elements of $\bgamma$ equal to zero.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The likelihood function of an HMM}
\label{sec:hmm_likelihood}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The likelihood function of an HMM requires, in principle, an summation over all possible state sequences. As shown e.g.~by \citet[p.~37]{zucchini}, a computationally convenient representation as a product of matrices is possible. Let $X^{(t)} = \{X_1, \ldots, X_t \}$ and $x^{(t)} = \{x_1, \ldots, x_t \}$ denote the history of the observed process $X_t$ and the observations $x_t$ from time zero up to time $t$. Moreover, let $\btheta$ denote the vector of model parameters, which consists of the parameters of the TPM and the parameters of the conditional probability density functions (pdf). Given these parameters, the likelihood of the observations $\{x_1, \ldots, x_T \}$ can then be expressed as
\begin{equation}
\label{eq:hmm_likelihood}
L($\btheta$) = \bcp(X^{(T)} = x^{(T)}) = \bfdelta \bcp(x_1) \bgamma \bcp(x_2) \bgamma \bcp(x_3) \ldots \bgamma \bcp(x_T) \bone',
\end{equation}
where
\begin{equation*}
\bcp(x) = \begin{pmatrix}
p_1(x)    &         &         & 0\\
          & p_2(x)  &         &\\
          &         & \ddots  &\\
0         &         &         & p_m(x)
\end{pmatrix}
\end{equation*}
corresponds to a diagonal matrix with the $m$ conditional pdfs evaluated at $x$ (we will use the term density despite the discrete support), and $\bone$ denotes a vector of ones. The first element of the likelhood function, the so-called initial distribution, is given by the stationary distribution $\bfdelta$ here. Alternatively, the initial distribution may be estimated freely, which requires minor changes to the likelihood function discussed in Section \ref{sec:tmb_cpp}.\\
Note that the treatment of missing data is comparably straightforward in this setup. If $x$ is a missing observations, one just has to set $p_i(x) =  1$, thus $\bcp(x)$ reduces to the unity matrix as detailed in \citet[p. ~40]{zucchini}.
\citet[p. ~41]{zucchini} also explains how to adjust the likelihood when entire intervals are missing. Furthermore, this representation of the likelihood is quite natural from an intuitive point of view. From left to right, it can be interpreted as a pass through the observations: one starts with the initial distribution multiplied by the conditional density of $x_1$ collected in $\bcp(x_1)$. This is followed by iterative multiplications with the TPM modeling the transition to the next observation, and yet another multiplication with contributions of the following conditional densities.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Forward algorithm and backward algorithm}
\label{sec:hmm_fwbw}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The pass through the observations described above actually forms the basis for an efficient evaluation of the  likelihood function. More precisely, the so-called ``forward algorithm'' allows for a recursive computation of the likelihood. For setting up this algorithm, we need to define the vector $\balpha_t$ by
\begin{align*}
\balpha_t &= \bfdelta \bcp(x_1)\bgamma \bcp(x_2) \bgamma \bcp(x_3) \ldots \bgamma \bcp(x_t)\\
&= \bfdelta \bcp(x_1) \prod_{s=2}^{t}\bgamma \bcp(x_s)\\
&= \left( \alpha_t(1), \ldots, \alpha_t(m) \right)
\end{align*}
for $t = 1, 2, \ldots, T$. 
The name forward algorithm originates from the way of calculating $\balpha_t$, i.e.
\begin{gather*}
\balpha_0 = \bfdelta \bcp(x_1)\\
\balpha_t = \balpha_{t-1} \bgamma \bcp(x_t) \text{ for } t = 1, 2, \ldots, T.
\end{gather*}
After a pass through all observations, the likelihood results from
\begin{gather*}
L(\btheta) = \balpha_T \bone'.
\end{gather*}
In a similar way, the ``backward algorithm'' also permits the recursive computation of the likelihood, but starting with the last observation. To formulate the backward algorithm, let us define the vector $\bfbeta_t$ for $t = 1, 2,
\ldots, T$ so that
\begin{align*}
\bfbeta'_t &= \bgamma \bcp(x_{t+1}) \bgamma \bcp(x_{t+2}) \ldots \bgamma \bcp(x_T) \ldots \bone'\\
&= \left(\prod_{s=t+1}^{T}\bgamma \bcp(x_s) \right) \bone'\\
&= \left( \beta_t(1), \ldots, \beta_t(m) \right)
\end{align*}
The name backward algorithm results from the way of calculating $\bfbeta_t$, i.e.
\begin{gather*}
\bfbeta_T = \bone'\\
\bfbeta_t = \bgamma \bcp(x_{t+1}) \bfbeta_{t+1} \text{ for } t = T-1, T-2, \ldots, 1.
\end{gather*}
Again, the likelihood can be calculated after a pass through all observations by
\begin{gather*}
L(\btheta) = \bfdelta \bfbeta_1.
\end{gather*}
In general, parameter estimation bases on the forward algorithm.
The backward algorithm is, however, still useful because the quantities $\balpha_t$ and $\bfbeta_t$ together serve for a couple of interesting tasks.
For example, they are the basis for deriving a particular type of conditional distributions and for state inference by local decoding \cite[Ch.~5, pp.~81-93]{zucchini}. We present details on local decoding at \underline{\url{https://github.com/timothee-bacri/hmm-tmb}}.
Last, it is well-known that the execution of the forward (or backward) algorithm may quickly lead to underflow errors, because many (for discrete distributions: all) elements of the vectors and matrices involved take values between zero and one.
To avoid these difficulties, a scaling factor can be introduced.
We follow the approach suggested by \citet[p. ~48]{zucchini} and implement a scaled version of the forward algorithm, which directly provides the (negative) log-likelihood as result.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Reparameterization of the likelihood function}
\label{sec:hmm_repar}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The representation of the likelihood and the algorithms shown above rely on the data and the set of parameters $\btheta$ as input. The data are subject to several constraints:
\begin{enumerate}
\item Typically there are various constraints of the parameters in the conditional distribution. For the Poisson HMM, all elements of the parameter vector $\bflambda\ = (\lambda_1, \dots, \lambda_m)$ must be non-negative. 
\item In general, the parameters $\gamma_{ij}$ of the TPM $\bgamma$ have to be non-negative, and the rows of $\bgamma$ must sum up to one.
\end{enumerate}
The constraints of the TPM can be difficult to deal with using constrained optimization of the likelihood. A common approach is to reparameterize the log-likelihood in terms of unconstrained ``working'' parameters $\{\bct, \bfeta\}= g^{-1}(\bgamma, \bflambda)$, as follows. A possible reparameterisation of $\bgamma$ is given by 

\begin{equation*}
\gamma_{ij} = \frac{\exp(\tau_{ij})}{1 + \sum_{k \neq i} \tau_{ik}}, \text{ for } i \neq j
\end{equation*}

where $\tau_{ij}$ are $m(m-1)$ real-valued, thus unconstrained, elements of an $m$ times $m$ matrix $\bct$ with no diagonal elements. The diagonal elements of $\bgamma$ follows implicitly from $\sum_j \gamma_{ij} = 1 \;\forall\; i$ \cite[p. ~51]{zucchini}. The corresponding reverse transformation is given by
\begin{equation*}
\tau_{ij} = \log\left(\frac{\gamma_{ij}}{1 - \sum_{k \neq i} \gamma_{ik}}\right) = \log(\gamma_{ij}/\gamma_{ii}), \text{ for } i \neq j
\end{equation*}

For the Poisson HMM the intensities can be reparameterised in terms of $\lambda_i = \exp(\eta_i)$, and consequently the unconstrained working parameters are given by $\eta_i = \log(\lambda_i), i = 1,\dots,m$. Estimates of the "natural" parameters $\{\bgamma, \bflambda\}$ can then be obtained by maximizing the reparameterised likelihood with respect to $\{\bct, \bfeta\}$ and then transforming the estimated working parameters back to natural parameters via the above transformations, i.e. $\{\hat{\bgamma}, \hat{\bflambda}\} = g(\hat{\bct}, \hat{\bfeta})$. Note that in general the function $g$ needs to be one-to-one for the above procedure to work.

% As illustrated in the previous section, we focus on unconstrained optimization, similar \citet[p. ~50]{zucchini}.
% Hence, we parameterize the likelihood function as a function of the working parameters.
% The computation of the likelihood then requires to transform the working parameters back to natural parameters before calculating the negative log-likelihood via \autoref{eq:hmm_likelihood}.
% This part is entirely set up in the C++ likelihood function.
% 
% Since the transformation of the transition probability Matrix (TPM) is identical for all HMMs, it is stored as a separate C++ function \texttt{Gamma\_w2n} which is available in the file \textit{utils.cpp} (Appendix A.2 \autoref{code:Gamma_w2n}).
% This file also contains the C++ function \texttt{Delta\_w2n} which, if necessary, we can define to transform the working stationary distribution vector into a natural parameter (Appendix A.2 \autoref{code:Delta_w2n}).
% It is however unneeded for this paper.
% 
% Given a working parameter vector \texttt{log\_lambda}, the C++ function turning the working vector of the Poisson means into a natural parameter is simple:
% \begin{lstlisting}
% vector<Type> lambda = tlambda.exp();
% \end{lstlisting}
% 
% Such a transformation can be adapted to other conditional distributions.
% Note that the parts concerning the Markov chain remain unchanged.
% The working parameters returned then serve as initial values for starting the optimization procedure of the likelihood via {\tt{TMB}}.
% 


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Needed? / to be integrated later}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% 
% We call $\begin{pmatrix}
% p_1(x_1)  & p_2(x_1) & \ldots & p_m(x_1)\\
% p_1(x_2)  & p_2(x_2) & \ldots & p_m(x_2)\\
% \vdots    &  \vdots  & \ddots & \vdots\\
% p_1(x_T)  & p_2(x_T) & \ldots & p_m(x_T)\\
% \end{pmatrix}$ the emission probability matrix.\\
% 
% 
% Similarly to \citet{zucchini}, we choose to minimize the negative log-likelihood for convenience purposes.
% It is therefore better to maximize the log-likelihood instead.
% 
% 
% In summary, we need to define the state-dependent probabilities (from Poisson distributions) and the transformations in R and C++ (depending on the distributions used, but can be easily adapted).
% We show below the next step: defining the likelihood function in C++ and using it to model a dataset in R.
% 
% 
% In practice, as explained in \autoref{sec:principles}, we first create a set of natural parameters and turn them into working parameters before passing them to an optimizer.\\
% 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Using TMB}
\label{sec:tmb}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In the following we show how parameter estimation for HMMs can be carried out  efficiently via TMB. The TMB (R and C++) code used below is available from
https://github.com/(TIMO:INSERT LINK TO REPOSITORY) to allow for consistent maintenance of the code. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Likelihood function}
\label{sec:tmb_cpp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Similar to the linear regression example presented in \ref{sec:linreg}, the first and essential step is to define our nll function to be minimized later in a suitable C++ file. In our case, this function calculates the negative log-likelihood presented by \citet[p. ~48]{zucchini}, and our C++ code is analog to the R-code shown by \citet[p. ~331 - 333]{zucchini}. This function, named \textit{poi\_hmm.cpp}, tackles our setting with conditional Poisson distributions only. An extension to for example Gaussian, binomial and exponential conditional distributions is straightforward. It only requires to modify the density function in the \textit{poi\_hmm.cpp} function and the related functions for parameter transformation presented in Section \ref{sec:hmm_repar}. We illustrate the implementation of these cases in the GitHub repository. However, note that the number of possible modelling setups is very large: e.g., the conditional distributions may vary from state to state, nested model specifications, the conditional mean may be linked to covariates, or the TPM could depend on covariates - to name only a few. Due to the very large number of possible extensions of the basic HMM, we refrain from implementing an R-package, but prefer to provide a proper guidance to the reader for building custom models suited to a particular application. As a small example, we illustrate how to implement a freely estimated initial distribution in the function \textit{poi\_hmm.cpp}. This modification can be achieved by uncommenting a couple of lines only.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Optimization}
\label{sec:tmb_r}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

With the nll function available in C++, we can carry out the parameter estimation and all pre-/post-processing in R. in the following we describe the steps to be carried out.

\begin{enumerate}
\item Loading of the necessary packages, compilation of the nll function with TMB and subsequent loading, and loading of the auxiliary functions for parameter transformation.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Load TMB and optimization packages}
\hlkwd{library}\hlstd{(TMB)}
\hlkwd{library}\hlstd{(optimr)}
\hlcom{# Run the C++ file containing the TMB code}
\hlstd{TMB}\hlopt{::}\hlkwd{compile}\hlstd{(}\hlstr{"code/poi_hmm.cpp"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0
\end{verbatim}
\begin{alltt}
\hlcom{# Load it}
\hlkwd{dyn.load}\hlstd{(}\hlkwd{dynlib}\hlstd{(}\hlstr{"code/poi_hmm"}\hlstd{))}
\hlcom{# Load the parameter transformation function}
\hlkwd{source}\hlstd{(}\hlstr{"functions/utils.R"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\item Loading of the observations. The data are part of a large data set collected with the "TrackYourTinnitus" (TYT) mobile application, a detailed description of which is presented in \citet{pryss} and \citet{pryssa}(TIMO: insert Pryss 2015a  2015b).
We analyze 87 successive days of the ``arousal'' variable, which is measured on a discrete scale. Higher values correspond to a higher degree of excitement, lower values to a more calm emotional state \citep[for details, see][]{probst}.\\

TIMO: I need these references included (in the tex)

% Pryss, R., Reichert, M., Herrmann, J., Langguth, B., Schlee, W. (2015a). "Mobile crowd sensing in clinical and psychological trials - a case study," in 28th IEEE Int'l Symposium on Computer-Based Medical Systems, 22-25 June 2015 (Sao Carlos: IEEE Computer Society Press), 23-24. Available online at: http://dbis.eprints.uni-ulm.de/1144/1/PCBMS_2015.pdf (Accessed July 15, 2017)

%Pryss, R., Reichert, M., Langguth, B., and Schlee, W. (2015b). "Mobile crowd sensing services for tinnitus assessment, therapy and research," in IEEE 4th International Conference on Mobile Services (MS 2015), June 27-July 2, 2015 (New York, NY: IEEE Computer Society Press), 352-359. Available online at: http://dbis.eprints.uni-ulm.de/1152/1/ms2015rpmrblws.pdf (Accessed: July 15, 2017).

% https://www.nature.com/articles/srep31166
Loading the ``arousal'' variable can be achieved simply with
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{load}\hlstd{(}\hlstr{"data/tinnitus.RData"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

TIMO: update the table as well please

\autoref{table:tinnitus_data} presents the raw data, which are also available for download at the GitHub repository.

\bigskip
% <<lamb-table, results = 'asis', echo = FALSE>>=
% temp <- paste(lamb_data, collapse = " ")
% temp <- as.data.frame(temp)
% temp <- xtable(temp,
%                align = "lp{15cm}",
%                caption = "Numbers of movements by a fetal lamb in 240 consecutive 5-second intervals. The table reads from left to right and from top to bottom.",
%                label = "table:lamb_data")
% print(temp, include.rownames = FALSE, include.colnames = FALSE,
%       hline.after = c(0, nrow(temp)))
% @

% latex table generated in R 4.0.5 by xtable 1.8-4 package
% Sat May 01 02:46:22 2021
\begin{table}[ht]
\centering
\begin{tabular}{p{15cm}}
   \hline
6 5 3 6 4 3 5 6 6 6 4 6 6 4 6 6 6 6 6 4 6 5 6 7 6 5 5 5 7 6 5 6 5 6 6 6 5 6 7 7 6 7 6 6 6 6 5 7 6 1 6 0 2 1 6 7 6 6 6 5 5 6 6 2 5 0 1 1 1 2 3 1 3 1 3 0 1 1 1 4 1 4 1 2 2 2 0 \\ 
   \hline
\end{tabular}
\caption{Tinnitus data} 
\label{table:tinnitus_data}
\end{table}


\item Initialization of the number of states and starting (or initial) values for the optimization. First, the number of states needs to be determined. As explained by \citet{pohlea}, \citet{pohle}, and \citet[][Section 6]{zucchini} (to name only a few), usually one would first fit models with a different number of states. Then, these models are evaluated e.g.~by means of model selection criteria \citep[as carried out by][]{leroux} or prediction performance \citep{celeux}. Since the results reported by \citet{leroux} show that a two-state model is preferred by the BIC, we focus on this model only here - although other choices would be possible, e.g.~the AIC selects a three-state model. The list object \texttt{TMB\_data} contains the data and the number of states.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Model with 2 states}
\hlstd{m} \hlkwb{<-} \hlnum{2}
\hlstd{TMB_data} \hlkwb{<-} \hlkwd{list}\hlstd{(}\hlkwc{x} \hlstd{= tinn_data,} \hlkwc{m} \hlstd{= m)}
\end{alltt}
\end{kframe}
\end{knitrout}
Secondly, initial values for the optimization procedure need to be defined. Although we will apply unconstrained optimization, we initialize the natural parameters, because this is much more intuitive and practical than handling the working parameters. 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Generate initial set of parameters for optimization}
\hlstd{lambda} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{3}\hlstd{)}
\hlstd{gamma} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{0.8}\hlstd{,} \hlnum{0.2}\hlstd{,}
                  \hlnum{0.2}\hlstd{,} \hlnum{0.8}\hlstd{),} \hlkwc{byrow} \hlstd{=} \hlnum{TRUE}\hlstd{,} \hlkwc{nrow} \hlstd{= m)}
\end{alltt}
\end{kframe}
\end{knitrout}

\item Transformation from natural to working parameters. The previously created initial values are transformed and stored in the list {\tt parameters} for the optimization procedure.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Turn them into working parameters}
\hlstd{parameters} \hlkwb{<-} \hlkwd{pois.HMM.pn2pw}\hlstd{(m, lambda, gamma)}
\end{alltt}
\end{kframe}
\end{knitrout}

\item Creation of the \texttt{TMB} negative log-likelihood function with its derivatives. This object, stored as \texttt{obj\_tmb} requires the data, the initial values, and the previously created the DLL as input. Setting argument \texttt{silent = TRUE} disables tracing information and is only used here to avoid excessive output.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{obj_tmb} \hlkwb{<-} \hlkwd{MakeADFun}\hlstd{(TMB_data, parameters,}
                     \hlkwc{DLL} \hlstd{=} \hlstr{"poi_hmm"}\hlstd{,} \hlkwc{silent} \hlstd{=} \hlnum{TRUE}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

This object also contains the previously defined initial values as a vector (\texttt{par}) rather than a list. The negative log-likelihood (\texttt{fn}), its gradient (\texttt{gr}), and Hessian (\texttt{he}) are functions of the parameters (in vector form) while the data are considered fixed: 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{obj_tmb}\hlopt{$}\hlstd{par}
\end{alltt}
\begin{verbatim}
##   tlambda   tlambda    tgamma    tgamma 
##  0.000000  1.098612 -1.386294 -1.386294
\end{verbatim}
\begin{alltt}
\hlstd{obj_tmb}\hlopt{$}\hlkwd{fn}\hlstd{(obj_tmb}\hlopt{$}\hlstd{par)}
\end{alltt}
\begin{verbatim}
## [1] 228.3552
\end{verbatim}
\begin{alltt}
\hlstd{obj_tmb}\hlopt{$}\hlkwd{gr}\hlstd{(obj_tmb}\hlopt{$}\hlstd{par)}
\end{alltt}
\begin{verbatim}
##          [,1]      [,2]     [,3]      [,4]
## [1,] -3.60306 -146.0336 10.52832 -1.031706
\end{verbatim}
\begin{alltt}
\hlstd{obj_tmb}\hlopt{$}\hlkwd{he}\hlstd{(obj_tmb}\hlopt{$}\hlstd{par)}
\end{alltt}
\begin{verbatim}
##           [,1]       [,2]       [,3]       [,4]
## [1,]  1.902009  -5.877900 -1.3799682  2.4054017
## [2,] -5.877900 188.088247 -4.8501589  2.3434284
## [3,] -1.379968  -4.850159  9.6066700 -0.8410438
## [4,]  2.405402   2.343428 -0.8410438  0.7984216
\end{verbatim}
\end{kframe}
\end{knitrout}



\item Execution of the optimization. For this step we rely again on the optimizer implemented in the {\tt{nlminb}} function. The arguments, i.e.~ initial values for the parameters and the function to be optimized, are extracted from the previously created TMB object. 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{mod_tmb} \hlkwb{<-} \hlkwd{nlminb}\hlstd{(}\hlkwc{start} \hlstd{= obj_tmb}\hlopt{$}\hlstd{par,} \hlkwc{objective} \hlstd{= obj_tmb}\hlopt{$}\hlstd{fn)}
\hlcom{# Check that it converged successfully}
\hlstd{mod_tmb}\hlopt{$}\hlstd{convergence} \hlopt{==} \hlnum{0}
\end{alltt}
\begin{verbatim}
## [1] TRUE
\end{verbatim}
\end{kframe}
\end{knitrout}
There are alternatives to {\tt{nlminb}}, but we focus on it here because of its good performance in terms of speed. Further details are available in Section \ref{sec:speed} \autoref{txt:all-methods-comparison}. 

\item Obtaining the ML estimates of the natural parameters together with their standard errors is possible by using the previously introduced command \texttt{sdreport}. Recall that this requires the parameters of interest to be treated by the \texttt{ADREPORT} statement in the C++ part. It should be noted that the presentation of the set of parameters \texttt{gamma} below results from a column-wise representation of the TPM.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{summary}\hlstd{(}\hlkwd{sdreport}\hlstd{(obj_tmb,} \hlkwc{par.fixed} \hlstd{= mod_tmb}\hlopt{$}\hlstd{par),} \hlstr{"report"}\hlstd{)}
\end{alltt}
\begin{verbatim}
##          Estimate Std. Error
## lambda 1.63641070 0.27758294
## lambda 5.53309626 0.31876141
## gamma  0.94980192 0.04374682
## gamma  0.02592209 0.02088689
## gamma  0.05019808 0.04374682
## gamma  0.97407791 0.02088689
## delta  0.34054163 0.23056401
## delta  0.65945837 0.23056401
\end{verbatim}
\end{kframe}
\end{knitrout}
Note that the table above also contains estimation results for $\bfdelta$ and accompanying standard errors, although $\bfdelta$ is not estimated, but derived from $\bgamma$. We provide further details on this aspect in \autoref{sec:hessian}.\\
The value of the nll function in the minimum found by the optimizer can also be extracted directly from the object {\tt mod\_tmb} by accessing the list element {\tt objective}:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{mod_tmb}\hlopt{$}\hlstd{objective}
\end{alltt}
\begin{verbatim}
## [1] 168.5361
\end{verbatim}
\end{kframe}
\end{knitrout}


\item In the optimization above we already benefited from an increased speed due to the evaluation of the nll in C++ compared to the forward algorithm being executed entirely in R. However, the use of {\tt{TMB}} also permits to introduce the gradient and/or the Hessian computed by {\tt{TMB}} into the optimization procedure. This is in general advisable, because {\tt{TMB}} provides an exact value of both gradient and Hessian up to machine precision, which is superior to approximations used by optimizing procedure. Similar to the nll, both quantities can be extracted directly from the {\tt{TMB}} object {\tt obj\_tmb}:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# The negative log-likelihood is accessed by the objective}
\hlcom{# attribute of the optimized object}
\hlstd{mod_tmb} \hlkwb{<-} \hlkwd{nlminb}\hlstd{(}\hlkwc{start} \hlstd{= obj_tmb}\hlopt{$}\hlstd{par,} \hlkwc{objective} \hlstd{= obj_tmb}\hlopt{$}\hlstd{fn,}
                  \hlkwc{gradient} \hlstd{= obj_tmb}\hlopt{$}\hlstd{gr,} \hlkwc{hessian} \hlstd{= obj_tmb}\hlopt{$}\hlstd{he)}
\hlstd{mod_tmb}\hlopt{$}\hlstd{objective}
\end{alltt}
\begin{verbatim}
## [1] 168.5361
\end{verbatim}
\end{kframe}
\end{knitrout}
Note that passing the exact gradient and Hessian as provided by {\tt{TMB}} to \texttt{nlminb} leads to the same minimum, i.e.~value of the nll function, here.

\end{enumerate}

On a minor note, when comparing our estimation results to those reported by \citet{leroux}, some non-negligible differences can be noted. The reasons for this are difficult to determine, but some likely explanations are given in the following. First, differences in the parameter estimates may result e.g.~from the optimizing algorithms used and related setting (e.g.~convergence criterion, number of steps, optimization routines used in 1992,...). Moreover, \citet{leroux} seem to base their calculations on an altered likelihood, which is reduced by removing the constant term $\sum_{i=1}^{T} \log(x_{i}!)$ from the log-likelihood. This modification may also possess an impact on the behavior of the optimization algorithm, as e.g.~relative convergence criteria and step size could be affected.\\

% TIMO: if we show the following, I would put all of it on GitHub. It could be in the part corresponding to Section 4, but as supplementary contents
 
% The reason is that their likelihood is altered while ours isn't.
% This becomes clear when comparing the likelihoods with only one state.
% 
% A 1 state Poisson HMM is the same as a Poisson regression model, for which the log-likelihood has the expression
% \begin{align*}
% l(\lambda) &= \log \left(\prod_{i=1}^{T} \frac{\lambda^{x_i} e^{-\lambda}}{x_{i}!} \right)\\
% &= - T \lambda + \log(\lambda) \left( \sum_{i=1}^{T} x_i \right) - \sum_{i=1}^{T} \log(x_{i}!).
% \end{align*}
% The authors find a ML estimate $\lambda = 0.3583$ and a log-likelihood of -174.26.
% In contrast, calculating the log-likelihood explicitly shows a different result.
% <<leroux-likelihood-calculation>>=
% x <- lamb_data
% # We use n instead of T in R code
% n <- length(x)
% l <- 0.3583
% - n * l + log(l) * sum(x) - sum(log(factorial(x)))
% @
% 
% The log-likelihood is different, but when the constant $- \sum_{i=1}^{T} \log(x_{i}!)$ is removed, it matches our result.
% <<leroux-likelihood>>=
% - n * l + log(l) * sum(x)
% @
% In this paper, we use the complete formula for the negative log-likelihood, which therefore differs sligthly from \citet{leroux}.


% TIMO: same here, this is too irrelevant for being an own section. I would put it as well into the readme of section 4, but as supplementary section. 

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Computing the stationary distribution}
% \label{sec:stat_dist}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% Within the objective function in \autoref{sec:tmb_cpp}, the stationary distribution of the $m$ state HMM's Markov chain with transition probability matrix $\bgamma$ is calculated.
% In this section, we explain that calculation.
% 
% \citet{zucchini} shows that calculating the stationary distribution can be achieved by solving \autoref{eq:stat-dist} for $\bfdelta$, where $\bci_m$ is the $m*m$ identity matrix, $\bcu$ is a $m*m$ matrix of ones, and $\bone$ is a row vector of ones.
% \begin{equation}
% \bfdelta(\bci_m - \bgamma + \bcu) = \bone
% \label{eq:stat-dist}
% \end{equation}
% 
% An implementation of this in R is shown here
% <<stat.dist>>=
% @
% and used in the supporting information.
% 
% In order to use it in {\tt{TMB}}, an implementation in C++ is necessary under a specific template.
% The file \textit{utils.cpp} (Appendix A.2 \autoref{code:Stat_dist}) shows how to achieve this.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Basic nested model specification}
\label{sec:nested}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In the context of HMMs (and other statistical models), nested  models or models subject to certain parameter restrictions are commonly used. For example, it may be necessary to fix some parameters because of biological or physical constraints.  {\tt{TMB}} can be instructed to treat selected parameters as constants, or impose equality constraints on a set of parameters.
For the practical implementation, it is noteworthy that such parameter restrictions should be imposed on the working parameters.
However, it is also easily possible to impose restrictions on a natural parameter (e.g.~$\lambda$), and then identify the corresponding restriction on the working parameter (i.e.~$\log(\lambda)$).\\
TIMO: lets chat about the following. Fixing a value in the transition probability matrix might be possible, but is clearly troublesome given the lack of a one-to-one correspondence with the natural parameters (one parameter of the natural TPM doesn't correspond to one parameter of the working TPM but multiple), so we fix a Poisson mean instead.

We illustrate a simple nested model specification by fixing $\lambda_1$ to one in our two-state Poisson HMM, the other parameter components correspond to the previous initial values. 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Get the previous values, and fix some}
\hlstd{fixed_par_lambda} \hlkwb{<-} \hlstd{lambda}
\hlstd{fixed_par_lambda[}\hlnum{1}\hlstd{]} \hlkwb{<-} \hlnum{1}
\end{alltt}
\end{kframe}
\end{knitrout}
We then transform these natural parameters into a set of working parameters.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Transform them into working parameters}
\hlstd{new_parameters} \hlkwb{<-} \hlkwd{pois.HMM.pn2pw}\hlstd{(}\hlkwc{m} \hlstd{= m,}
                                 \hlkwc{lambda} \hlstd{= fixed_par_lambda,}
                                 \hlkwc{gamma} \hlstd{= gamma)}
\end{alltt}
\end{kframe}
\end{knitrout}
For instructing {\tt{TMB}} to treat selected parameters as constants, the \texttt{map} argument of the \texttt{MakeADFun} has to be specified in addition to the usual arguments. The \texttt{map} argument is a list consisting factor-valued vectors which possess the same length as the working parameters and carry their names as well. 
The factor levels have to be unique for the regular parameters not subject to specific restrictions. If a parameter is fixed the corresponding entry of the \texttt{map} argument is filled with \texttt{NA}. To impose equality constraints (e.g.~$\lambda_1 = \lambda_2$), the corresponding factor level has to be identical for the concerned entries. In our example, this leads to:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{map} \hlkwb{<-} \hlkwd{list}\hlstd{(}\hlkwc{tlambda} \hlstd{=} \hlkwd{as.factor}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{NA}\hlstd{,} \hlnum{1}\hlstd{)),}
            \hlkwc{tgamma} \hlstd{=} \hlkwd{as.factor}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{2}\hlstd{,} \hlnum{3}\hlstd{)))}
\hlstd{fixed_par_obj_tmb} \hlkwb{<-} \hlkwd{MakeADFun}\hlstd{(TMB_data, new_parameters,}
                               \hlkwc{DLL} \hlstd{=} \hlstr{"poi_hmm"}\hlstd{,}
                               \hlkwc{silent} \hlstd{=} \hlnum{TRUE}\hlstd{,}
                               \hlkwc{map} \hlstd{= map)}
\end{alltt}
\end{kframe}
\end{knitrout}

% The estimates and standard errors vary after fixing the parameters (\autoref{table:nested-model}) and the standard errors for the fixed parameters are zero.
Estimation of the remaining model parameters and extraction of the results is achieved as before.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{fixed_par_mod_tmb} \hlkwb{<-} \hlkwd{nlminb}\hlstd{(}\hlkwc{start} \hlstd{= fixed_par_obj_tmb}\hlopt{$}\hlstd{par,}
                            \hlkwc{objective} \hlstd{= fixed_par_obj_tmb}\hlopt{$}\hlstd{fn,}
                            \hlkwc{gradient} \hlstd{= fixed_par_obj_tmb}\hlopt{$}\hlstd{gr,}
                            \hlkwc{hessian} \hlstd{= fixed_par_obj_tmb}\hlopt{$}\hlstd{he)}
\hlkwd{summary}\hlstd{(}\hlkwd{sdreport}\hlstd{(fixed_par_obj_tmb),} \hlstr{"report"}\hlstd{)}
\end{alltt}
\begin{verbatim}
##          Estimate Std. Error
## lambda 1.00000000 0.00000000
## lambda 5.50164872 0.30963641
## gamma  0.94561055 0.04791050
## gamma  0.02655944 0.02133283
## gamma  0.05438945 0.04791050
## gamma  0.97344056 0.02133283
## delta  0.32810136 0.22314460
## delta  0.67189864 0.22314460
\end{verbatim}
\end{kframe}
\end{knitrout}
Note that the standard error of $\lambda_1$ equals zero, because it is no longer considered a parameter and does not enter the optimization procedure.

% 
% <<estimates-nested-model, results = 'asis', echo = FALSE>>=
% adrep1 <- summary(sdreport(obj_tmb), "report")
% row_names_latex <- paste0(rep("$\\lambda_{", m), 1:m, "}$")
% for (gamma_idx in 1:m ^ 2) {
%   row_col_idx <- matrix.col.idx.to.rowcol(gamma_idx, m)
%   row_names_latex <- c(row_names_latex,
%                        paste0("$\\gamma_{", toString(row_col_idx), "}$"))
% }
% row_names_latex <- c(row_names_latex,
%                      paste0(rep("$\\delta_{", m), 1:m, "}$"))
% 
% mat1 <- matrix(adrep1, ncol = 2,
%                dimnames = list(row_names_latex, colnames(adrep1)))
% 
% adrep2 <- summary(sdreport(fixed_par_obj_tmb), "report")
% mat2 <- matrix(adrep2, ncol = 2,
%                dimnames = list(row_names_latex, colnames(adrep2)))
% 
% addtorow <- list()
% addtorow$pos <- list(- 1)
% addtorow$command <- paste0(paste0('& \\multicolumn{2}{c}{', c('Original model', 'Nested model'), '}',
%                                   collapse=''),
%                            '\\\\')
% 
% mat3 <- cbind(mat1, mat2)
% table <- xtable(mat3,
%                 caption = "2 state Poisson HMM before and after fixing $\\lambda_1$ to 1 using a nested model",
%                 label = "table:nested-model")
% print(table,
%       sanitize.rownames.function = identity,
%       add.to.row = addtorow)
% @
% 
% Since the nested model isn't the optimal model, the likelihood becomes worse, going from
% <<original-likelihood>>=
% # Original model negative log-likelihood
% mod_tmb$objective
% @
% 
% to
% <<decayed-likelihood>>=
% # Nested model negative log-likelihood
% fixed_par_mod_tmb$objective
% @
% 
% 
% Note that some inconsistencies can happen.
% 
% The stationary distribution is a vector of probabilities and should sum to 1.
% However, it doesn't behave as expected.
% <<1-diff-1>>=
% adrep <- summary(sdreport(obj_tmb), "report")
% estimate_delta <- adrep[rownames(adrep) == "delta", "Estimate"]
% sum(estimate_delta)
% sum(estimate_delta) == 1
% @
% 
% As noted on \citet[pp.~159-160]{zucchini}, ``with the specifications of $\bfdelta$, $\bgamma$ and $\bcp(x_t)$ given above, the row sums of $\bgamma$ will only approximately equal 1, and the components of the vector $\bfdelta$ will only approximately total 1. This can be remedied by scaling the vector $\bfdelta$ and each row of $\bgamma$ to total 1''.
% The code in this paper doesn't remedy this because it provides no benefit here.
% 
% % This is likely due to machine approximations when numbers far apart from each other interact together.
% % In R, a small number is not 0 but is treated as 0 when added to a much larger number.
% % <<0-diff-0>>=
% % 1e-100 == 0
% % (1 + 1e-100) == 1
% % @
% This can result in incoherent findings when checking equality between 2 numbers.
% Fortunately, no issue arose from this.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{State inference and forecasting}
\label{sec:state_inference}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

After estimating a HMM by the procedures illustrated in \autoref{sec:tmb_r}, it is possible to carry out a couple analyses that provide insight into the interpretation of the estimated model. These include, e.g., the so-called smoothing probabilities, which correspond to the probability of being in state $i$ at time $t$ for $i = 1,...,m$, $t=1,...,n$, given all observations. These probabilities can be obtained by
\begin{equation*}
P(C_t = i \vert X^{(n)} = x^{(n)}) = \frac{\alpha_t(i) \beta_t(i)}{L(\hat \btheta)},
\end{equation*}
where $\hat \btheta$ denotes the set of ML estimates. The derived smoothing probabilities then serve for determining the most probable state $i_t^*$ at time $t$ given the observations by
\begin{equation*}
i_t^* = \argmax_{i_t \in \{1, \ldots, m \}} P(C_t = i_t \vert X^{(n)} = x^{(n)}).
\end{equation*}
Furthermore, the Viterbi algorithm determines the overall most probable sequence of states $i_1^*, \ldots, i_T^*$, given the observations. This is achieved by evaluating
\begin{equation*}
(i_1^*, \ldots, i_n^*) = \argmax_{i_1, \ldots, i_n \in \{1, \ldots, m \}} P(C_1 = i_1, \ldots, C_n = i_n \vert X^{(n)} = x^{(n)}).
\end{equation*}
Other quantities of interest include the forecast distribution or $h$-step-ahead probabilities, which are obtained through
\begin{equation*}
P(X_{n+h} = x \vert X^{(n)} = x^{(n)}) = \frac{\balpha_n \bgamma^h \bcp(x) \bone'}{\balpha_n \bone'} = \bfphi_n \bgamma^h \bcp(x) \bone',
\end{equation*}
where $\bfphi_n = {\balpha_n} / {\balpha_n \bone'}$.\\
All the quantities shown above and the related algorithms for deriving them are described in detail in \citet[][Chapter 5]{zucchini}. In order to apply these algorithms, it is only necessary to extract the quantities required as input from a suitable \texttt{MakeADFun} object. Note that most algorithms rely on scaled versions of the forward- and backward-algorithm. This is illustrated in detail on GitHub.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Confidence intervals}
\label{sec:confint}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A common approach for deriving confidence intervals (CIs) for the estimated parameters of statistical models bases on finite-difference approximations of the Hessian. This technique is, however, not suited for most HMMs due to computational difficulties, as already pointed out by \citet{visser}. The same authors suggest likelihood profile CIs or bootstrap based CIs as potentially better alternatives. Despite the potentially high computational load, bootstrap based CIs have become an established method in the context of HMMs \citep{bulla, zucchini} and found widespread application by practitioners.\\
In this section we illustrate how CIs based on the Hessian, likelihood profiling, and the bootstrap can be efficiently implemented by integrating TMB.
This permits in particular to obtain Hessian based and likelihood profile based CIs at very low computational cost. For simplicity, we illustrate our procedures by means of the parameter $\lambda_2$ of our two-state Poisson HMM. We will further address the resulting CIs for $\bgamma$ and $\bflambda$ and performance-related aspects in \autoref{sec:application_datasets}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hessian based confidence intervals}
\label{sec:hessian}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Since the negative log-likelihood function of HMMs typically depends on the working parameters, evaluation of the Hessian in the optimum found by numerical optimization only serves for inference about the working parameters. From a practical perspective, however, inference about the natural parameters usually is of interest. As the Hessian $\nabla^2\log L(\{\hat{\bct}, \hat{\bfeta}\})$ refers to the working parameters $\{\bct, \bfeta\}$, the delta method is suitable to obtain an estimate of the covariance matrix of $\{\hat{\bgamma}, \hat{\bflambda}\}$ by
\begin{equation}
\Sigma_{\hat{\bgamma}, \hat{\bflambda}} = - \nabla g(\hat{\bct}, \hat{\bfeta})\left(\nabla^2\log L(\hat{\bct}, \hat{\bfeta})\right)^{-1}\nabla g(\hat{\bct}, \hat{\bfeta})^\prime,
\label{eq:deltamethod}
\end{equation}
with $\{\hat{\bgamma}, \hat{\bflambda}\} = g(\hat{\bct}, \hat{\bfeta})$ as defined in \autoref{sec:hmm_repar}.
From a user's perspective, it is highly convenient that the entire right-hand side of \autoref{eq:deltamethod} can be directly computed via automatic differentiation in \texttt{TMB}. Moreover, it is particularly noteworthy that the standard errors of derived parameters can be calculated by the delta-method similarly. For example, the stationary distribution $\bfdelta$ is a function of $\bgamma$ in our case, and \texttt{TMB} provides a straightforward way to obtain standard errors of $\bfdelta$. This is achieved by first defining $\bfdelta$ inside the C++ file  \texttt{poi\_hmm.cpp} (or, in our implementation, the related \texttt{utils.cpp}, which gathers auxiliary functions). Secondly, it is necessary to call \texttt{ADREPORT} on $\bfdelta$ within the \texttt{poi\_hmm.cpp} file. To display the resulting estimates and corresponding standard errors in \texttt{R}, one can rely on the command shown previously in \autoref{sec:tmb_r}.\\
Subsequently, Wald-type confidence intervals \citep{wald} follow in the usual manner. For example, the $(1 - \alpha) \%$ CI for $\lambda_1$ is given by $\lambda_1 \pm z_{1-\alpha/2} * \sigma_{\lambda_1}$ where $z_{x}$ is the $x$-percentile of the standard normal distribution, and $\sigma_{\lambda_1}$ is the standard error of $\lambda_1$ obtained via the delta method. This part is easily implemented in \texttt{R}. We illustrate the calculation of these CIs for our two-state Poisson HMM on GitHub.\\

Finally, note that the reliability of Wald-type CIs may suffer from a singular Fisher information matrix, which can occur for many different types of statistical models, including HMMs. This also jeopardizes the validity of AIC and BIC criteria. For further details on this topic, see e.g. \citet{drton}. TIMO: Insert the reference below here.\\

% Reference: Drton M, Plummer M. A Bayesian information criterion for singular models. Journal of the Royal Statistical Society: Series
% B (Statistical Methodology). 2017;79(2):323-380.

\bigskip

TIMO: All this on Github only\\

The $100(1-\alpha)\%$ confidence interval for
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{adrep} \hlkwb{<-} \hlkwd{summary}\hlstd{(}\hlkwd{sdreport}\hlstd{(obj_tmb),} \hlstr{"report"}\hlstd{)}

\hlcom{# Get the 97.5 percentile of the standard normal distribution}
\hlstd{q95_norm} \hlkwb{<-} \hlkwd{qnorm}\hlstd{(}\hlnum{1} \hlopt{-} \hlnum{0.05} \hlopt{/} \hlnum{2}\hlstd{)}

\hlcom{# Create the confidence interval}
\hlcom{# Extract the values}
\hlstd{estimates} \hlkwb{<-} \hlstd{adrep[,} \hlstr{"Estimate"}\hlstd{]}
\hlstd{std_errors} \hlkwb{<-} \hlstd{adrep[,} \hlstr{"Std. Error"}\hlstd{]}
\hlcom{# Create the bounds}
\hlstd{lower_bound} \hlkwb{<-} \hlstd{estimates} \hlopt{-} \hlstd{q95_norm} \hlopt{*} \hlstd{std_errors}
\hlstd{upper_bound} \hlkwb{<-} \hlstd{estimates} \hlopt{+} \hlstd{q95_norm} \hlopt{*} \hlstd{std_errors}
\hlcom{# Show the CI}
\hlkwd{cbind}\hlstd{(lower_bound, upper_bound)}
\end{alltt}
\begin{verbatim}
##        lower_bound upper_bound
## lambda  1.09235840  2.18046360
## lambda  4.90833475  6.15785677
## gamma   0.86406003  1.03554416
## gamma  -0.01501550  0.06685957
## gamma  -0.03554416  0.13593997
## gamma   0.93314043  1.01501550
## delta  -0.11135586  0.79243986
## delta   0.20756014  1.11135586
\end{verbatim}
\end{kframe}
\end{knitrout}

% For comparison, we have included the corresponding standard deviations resulting from replacing $\nabla^2\log L(\hat{\psi})$ in (\autoref{eq:deltamethod}) with traditional numerical approximations using the R functions nlm (nlmb) and fdHess (nlme).
% The difference when using these approximations are of order $1e-05$ or less, however one would still have to calculate $\nabla g(\hat{\psi})$ for these approximations to be useful.

%As mentioned earlier, for a larger amount of hidden states, {\tt{TMB}} may be unable to give some or any standard standard errors because some variables are close to their boundaries. In that situation, using a nested model might solve this issue. We refer the reader to \autoref{sec:nested} for information on how to specify a nested model using {\tt{TMB}}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Likelihood profile based confidence intervals}
\label{sec:likelihood}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The Hessian based CIs presented above rely on asymptotic normality of the ML estimator. Properties of the ML estimator may, however, change in small samples. Moreover, symmetric CIs may not be suitable if the ML estimator lies close to a boundary of the parameter space. This occurs, e.g., when states are highly persistent, which leads to entries close to one in the TPM. An alternative approach to construct CIs bases on the so-called profile likelihood \citep[see, e.g.,][]{venzon, meeker}, which has also shown a satisfactory performance in the context of HMMs \citep{visser}.\\
In the following, we illustrate the principle of likelihood profile based CIs by the example of the parameter $\lambda_2$ in our two-state Poisson HMM. The underlying basic idea is to identify those values of our parameter of interest $\lambda_2$ in the neighborhood of $\hat \lambda_2$ that lead to a significant change in the log-likelihood, whereby the other parameters (i.e.~$\bgamma$, $\lambda_1$) are considered nuisance parameters \citep{meeker}. The "term nuisance parameters" means that these parameters need to be re-estimated (by maximizing the likelihood) for any fixed value of $\lambda_2$ different to $\hat \lambda_2$. That is, the profile likelihood of $\lambda_2$ is defined as
\begin{equation*}
L_p(\lambda_2) = \max_{\bgamma, \lambda_1} L(\bgamma, \bflambda)
\end{equation*}
\\
In order to construct profile likelihood based CIs, let $\{\hat{\bgamma}, \hat{\bflambda}\}$ denote the ML estimate for our HMM computed as described in \autoref{sec:tmb_r}. Evaluation of the log-likelihood function in this point results in the value $\log L(\{\hat{\bgamma}, \hat{\bfdelta}\})$. The deviation of the likelihood of the ML estimate and the profile likelihood in the point $\lambda_2^p$ is then captured by the following likelihood ratio:
\begin{equation}
R_p(\lambda_2) = -2 \left[ \log(L_p(\lambda_2)) - \log(L(\hat{\bgamma}, \hat{\bflambda}))\right]
\label{eq:profileLR}
\end{equation}
As described above, the log-likelihood $\log(L_p(\lambda_2))$ results from re-estimating the two-state HMM with fixed parameter $\lambda_2$. Therefore, this model effectively corresponds to a nested model of the full model with ML estimate $\hat{\bgamma}, \hat{\bflambda}$. Consequently, $R_p$ asymptotically follows a $\chi^{2}$ distribution with one degree of freedom - the difference in degrees of freedom between the two models. Based on this, a CI for $\lambda_2$ can be derived by evaluating $R_p$ at many different values of $\lambda_2^p$ and determining when the resulting value of $R_p$ becomes "too extreme". That is, for a given $\alpha$, one needs to calculate the $1-\alpha$ quantile of the $\chi^{2}_{1}$ distribution (e.g., 3.841 for $\alpha = 5\%$). The CI at level $1-\alpha$ for the parameter $\lambda_2$ is then given by 
\begin{equation}
\left\{\lambda_2: R_p(\lambda_2)  < \chi^{2}_{1, (1-\alpha)}\right\}
\label{eq:profileCI}
\end{equation}

\bigskip

% OLD - OUT?\\
% 
% Next, we consider evaluating uncertainty using likelihood-profiles.
% 
% Let $\eta$ be single parameter in a model with parameters $(\eta, \theta)$ and likelihood $L(\eta, \theta)$, and let $L_p(\eta)$ be the profile likelihood defined by $L_p(\eta)=\max_{\theta} L(\eta, \theta)$.
% Then a likelihood-based CI for $\eta$ is given by
% \begin{equation}
% \left\{\eta: 2\log(\frac{L(\hat{\eta},\hat{\theta})}{L_p(\eta)} < \chi^{2}_{1, (1-\alpha)}\right\}
% \label{eq:profileCI}
% \end{equation}
% where $\chi^{2}_{1, (1-\alpha)}$ is the $1-\alpha$ quantile of a $\chi^2$ distribution with $1$ degree of freedom.
% {\tt{TMB}} allows for very efficient computation of both the profile likelihood $L(\eta)$ and the CI given by \autoref{eq:profileCI}, and this has been used to produce \autoref{table:lamb_estimates_std_errors} and \autoref{table:simul_estimates_std_errors} which display the profile log-likelihood and the corresponding likelihood-based CI's in the model for the lamb and the simulated dataset.

For simplicity, the principles of likelihood profiling shown above rely on the natural parameters. Our nll function is, however, parameterized in terms of and optimized with respect to the working parameters. In practice, this aspect is easy to deal with. Once a profile CI for the working parameter (here $\eta_2$) has been obtained following the procedure above, the corresponding CI for the natural parameter $\lambda_2$ results directly from transforming the upper and lower boundary of the CI for $\eta_2$ by the one-to-one transformation $\lambda_2 = \exp(\eta_2)$. Basis for this transformation is the the invariance property of ML estimation, as described e.g. by \citet[Theorem 7.2.10 on p. ~320]{casella}TIMO: insert a reference to Casalla Berger Section?.   

{\tt{TMB}} provides an easy way to profiling through the function {\tt{tmbprofile}}, which requires several inputs. First, the well-known \texttt{MakeADFun} object called \texttt{obj\_tmb} from our two-state Poisson HMM. Secondly, the position of the (working) parameter to be profiled via the \texttt{name} argument. This position refers to the position in the parameter vector \texttt{obj\_tmb\$par}. Moreover, here the optional \texttt{trace} argument indicates how much information on the optimization is displayed. The following commands permit to profile the second working parameter $\eta_2 = \log(\lambda_2)$. 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{profile} \hlkwb{<-} \hlkwd{tmbprofile}\hlstd{(}\hlkwc{obj} \hlstd{= obj_tmb,}
                      \hlkwc{name} \hlstd{=} \hlnum{2}\hlstd{,}
                      \hlkwc{trace} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\hlkwd{plot}\hlstd{(profile,} \hlkwc{level} \hlstd{=} \hlnum{0.95}\hlstd{)}
\end{alltt}
\end{kframe}\begin{figure}[htb]

{\centering \includegraphics[width=\maxwidth]{figure/profile_plot-1} 

}

\caption[Profile likelihood]{Profile likelihood}\label{fig:profile_plot}
\end{figure}


\end{knitrout}
TIMO: we need this figure in an includefigure environment such that we can label / refer to it\\
JAN: look at tex file here
% \autoref{fig:profile_plot}

Furthermore, \autoref{fig:profile_plot} obtained via the \texttt{plot} function shows the resulting profile nll as a function of the working parameter $\eta_2$. The vertical and horizontal lines correspond to the boundaries of the confidence interval and the critical value of the nll derived from \autoref{eq:profileCI}, respectively. The CI for $\eta_2$ can directly be extracted via the function \texttt{confint}:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Confidence interval of tlambda}
\hlkwd{confint}\hlstd{(profile,} \hlkwc{level} \hlstd{=} \hlnum{0.95}\hlstd{)}
\end{alltt}
\begin{verbatim}
##            lower    upper
## tlambda 1.593141 1.820641
\end{verbatim}
\end{kframe}
\end{knitrout}
The corresponding CI for $\lambda_2$ the follows from:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Confidence interval of lambda}
\hlkwd{exp}\hlstd{(}\hlkwd{confint}\hlstd{(profile,} \hlkwc{level} \hlstd{=} \hlnum{0.95}\hlstd{))}
\end{alltt}
\begin{verbatim}
##            lower    upper
## tlambda 4.919178 6.175815
\end{verbatim}
\end{kframe}
\end{knitrout}
While simple linear combinations of variables can be profiled through the argument \texttt{lincomb} in the \texttt{tmbprofile} function, this is not possible for more complex functions of the parameters. This includes the stationary distribution $\bfdelta$, for which CIs cannot be obtained by this method.\\
Last, note that the function \texttt{tmbprofile} carries out several optimization procedures internally for calculating profile CIs. If this approach fails, or prefers a specific optmimization routine, the necessary steps for profiling can also be implemented by the user. To do so, it would be - roughly speaking - necessary to compute $R_p(\eta_2)$ for a sufficient number of $\eta_2$ values to achieve the desired precision.\\

TIMO: script on Github showing how to do it for the elements of the TPM as 2nd section / additional code - check this part with Geir


%GEIR: REFORMULATE? In addition, this method can sometimes give NA values, usually when a ML estimate is close to a boundary, but not only. For example, the first working parameter \texttt{log\_lambda} is pretty low ($round(sdreport(obj_tmb)$par.fixed[m ^ 2], 2)$). It becomes too difficult to profile the likelihood so the function fails to provide a meaningful confidence interval.\\

%Another important issue is about profiling with a univariate model. With only 1 (hidden) state, a Poisson HMM becomes a univariate Poisson regression model. Therefore, the profile becomes a plot of the likelihood when the Poisson mean varies. However, \texttt{tmbprofile} fails to provide a confidence interval. The reason is likely that it tries to optimize the likelihood despite the lack of parameters to change, and therefore fails.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bootstrap based confidence intervals}
\label{sec:bootstrapping}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The last approach for deriving CIs is the bootstrap, which is frequently applied by many practitioner. \citet{efron} describe the underlying concepts of the bootstrap in their seminal manuscript. Many different bootstrap techniques have evolved since then, leading to an extensive treatment of this subject in the scientific literature.\\
A thorough overview of this subject would go beyond the scope of this paper. As pointed out by \citet{hardle}, the so-called parametric bootstrap is suitable in the context of time series models. For further details on the bootstrap for HMMs including the implementation of a parametric bootstrap, we refer to \citet[][Ch.~3, pp.~56-60]{zucchini}.\\ 
Basically all versions of the bootstrap have in common that some kind of re-sampling procedure needs to be carried out first. Secondly, the model of interest is re-estimated for each of the re-sampled data sets. A natural way to accelerate the second part consists in the use of \texttt{tmb} for the model estimation by means of the procedures presented in \autoref{sec:tmb_r}. Our GitHub page contains a detailed example illustrating the implementation of a parametric percentile bootstrap for our 2-state Poisson HMM.


\bigskip

All the following to Github\\
JAN: What came here is now commented out, and on GitHub

TIMO: parametric BS, not non-parametric BS. right?!\\


% From the parameters' ML estimates, we generate new data and re-estimate the parameters BOOTSTRAP_SAMPLES times.
% From that list of new estimates we can get the 2.5th and 97.5th percentiles and get 95\% confidence intervals for the parameters.\\
% 
% We show below how we get confidence intervals using bootstrap, based on the 2 state Poisson HMM estimates from above. TIMO: we had a 2-state above, right?
% 
% \begin{enumerate}
% 
% \item
% First, we need a function to generate random data from a HMM.
% <<pois.HMM.generate_sample>>=
% @
% 
% \item
% Then, when the model is estimated each time, we don't impose an order for the states.
% This can lead to the label switching problem, where states aren't ordered the same way in each model.
% To address this, we re-ordered the states by ascending Poisson means.\\
% Sorting the means is pretty straightforward.
% Re-ordering the TPM is a little trickier.
% To do so, we took the permutations of the states given by the sorted Poisson means, and permuted each row index and column index to its new value.\\
% The function we used is
% <<pois.HMM.label.order>>=
% @
% 
% Let's show an example to understand the process.
% For readability, the TPM is filled with row and column indexes instead of probabilities.
% <<relabel>>=
% lambda <- c(30, 10, 20)
% gamma <- matrix(c(11, 12, 13,
%                   21, 22, 23,
%                   31, 32, 33), byrow = TRUE, ncol = 3)
% pois.HMM.label.order(m = 3, lambda, gamma)
% @
% State 1 has been relabeled state 3, state 2 became state 1, and state 3 became state 2.
% 
% \item
% Bootstrap code
% <<bootstrap-code>>=
% bootstrap_estimates <- data.frame()
% DATA_SIZE <- length(lamb_data)
% # Set how many parametric bootstrap samples we create
% BOOTSTRAP_SAMPLES <- 1000
% 
% # ML parameters
% ML_working_estimates <- obj_tmb$env$last.par.best
% ML_natural_estimates <- obj_tmb$report(ML_working_estimates)
% gamma <- ML_natural_estimates$gamma
% lambda <- ML_natural_estimates$lambda
% delta <- ML_natural_estimates$delta
% 
% # Parameters for TMB
% cols <- names(ML_working_estimates)
% tgamma <- ML_working_estimates[cols == "tgamma"]
% tlambda <- ML_working_estimates[cols == "tlambda"]
% ML_TMB_parameters <- list(tlambda = tlambda,
%                           tgamma = tgamma)
% 
% params_names <- c("lambda", "gamma", "delta")
% # counter1 <- counter2 <- counter3 <- counter_threshold_gamma <- counter_threshold_lambda <- 0
% # conv1 <- c()
% # conv2 <- data.frame(message = character(),
% #                     min_g = numeric(),
% #                     max_g = numeric(),
% #                     min_l = numeric(),
% #                     max_l = numeric())
% # # nll <- c()
% # idx_sample <- 1
% # myAIC <- myBIC <- data.frame()
% for (idx_sample in 1:BOOTSTRAP_SAMPLES) {
%   # Loop as long as there is an issue with nlminb
%   repeat {
%     #simulate the data
%     bootstrap_data <- pois.HMM.generate_sample(DATA_SIZE,
%                                                list(m = m,
%                                                     lambda = lambda,
%                                                     gamma = gamma,
%                                                     delta = delta))
%     
%     # # TRUE STATE FILTERING
%     # tab <- as.numeric(table(bootstrap_data$state)) / DATA_SIZE
%     # if (any(tab >= 0.99)) {
%     #   counter = counter + 1
%     #   next
%     # }
%     
%     # Parameters for TMB
%     TMB_data_bootstrap <- list(x = bootstrap_data$data, m = m)
%     
%     # Estimate the parameters
%     obj <- MakeADFun(TMB_data_bootstrap,
%                      ML_TMB_parameters,
%                      DLL = "poi_hmm",
%                      silent = TRUE)
%     
%     # Using tryCatch to break the loop if an error happens
%     # interrupts the outer (for) loop, not the inner one (repeat)
%     mod_bootstrap <- NULL
%     try(mod_bootstrap <- nlminb(start = obj$par, objective = obj$fn,
%                                 gradient = obj$gr, hessian = obj$he),
%         silent = TRUE)
%     
%     # If nlminb doesn't reach any result, retry
%     if (is.null(mod_bootstrap)) {
%       # conv1 <- c(conv1, mod_bootstrap$convergence)
%       # counter1 <- counter1 + 1
%       next
%     }
%     # If nlminb doesn't converge successfully, retry
%     if (mod_bootstrap$convergence != 0) {
%       # working_parameters <- obj$env$last.par.best
%       # natural_parameters <- obj$report(working_parameters)
%       # 
%       # g <- natural_parameters$gamma
%       # conv2[counter2 + 1, ] <- cbind(mod_bootstrap$message,
%       #                                round(min(g), 4),
%       #                                round(max(g), 4),
%       #                                round(min(l), 4),
%       #                                round(max(l), 4))
%       # if(min(l) <= 1e-3) {
%       #   print(min(l), digits = 10)
%       # }
%       # counter2 <- counter2 + 1
%       next
%     }
%     
%     # # SMOOTHING FILTERING
%     # smoot <- HMM.decode(obj)$ldecode
%     # tab <- as.numeric(table(smoot)) / DATA_SIZE
%     # if (any(tab >= 0.99)) {
%     #   counter = counter + 1
%     #   next
%     # }
%     
%     working_parameters <- obj$env$last.par.best
%     natural_parameters <- obj$report(working_parameters)
% 
%     l <- natural_parameters$lambda
%     g <- natural_parameters$gamma
%     d <- natural_parameters$delta
%     
%     # # VITERBI FILTERING
%     # vit <- pois.HMM.viterbi(bootstrap_data$sample, m, lambda = l, gamma = g, delta = d)
%     # tab <- as.numeric(table(vit)) / DATA_SIZE
%     # if (any(tab >= 0.99)) {
%     #   counter = counter + 1
%     #   next
%     # }
%     
% 
%     # Label switching
%     natural_parameters <- pois.HMM.label.order(m = m, lambda = l,
%                                                gamma = g, delta = d)
% 
%     # If some parameters are NA for some reason, retry
%     if (anyNA(natural_parameters[params_names], recursive = TRUE)) {
%       # counter3 <- counter3 + 1
%       next
%     }
%     
%     # if(max(g) >= 0.999999) {
%     #   counter_threshold_gamma <- counter_threshold_gamma + 1
%     # }    
%     # if(min(l) <= 1e-3) {
%     #   counter_threshold_lambda <- counter_threshold_lambda + 1
%     # }
%     
%     # # ATTEMPT ONE STATE, AIC AND BIC
%     # lambda1state <- mean(bootstrap_data$data)
%     # gamma1state <- matrix(1, nrow = 1, ncol = 1)
%     # # parvect <- pois.HMM.pn2pw(m = 1, lambda = lambda1state, gamma = gamma1state)
%     # # mllk <- pois.HMM.mllk(parvect, x_alias = bootstrap_data$data, m_alias = 1)
%     # mllk <- -log(prod(dpois(bootstrap_data$data, lambda1state)))
%     # # np <- length(unlist(parvect))
%     # np <- 1
%     # myAIC[idx_sample, "1state"] <- 2 * (mllk + np)
%     # n <- sum(!is.na(bootstrap_data$data))
%     # myBIC[idx_sample, "1state"] <- 2 * mllk + np * log(n)
%     
%     
%     # # 2 STATE AIC AND BIC
%     # mllk <- mod_bootstrap$objective
%     # np <- length(mod_bootstrap$par)
%     # myAIC[idx_sample, "2state"] <- 2 * (mllk + np)
%     # myBIC[idx_sample, "2state"] <- 2 * mllk + np * log(n)
%     
%     # LOGLIKELIHOOD VALUES
%     # nll <- c(nll, mod_bootstrap$objective)
%     # nll <- mod_bootstrap$objective
%     # if (nll <= 142.1408 | nll >= 215.7791) {
%     #   counter = counter + 1
%     #   next
%     # }
%     # if (nll <= 146.7423 | nll >= 208.3097) {
%     #   counter = counter + 1
%     #   next
%     # }
%     
%     # If everything went well, end the "repeat" loop
%     break
%   }
%   # The values from gamma are taken columnwise
%   natural_parameters <- unlist(natural_parameters[params_names])
%   len_par <- length(natural_parameters)
%   bootstrap_estimates[idx_sample, 1:len_par] <- natural_parameters
% }
% 
% # Lower and upper (2.5% and 97.5%) bounds
% q <- apply(bootstrap_estimates, 2, function(par_estimate) {
%   quantile(par_estimate, probs = c(0.025, 0.975))
% })
% 
% params_names <- paste0(rep("lambda", m), 1:m)
% # Get row and column indexes for gamma instead of the default
% # columnwise index: the default indexes are 1:m for the 1st column,
% # then (m + 1):(2 * m) for the 2nd, etc...
% for (gamma_idx in 1:m ^ 2) {
%   row <- (gamma_idx - 1) %% m + 1
%   col <- (gamma_idx - 1) %/% m + 1
%   row_col_idx <- c(row, col)
%   params_names <- c(params_names,
%                     paste0("gamma",
%                            paste0(row_col_idx, collapse = "")))
% }
% params_names <- c(params_names,
%                   paste0(rep("delta", m), 1:m))
% bootstrap_CI <- data.frame("Parameter" = params_names,
%                            "Estimate" = c(lambda, gamma, delta),
%                            "Lower bound" = q[1, ],
%                            "Upper bound" = q[2, ])
% print(bootstrap_CI, row.names = FALSE)
% # print(counter)
% # plot(x = 1:BOOTSTRAP_SAMPLES, y = nll)
% # hist(nll)
% # apply(bootstrap_estimates[, 1:2], 2, function(est) {
% #   hist(est, breaks = "FD")
% # })
% # print(paste("Lack of convergence/missing results", counter2, counter1, "times"))
% # myAIC[, "pref"] <- ifelse(myAIC[, "1state"] <= myAIC[, "2state"], 1, 2)
% # myBIC[, "pref"] <- ifelse(myBIC[, "1state"] <= myBIC[, "2state"], 1, 2)
% # print(paste("AIC prefers 1 state", sum(myAIC[, "pref"] == 1), "times, and 2 states", sum(myAIC[, "pref"] == 2), "times"))
% # print(paste("BIC prefers 1 state", sum(myBIC[, "pref"] == 1), "times, and 2 states", sum(myBIC[, "pref"] == 2), "times"))
% # 
% # 
% # print(paste(counter1, "don't reach any result"))
% # # print(paste("conv codes:", conv1))
% # print(paste(counter2, "don't converge successfully"))
% # # print(paste("conv codes:", conv2))
% # print(paste(counter3, "parameters are NA"))
% # # print(paste("gamma over the threshold", counter_threshold_gamma, "times"))
% # print(paste("lambda under the threshold", counter_threshold_lambda, "times"))
% @
% 
% It should be noted that some estimates can be very large or very small.
% One possible reason is that the randomly generated bootstrap sample might contain long chains of the same values, thus causing a probability in the TPM to be almost 0 or almost 1.
% However, a large number of bootstrap samples lowers that risk since we leave out 5\% of the most extreme values when computing the 95\% CI.
% 
% TIMO: REALLY? So the biologist / MD determines which samples are not convenient, leaves them out, and cites us. Patients might die...
% 
% \end{enumerate}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Application to different data sets}
\label{sec:application_datasets}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The aim of this section is do demonstrate the performance of \texttt{tmb} by means of a couple of practical examples that differ in terms of the number of observations and model complexity. These examples include the TYT data shown above, a data set of fetal lamb movements and one of hospital visits, and simulated data sets. For the performance comparisons, focus lies on computational speed and the reliability of confidence intervals. The R-scripts necessary for this section may serve interested users for investigating their own HMM-setting, and are all available on GitHub.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{TYT data}
\label{sec:tyt_data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We begin by investigating the speed of five approaches for parameter estimation: one without the usage of {\tt{TMB}}, and four with {\tt{TMB}}. In the following, $DM$ denotes direct maximization of the log-likelihood through the optimization function \texttt{nlminb} without {\tt{TMB}}. Furthermore, $TMB$, $TMB_H$, $TMB_G$, and $TMB_{GH}$ denote direct maximization with {\tt{TMB}} without making use of the exact gradient and Hessian that {\tt{TMB}} can provide, with the Hessian, with the gradient, and with both gradient and Hessian, respectively.\\
As preliminary reliability check of our IT infrastructure and setup, we timed the fitting of our 2-state HMM to the TYT data with the help of the {\tt{microbenchmark}} package. For this data set, all five approached converged to the same maximum. \autoref{table:speed-consistency-tinn} shows the resulting average time required for the parameter estimation and the number of iterations needed by each approach, measured over $100$ replications. The results show that the use of {\tt{TMB}} significantly accelerates parameter estimation in comparison with $DM$. The strongest acceleration is achieved by $TMB_G$, underlining the benefit of using the gradient provided by {\tt{TMB}}. Moreover, $TMB_{GH}$ requires less iterations than the other approaches. However, the approximation of the Hessian seems to increase the computational burden.\\ [1ex]     

TIMO: did all methods converge to the same model estimates? Or did $TMB_GH$ need less iterations because it terminated with an error? It would be good to have a table here similar to Table 7 from the next section, (with the parameter estimates)\\[1ex]
JAN: They all converge successfully. However, the estimates vary in the order of $10^{-6}$. The nll's are equal.

{\tt{TMB}} doesn't affect the optimization results, as can be seen in the estimates of a 2 state Poisson HMM on the tinnitus dataset (\autoref{table:2-state-tinn-estimates}).

% latex table generated in R 4.0.5 by xtable 1.8-4 package
% Sat May 01 02:46:23 2021
\begin{table}[ht]
\centering
\begin{tabular}{rrrrrrr}
  \hline
 & \textit{${DM}$} & \textit{${TMB}$} & \textit{${TMB_G}$} & \textit{${TMB_H}$} & \textit{${TMB_{GH}}$} & NA \\ 
  \hline
$\lambda_{1}$ & 1.64 & 1.64 & 1.64 & 1.64 & 1.64 &  \\ 
  $\lambda_{2}$ & 5.53 & 5.53 & 5.53 & 5.53 & 5.53 &  \\ 
  $\gamma_{1, 1}$ & 0.95 & 0.95 & 0.95 & 0.95 & 0.95 &  \\ 
  $\gamma_{2, 1}$ & 0.03 & 0.03 & 0.03 & 0.03 & 0.03 &  \\ 
  $\gamma_{1, 2}$ & 0.05 & 0.05 & 0.05 & 0.05 & 0.05 &  \\ 
  $\gamma_{2, 2}$ & 0.97 & 0.97 & 0.97 & 0.97 & 0.97 &  \\ 
  $\delta_{1}$ & 0.34 & 0.34 & 0.34 & 0.34 & 0.34 &  \\ 
  $\delta_{2}$ & 0.66 & 0.66 & 0.66 & 0.66 & 0.66 &  \\ 
   \hline
\end{tabular}
\caption{Estimates of 2 state Poisson HMM with and without using TMB, estimated on the tinnitus dataset.} 
\label{table:2-state-tinn-estimates}
\end{table}


TIMO: in the table \autoref{table:speed-consistency-tinn}, can you remove the column with m and the 2? moreover, instead of the "2" put two entries with "Time (ms)" and "Iterations"\\[1ex]

% latex table generated in R 4.0.5 by xtable 1.8-4 package
% Sat May 01 02:46:23 2021
\begin{table}[ht]
\centering
\begin{tabular}{llllll}
  \hline
 & \textit{${DM}$} & \textit{${TMB}$} & \textit{${TMB_G}$} & \textit{${TMB_H}$} & \textit{${TMB_{GH}}$} \\ 
  \hline
Time & 23.9 (23.5, 24.4) & 1.9 (1.77, 2.02) & 0.868 (0.857, 0.88) & 1.78 (1.77, 1.8) & 2.39 (2.19, 2.59) \\ 
  Iterations & 13 & 13 & 13 & 13 & 7 \\ 
   \hline
\end{tabular}
\caption{Average duration (in milliseconds) together with 95\% CI and number of iterations required for fitting a 2-state Poisson HMM to the TYT data. The CIs are of Wald-type and base on the standard error of the mean derived from 100 replications.} 
\label{table:speed-consistency-tinn}
\end{table}




%paste0("Average duration (in milliseconds) together with 95\\% CI and number of iterations required for fitting a 2-state Poisson HMM to the TYT data. The CIs are of Wald-type and base on the the standard error of the mean derived from", $CONSISTENCY_BENCHMARK_TINN$, "replications.")



% \autoref{table:notation} summarizes the notation.
% 
% \begin{table}[h!]
% \begin{center}
% \caption{Naming of TMB parameters}
% \label{table:notation}
% \begin{tabular}{l|cccc}
% \hline
%                         & TMB1  & TMB2  & TMB3  & TMB4\\
% \hline
% Exact gradient is used  & No    & No    & Yes   & Yes\\
% Exact hessian is used   & No    & Yes   & No    & Yes\\
% \hline
% \end{tabular}
% \end{center}
% \end{table}

Next, we verified the reproducibility of the acceleration by {\tt{TMB}} in a parametric bootstrap setting. More specifically, we simulated $100$ data sets from the model estimated on the TYT data. Then, we re-estimated the same model by our five approaches and derived acceleration ratios (with $DM$ as reference approach) and their corresponding percentile CIs. As shown in \autoref{table:speed-tinn},  all acceleration ratios take values significantly larger than one, whether the gradient and Hessian are passed from {\tt{TMB}} or not. In addition, the findings from the single TYT data set are confirmed with $TMB_G$ providing the strongest acceleration.    

TIMO: can you also add the number of iterations in this table?

JAN: done

% latex table generated in R 4.0.5 by xtable 1.8-4 package
% Sat May 01 02:46:23 2021
\begin{table}[ht]
\centering
\begin{tabular}{lllll}
  \hline
 & \textit{${TMB}$} & \textit{${TMB_G}$} & \textit{${TMB_H}$} & \textit{${TMB_{GH}}$} \\ 
  \hline
Time & 14.5 (12.8, 15.7) & 28.7 (25.3, 31.6) & 14.4 (12.8, 15.7) & 11.4 (9.21, 14.6) \\ 
  Iterations & 15.6 (13.5, 19) & 15.6 (13.5, 19) & 15.6 (13.5, 19) & 6.85 (6, 9) \\ 
   \hline
\end{tabular}
\caption{Acceleration ratios together with 95\% CIs when using {\tt TMB} in a bootstrap setting with 100 replications} 
\label{table:speed-tinn}
\end{table}


TIMO: In the table caption you previously wrote "Acceleration for m state Poisson HMM parameter estimation, estimated on the tinnitus dataset". This somehow suggests that the same data set was used all the time. But you simulated data sets from the "true" parameters estimated on the TYT data set, right?\\[1ex]

JAN: Yes, \autoref{table:speed-tinn} is based on simulated datasets. \autoref{table:speed-consistency-tinn} is based only on the tinnitus dataset


For some bootstrap samples, one or several of the estimation algorithms did not converge properly. In such cases, we discarded the results, generated an additional bootstrap sample, and re-ran the parameter estimation. Convergence problems mostly occurred due to $TMB$ and $TMB_H$ failing. We therefore recommend to pass at least the gradient when optimizing with {\tt{TMB}} for increased stability of the parameter estimation with {\tt nlminb}.\\
As last check of the acceleration obtained by {\tt{TMB}}, we also timed the computation of log-likelihood alone. To do so, we used the same parametric bootstrap-type setting as the one described above with $100$ replications. The acceleration factor of {\tt{TMB}} compared to conventional {\tt R} code was estimated as 12.5 (8.12, 16.2).



TIMO: I think \autoref{table:speed-tinn-nll} can be removed - it does not make sense to measure the likelihood acceleration for different estimation procedures, because the log-likelihood computation should always be the exact same procedure, right? Or did nlminb play any here I believe it should not play any role at all. What would be interesting to just report how long computation of the logL in TMB compared to R takes. I put some figures hard-copied in above, maybe you find a way to do this dynamically.

JAN: the likelihood acceleration is measured on the 100 simulated datasets (the same ones used for the estimation acceleration TMB/no\_TMB, and for the estimation duration nlm/nlminb/marqLevAlg/BFGS/L-BFGS-B/hjn).
How long the computation of the logL in TMB compared to no\_TMB is, is shown in that table.


% latex table generated in R 4.0.5 by xtable 1.8-4 package
% Sat May 01 02:46:23 2021
\begin{table}[ht]
\centering
\begin{tabular}{rllll}
  \hline
m & \textit{${TMB}$} & \textit{${TMB_G}$} & \textit{${TMB_H}$} & \textit{${TMB_{GH}}$} \\ 
  \hline
2 & 11.2 (7.36, 14.7) & 10.7 (6.52, 13.8) & 10.6 (6.61, 13.8) & 10.6 (6.76, 14.3) \\ 
   \hline
\end{tabular}
\caption{Acceleration when using TMB for m state Poisson HMM negative log-likelihood calculation, estimated on the tinnitus dataset. Each procedure was repeated and timed once on 100 sample datasets. With 2 hidden states, $TMB_G$ accelerated the estimation by an average factor of 10, with a 95\% quantile confidence interval of 7 (6.52,} 
\label{table:speed-tinn-nll}
\end{table}


TIMO: which estimation method did you use for the three CIs, and are they of 95\% level? Always $TMB_{GH}$? For the bootstrap CIs, also $TMB_{GH}$?, and with how many repetitions / bootstrap samples? And: there are still negative values for lower bounds of TMB, these should be zero.\\
Moreover, which simulation approach did you use for the coverage probabilities? Is the same bootstrap-type setting as above used (this would maybe make sense)? If so, how many replications?\\
Last, in the Table it would be good to get percent values for the coverage probabilities, with one digit after the comma. And: the order of the methods should be the same as our sections, ie TMB, profiling, bootstrap.\\[1ex]

JAN: For profile CI, I used TMB::tmbprofile and confint. For bootstrap, percentile CI. For TMB CI, estimate +/- 1.96 * std\_error.\\
Always 95\%.\\
Always $TMB_{GH}$.\\
The negative values are expected for estimates close to 0, because the interval is Wald-type. Similarly, $\gamma_{2,2}$ has an upper bound of 1.04, above 1. Is it normal to manually prevent impossible values? We can also switch to percentile CIs to prevent this.\\
I used the same technique for all simulations: generate a dataset as long as TMB or marqLevAlg fail to converge,\\
BOOTSTRAP\_SAMPLES=500 replications for bootstrap CIs\\
COVERAGE\_SAMPLES=200 replications for coverage CIs\\
BENCHMARK\_SAMPLES=100 replications for the benchmarking.\\


Last, we investigate CIs obtained for the TYT data by the three different methods described in \autoref{table:tinn_cis}, $TMB_{GH}$ served as estimation approach. The columns to the left of \autoref{table:tinn_estimates_std_errors} show the parameter estimates and 95\% CIs obtained via the Hessian provided by {\tt TMB}, likelihood profiling, and bootstrapping. For this data set, no major differences between the different CIs are visible. Furthermore, in we assessed the accuracy of the different CIs by coverage probabilities. For calculating these coverage probabilities, we used a Monte Carlo setting similar the one described above with 100 replications. The results, shown on the right of \autoref{table:tinn_cis} indicate that all methods provide comparably reliable CI estimates. Only the coverage probabilities for the parameters $\delta_1$ and $\delta_2$ take values deviating considerably from the 95\% level.\\[1ex]



TIMO: You wrote this "Instead of showing the standard error, we display the lower and upper bounds of the confidence intervals. This is due to profiling sometimes failing to provide both bounds of the interval."\\
What did you do when profiling did not provide a lower bound of an interval? Exclude the sample / generate another sample / simply use the values that were not missing?\\
Also, I think we do not need the columns with "m".

JAN: I used only the values that were not missing. For the stationary dist, it is complicated to compute the profile CI, so I did not do it.



% latex table generated in R 4.0.5 by xtable 1.8-4 package
% Sat May 01 02:46:23 2021
\begin{table}[ht]
\centering
\begin{tabular}{ccccccccccc}
  && \multicolumn{2}{c}{{\tt TMB} CI}& \multicolumn{2}{c}{Profile CI}& \multicolumn{2}{c}{Bootstrap CI}& \multicolumn{3}{c}{Coverage prob.}\\ \hline
Parameter & Estimate & L. & U. & L. & U. & L. & U. & {\tt TMB} & Profile & Bootstrap \\ 
  \hline
$\lambda_{1}$ & 1.64 & 1.09 & 2.18 & 1.15 & 2.23 & 1.29 & 2.12 & 0.96 & 0.96 & 0.99 \\ 
  $\lambda_{2}$ & 5.53 & 4.91 & 6.16 & 4.92 & 6.18 & 5.18 & 5.97 & 0.97 & 0.97 & 0.97 \\ 
  $\gamma_{1, 1}$ & 0.95 & 0.86 & 1.04 & 0.82 & 1.00 & 0.78 & 0.98 & 0.96 & 0.94 & 0.94 \\ 
  $\gamma_{2, 1}$ & 0.03 & -0.02 & 0.07 & 0.00 & 0.09 & 0.01 & 0.07 & 0.93 & 0.95 & 0.95 \\ 
  $\gamma_{1, 2}$ & 0.05 & -0.04 & 0.14 & 0.00 & 0.18 & 0.02 & 0.22 & 0.96 & 0.94 & 0.94 \\ 
  $\gamma_{2, 2}$ & 0.97 & 0.93 & 1.02 & 0.91 & 1.00 & 0.93 & 0.99 & 0.93 & 0.95 & 0.95 \\ 
  $\delta_{1}$ & 0.34 & -0.11 & 0.79 &  &  & 0.08 & 0.64 & 0.86 &  & 0.95 \\ 
  $\delta_{2}$ & 0.66 & 0.21 & 1.11 &  &  & 0.36 & 0.92 & 0.86 &  & 0.95 \\ 
   \hline
\end{tabular}
\caption{CIs for the TYT dataset. From left to right, the columns contain: the parameter name, parameter estimate, and lower (L.) and upper (U.) bound of the corresponding 95\% CI derived by {\tt TMB}, likelihood profiling, and percentile bootstrap. Then follow coverage probabilities derived for these three methods in a Monte-Carlo study.} 
\label{table:tinn_cis}
\end{table}


TIMO: you previously named this table "Estimates and standard errors on the tinnitus dataset". I changed this and also changed the label. Please do the same for the other tables



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Lamb data}
\label{sec:lamb_data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We chose the well-known data set presented in \citet{leroux}. It consists of the number of movements by a fetal lamb in 240 consecutive 5-second intervals.

The counts are available in the supporting information.



($2$) hidden states were specified in the HMM estimated on the lamb dataset,


We timed estimations of HMMs using different parameters on a dataset provided by \citet{leroux}.


Speed of {\tt{TMB}}\\
{\tt{TMB}} accelerates the estimation time for the HMMs, as with the hospital dataset.\\


% TIMO: if I see a 1-state model one one more occasion, I will get nuts!
% 
% <<tmb-acceleration-lamb, fig.height = 3, fig.cap = 'm state Poisson HMMs estimation time with and without using TMB, estimated on the lamb dataset. Each procedure was repeated and timed. Boxplots are shown for visual comparison.', echo = FALSE>>=
% # include_graphics(paste0("Plots/lamb_m=", max(M_LIST_LAMB), ".png"))
% # title <- paste0("Lamb data, size = ", DATA_SIZE_LAMB)
% # ggplot(benchmarks_df_lamb, aes(x = procedure, y = time)) +
% #   geom_boxplot() +
% #   facet_grid(. ~ m, space ="free_x", scales="free_x", switch="x") +
% #   theme_Publication() +
% #   xlab("Exact/inexact gradient and hessian") +
% #   ylab("Time (seconds)") +
% #   ggtitle(title, "Parameter estimation time") +
% #   scale_y_log10()
% plots <- list()
% for (i in 1:length(M_LIST_LAMB)) {
%   m_i <- M_LIST_LAMB[i]
%   subtitle <- paste0("m = ", m_i)
%   plots[[i]] <- estim_benchmarks_df_lamb %>%
%     filter(m == m_i) %>%
%     ggplot(aes(x = procedure, y = time)) +
%     geom_boxplot() +
%     theme_Publication() +
%     xlab("Exact/inexact gradient and hessian") +
%     ylab("Time (seconds)") +
%     ggtitle("Parameter estimation time", subtitle) +
%     scale_y_log10()
% }
% ggarrange(plotlist = plots, ncol = length(M_LIST_LAMB), nrow = 1)
% @
% 
% 
% TIMO: for the table below, did we not have something more intuitive for the talk as well (factor instead of percent?)?\\

The average time ratios from using {\tt{TMB}} on HMMs estimated on the lamb dataset are all above 1 (\autoref{table:speed-lamb}), thus showing that estimation using {\tt{TMB}} is faster than a regular estimation procedure without using {\tt{TMB}} even on a small dataset.
% latex table generated in R 4.0.5 by xtable 1.8-4 package
% Sat May 01 02:46:23 2021
\begin{table}[ht]
\centering
\begin{tabular}{rllll}
  \hline
m & \textit{${TMB}$} & \textit{${TMB_G}$} & \textit{${TMB_H}$} & \textit{${TMB_{GH}}$} \\ 
  \hline
2 & 21.6 (19, 23.6) & 42.7 (33.2, 54.5) & 21.8 (19.4, 23.8) & 13.2 (9.62, 24.9) \\ 
   \hline
\end{tabular}
\caption{Acceleration when using TMB for m state Poisson HMM parameter estimation, estimated on the lamb dataset. Each procedure was repeated and timed once on 100 sample datasets. With 2 hidden states, $TMB_G$ accelerated the estimation by an average factor of 42, with a 95\% quantile confidence interval of 7 (33.2,} 
\label{table:speed-lamb}
\end{table}


% latex table generated in R 4.0.5 by xtable 1.8-4 package
% Sat May 01 02:46:23 2021
\begin{table}[ht]
\centering
\begin{tabular}{rlllll}
  \hline
m & \textit{${DM}$} & \textit{${TMB}$} & \textit{${TMB_G}$} & \textit{${TMB_H}$} & \textit{${TMB_{GH}}$} \\ 
  \hline
2 & 3.1 (2.85, 3.87) & 1.52 (1.38, 1.99) & 3.07 (2.84, 3.93) & 4.23 (3.88, 5.57) & 70.1 (62.8, 84.3) \\ 
   \hline
\end{tabular}
\caption{Duration (in millisecond) of m state Poisson HMM parameter estimation on the lamb dataset. Each procedure was timed 100 times. This table summarizes the computer's performance across multiple attempts at the same computation, in order to verify how reliable timing an estimation is. With 2 hidden states, estimation with $TMB_G$ took an average 3.0 ms to compute, with an empirical 95\% quantile confidence interval of 7 (2.84, 3.} 
\label{table:speed-consistency-lamb}
\end{table}


Optimizing with {\tt{TMB}} gives the same estimates as optimizing without (\autoref{table:2-state-lamb-estimates}) when estimating a 2 state Poisson HMM on the lamb dataset.

% latex table generated in R 4.0.5 by xtable 1.8-4 package
% Sat May 01 02:46:24 2021
\begin{table}[ht]
\centering
\begin{tabular}{rrrrrr}
  \hline
 & \textit{${DM}$} & \textit{${TMB}$} & \textit{${TMB_G}$} & \textit{${TMB_H}$} & \textit{${TMB_{GH}}$} \\ 
  \hline
$\lambda_{1}$ & 0.26 & 0.26 & 0.26 & 0.26 & 0.26 \\ 
  $\lambda_{2}$ & 3.11 & 3.11 & 3.11 & 3.11 & 3.11 \\ 
  $\gamma_{1, 1}$ & 0.99 & 0.99 & 0.99 & 0.99 & 0.99 \\ 
  $\gamma_{2, 1}$ & 0.31 & 0.31 & 0.31 & 0.31 & 0.31 \\ 
  $\gamma_{1, 2}$ & 0.01 & 0.01 & 0.01 & 0.01 & 0.01 \\ 
  $\gamma_{2, 2}$ & 0.69 & 0.69 & 0.69 & 0.69 & 0.69 \\ 
  $\delta_{1}$ & 0.96 & 0.96 & 0.96 & 0.96 & 0.96 \\ 
  $\delta_{2}$ & 0.04 & 0.04 & 0.04 & 0.04 & 0.04 \\ 
   \hline
\end{tabular}
\caption{Estimates of 2 state Poisson HMM with and without using TMB, estimated on the lamb dataset.} 
\label{table:2-state-lamb-estimates}
\end{table}




Moreover, the acceleration ratios from using {\tt{TMB}} (\autoref{table:speed-lamb-nll} and \autoref{table:speed-simu-nll}) are all above 1, thus showing that on a comparably small dataset, the objective function computation can be accelerated.

% latex table generated in R 4.0.5 by xtable 1.8-4 package
% Sat May 01 02:46:24 2021
\begin{table}[ht]
\centering
\begin{tabular}{rllll}
  \hline
m & \textit{${TMB}$} & \textit{${TMB_G}$} & \textit{${TMB_H}$} & \textit{${TMB_{GH}}$} \\ 
  \hline
2 & 19.1 (13.1, 24.2) & 18.8 (13.4, 24.5) & 18.2 (10.7, 24.5) & 19.1 (13.3, 23.7) \\ 
   \hline
\end{tabular}
\caption{Acceleration when using TMB for m state Poisson HMM negative log-likelihood calculation, estimated on the lamb dataset. Each procedure was repeated and timed once on 100 sample datasets. With 2 hidden states, $TMB_G$ accelerated the estimation by an average factor of 18, with a 95\% quantile confidence interval of 8 (13.4,} 
\label{table:speed-lamb-nll}
\end{table}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Simulation study}
\label{sec:simu_study}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 ($2, 3$) for the simulated dataset,



The second dataset is a sequence of random numbers generated by a $m$-state Poisson HMM.
We used the code below to generate that simulated dataset, with $m=2, 3$.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{DATA_SIZE_SIMU} \hlkwb{<-} \hlnum{2000}
\hlstd{m} \hlkwb{<-} \hlnum{2}
\hlcom{# Generate parameters}
\hlstd{lambda} \hlkwb{<-} \hlkwd{seq}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{7}\hlstd{,} \hlkwc{length.out} \hlstd{= m)}
\hlcom{# Create the transition probability matrix with 0.8 on its diagonal}
\hlstd{gamma} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{0.2} \hlopt{/} \hlstd{(m} \hlopt{-} \hlnum{1}\hlstd{),} \hlkwc{nrow} \hlstd{= m,} \hlkwc{ncol} \hlstd{= m)}
\hlkwd{diag}\hlstd{(gamma)} \hlkwb{<-} \hlnum{0.8}
\hlstd{delta} \hlkwb{<-} \hlkwd{stat.dist}\hlstd{(gamma)}

\hlcom{#simulate the data}
\hlstd{simul_data} \hlkwb{<-} \hlkwd{pois.HMM.generate_sample}\hlstd{(}\hlkwc{ns} \hlstd{= DATA_SIZE_SIMU,}
                                       \hlkwc{mod} \hlstd{=} \hlkwd{list}\hlstd{(}\hlkwc{m} \hlstd{= m,}
                                                  \hlkwc{lambda} \hlstd{= lambda,}
                                                  \hlkwc{gamma} \hlstd{= gamma,}
                                                  \hlkwc{delta} \hlstd{= delta))}
\end{alltt}
\end{kframe}
\end{knitrout}




% \autoref{fig:tmb-acceleration-simul} shows the time acceleration when using {\tt{TMB}} on a simulated dataset through boxplots.
% Similarly to the other datasets, it shows that using {\tt{TMB}} results in faster estimations of Poisson HMMs' parameters.
% <<tmb-acceleration-simul, fig.cap = 'm state Poisson HMM estimation time with and without using TMB, simulated data.', echo = FALSE>>=
% # include_graphics(paste0("Plots/simul_m=", max(M_LIST_SIMU), ".png"))
% # title <- paste0("Simulated data, size = ", DATA_SIZE_SIMU)
% # ggplot(benchmarks_df_simul, aes(x = procedure, y = time)) +
% #   geom_boxplot() +
% #   facet_grid(. ~ m, space ="free_x", scales="free_x", switch="x") +
% #   theme_Publication() +
% #   xlab("Exact/inexact gradient and hessian") +
% #   ylab("Time (seconds)") +
% #   ggtitle(title, "Parameter estimation time") +
% #   scale_y_log10()
% plots <- list()
% for (i in M_LIST_SIMU) {
%   title <- paste0("Simulated data, m = ", i)
%   plots[[i]] <- benchmarks_df_simul %>%
%     filter(m == i) %>%
%     ggplot(aes(x = procedure, y = time)) +
%     geom_boxplot() +
%     theme_Publication() +
%     xlab("Exact/inexact gradient and hessian") +
%     ylab("Time (seconds)") +
%     ggtitle(title, "Parameter estimation time") +
%     scale_y_log10()
% }
% ggarrange(plotlist = plots, ncol = 2, nrow = 2)
% @
The average speed increases using a simulated dataset (\autoref{table:speed-simul}) shows similar results to the speed gains from the other datasets, showing that {\tt{TMB}} is useful for estimating Poisson HMMs on medium sized datasets.
% latex table generated in R 4.0.5 by xtable 1.8-4 package
% Sat May 01 02:46:24 2021
\begin{table}[ht]
\centering
\begin{tabular}{rllll}
  \hline
m & \textit{${TMB}$} & \textit{${TMB_G}$} & \textit{${TMB_H}$} & \textit{${TMB_{GH}}$} \\ 
  \hline
2 & 15.5 (14.2, 16.8) & 30.7 (24.7, 37.7) & 15.5 (13.6, 16.5) & 18.2 (12.8, 22.7) \\ 
  3 & 11.6 (10.6, 12.4) & 45.4 (36.7, 55.6) & 11.6 (10.6, 12.4) & 22.1 (14.2, 28.5) \\ 
   \hline
\end{tabular}
\caption{Acceleration when using TMB for m state Poisson HMM parameter estimation, estimated on the simulated dataset. Each procedure was repeated and timed once on 100 sample datasets. With 2 hidden states, $TMB_G$ accelerated the estimation by an average factor of 30, with a 95\% quantile confidence interval of 7 (24.7,} 
\label{table:speed-simu}
\end{table}


% latex table generated in R 4.0.5 by xtable 1.8-4 package
% Sat May 01 02:46:24 2021
\begin{table}[ht]
\centering
\begin{tabular}{rlllll}
  \hline
m & \textit{${DM}$} & \textit{${TMB}$} & \textit{${TMB_G}$} & \textit{${TMB_H}$} & \textit{${TMB_{GH}}$} \\ 
  \hline
2 & 46 (44, 48) & 19 (18, 20) & 46 (44, 48) & 14 (14, 15) & 853 (831, 1001) \\ 
  3 & 206 (202, 211) & 59 (57, 81) & 207 (202, 217) & 57 (56, 59) & 2584 (2521, 2737) \\ 
   \hline
\end{tabular}
\caption{Duration (in millisecond) of m state Poisson HMM parameter estimation on the simulated dataset. Each procedure was timed 100 times. This table summarizes the computer's performance across multiple attempts at the same computation, in order to verify how reliable timing an estimation is. With 2 hidden states, estimation with $TMB_G$ took an average 46  ms to compute, with an empirical 95\% quantile confidence interval of (44, 48)} 
\label{table:speed-consistency-simu}
\end{table}

The acceleration ratios displayed in \autoref{table:speed-simu} are all above 1, thus showing that on medium sized datasets, the estimation computation can be accelerated.

Moreover, the acceleration ratios from using {\tt{TMB}} (\autoref{table:speed-simu-nll}) are all above 1, thus showing that on a medium sized datasets, the objective function computation can be accelerated.

Similar conclusions are obtained for Poisson HMMs estimated on the hospital dataset and simulated datasets.

% latex table generated in R 4.0.5 by xtable 1.8-4 package
% Sat May 01 02:46:24 2021
\begin{table}[ht]
\centering
\begin{tabular}{rllll}
  \hline
m & \textit{${TMB}$} & \textit{${TMB_G}$} & \textit{${TMB_H}$} & \textit{${TMB_{GH}}$} \\ 
  \hline
2 & 16 (14, 19) & 16 (13, 19) & 16 (13, 19) & 16 (12, 18) \\ 
  3 & 11 (9, 12) & 11 (9, 13) & 11 (9, 13) & 11 (9, 13) \\ 
   \hline
\end{tabular}
\caption{Acceleration when using TMB for m state Poisson HMM negative log-likelihood calculation, estimated on the simulated dataset. Each procedure was repeated and timed once on 100 sample datasets. With 2 hidden states, $TMB_G$ accelerated the estimation by an average factor of 16, with a 95\% quantile confidence interval of (13, 19)} 
\label{table:speed-simu-nll}
\end{table}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hospital data}
\label{sec:hosp_data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 and ($2, 3$) for the hospital dataset) to compare speeds.


% 
% We timed $m$ states HMM parameter estimations using different parameters on a dataset provided by a hospital from Assistance Publique  Hpitaux de Paris, a french hospital trust, with $m = M_LIST_HOSP$.
% Other datasets are considered in \autoref{sec:speed_evaluation}.
% 
% The dataset contains the number of patients who entered a hospital each hour between the first hour of 2010 and the last hour of 2019.
% The data was anonymized and grouped using the R package {\tt{dplyr}}.
% That data contains DATA_SIZE_HOSP data points.\\
% The column PATIENTS represents the number of people arriving at the hospital between the hour mentioned and the next.
% The zeroes there indicate that no patient arrived at the hospital during the hour.\\
% The column DATE is a datetime item useful for sorting and plotting the data.\\
% The column WDAY represents the number of the day in the week: 1 is Monday and 7 is Sunday.\\
% The columns YEAR is an integer taking values 2010, 2011, \ldots, 2019.\\
% For example, the dataset starts on Januray $1^{st}$ 2010 on the hour 0 with an amount of patients of 6, indicating that 6 people arrived between 00:00 and 00:59.
% 
% See the supporting information for details on importing the data.
% 

% We timed the parameter estimation of $m$ states Poisson HMMs using the approaches DM, TMB1, TMB2, TMB3, and TMB4, with $m = M_LIST_HOSP$ and found that using {\tt{TMB}} accelerates the estimation in all cases as can be seen in \autoref{fig:tmb-acceleration-hosp} by the drop in time from DM to any estimation using {\tt{TMB}}.
% The times can be found in the supporting information.
% As could have been expected, estimating a model's parameters takes a longer time as the complexity of the model ($m$) increases.
% <<tmb-acceleration-hosp, fig.cap = 'm state Poisson HMM estimation time with and without using TMB, estimated on the hospital dataset. Each procedure was repeated and timed. Boxplots are shown for visual comparison. The naming convention is found in \\autoref{table:notation}.', echo = FALSE>>=
% # include_graphics(paste0("Plots/hosp_m=", max(M_LIST_HOSP), ".png"))
% # title <- paste0("Hospital data, size = ", DATA_SIZE_HOSP)
% # ggplot(benchmarks_df_hosp, aes(x = procedure, y = time)) +
% #   geom_boxplot() +
% #   facet_grid(. ~ m, space ="free_x", scales="free_x", switch="x") +
% #   theme_Publication() +
% #   xlab("Exact/inexact gradient and hessian") +
% #   ylab("Time (seconds)") +
% #   ggtitle(title, "Parameter estimation time") +
% #   scale_y_log10()
% plots <- list()
% for (i in M_LIST_HOSP) {
%   subtitle <- paste0("m = ", i)
%   plots[[i]] <- benchmarks_df_hosp %>%
%     filter(m == i) %>%
%     ggplot(aes(x = procedure, y = time)) +
%     geom_boxplot() +
%     theme_Publication() +
%     xlab("Exact/inexact gradient and hessian") +
%     ylab("Time (seconds)") +
%     ggtitle("Parameter estimation time", subtitle) +
%     scale_y_log10()
% }
% ggarrange(plotlist = plots, ncol = 2, nrow = 2)
% @
% Interestingly, for $m > 1$, TMB3 shows a clear acceleration when compared to other estimations using {\tt{TMB}}.
% The reason is unclear.
% latex table generated in R 4.0.5 by xtable 1.8-4 package
% Sat May 01 02:46:24 2021
\begin{table}[ht]
\centering
\begin{tabular}{rllll}
  \hline
m & \textit{${TMB}$} & \textit{${TMB_G}$} & \textit{${TMB_H}$} & \textit{${TMB_{GH}}$} \\ 
  \hline
2 & 14 (12, 15) & 27 (22, 30) & 14 (13, 15) & 14 (11, 18) \\ 
  3 & 11 (10, 11) & 49 (40, 56) & 11 (10, 11) & 13 (10, 19) \\ 
   \hline
\end{tabular}
\caption{Acceleration when using TMB for m state Poisson HMM parameter estimation, estimated on the hospital dataset. Each procedure was repeated and timed once on 100 bootstrap sample datasets. With 2 hidden states, $TMB_G$ accelerated the estimation by an average factor of 27, with a 95\% quantile bootstrap confidence interval of (22, 30)} 
\label{table:speed-hosp}
\end{table}


% latex table generated in R 4.0.5 by xtable 1.8-4 package
% Sat May 01 02:46:24 2021
\begin{table}[ht]
\centering
\begin{tabular}{rlllll}
  \hline
m & \textit{${DM}$} & \textit{${TMB}$} & \textit{${TMB_G}$} & \textit{${TMB_H}$} & \textit{${TMB_{GH}}$} \\ 
  \hline
2 & 2762.6 (2684.3, 3366) & 1455.3 (1416.4, 1874.1) & 2796.9 (2693.3, 3703.2) & 1581.9 (1548.5, 1942.4) & 48176.9 (47972, 48595.3) \\ 
  3 & 21562.9 (21053.3, 24389.2) & 7108.2 (6967.6, 8349.3) & 21445.4 (21051.3, 23429.3) & 15396.4 (15185.4, 16571.1) & 253005.3 (252085.1, 260945.8) \\ 
   \hline
\end{tabular}
\caption{Duration (in millisecond) of m state Poisson HMM parameter estimation on the hospital dataset. Each procedure was timed 100 times. This table summarizes the computer's performance across multiple attempts at the same computation, in order to verify how reliable timing an estimation is. With 2 hidden states, estimation with $TMB_G$ took an average 279 ms to compute, with an empirical 95\% quantile confidence interval of 6.9 (2693.3} 
\label{table:speed-consistency-hosp}
\end{table}

% 
% Each box of the boxplots summarizes $BENCHMARK_SAMPLES$ data points but shows little variation.
% A percentage mean speed increase of TMB1, ..., TMB4 when compared to the mean times of DM summarizes these graphs nicely (see \autoref{table:speed-hospital}).
% For example, the average 4 state Poisson HMM estimation speed when providing {\tt{TMB}}'s exact gradient to {\tt{nlminb}} and not the hessian (TMB3) is $round(incr_hosp[incr_hosp$m == 4, "TMB3"])$\% faster than without making use of {\tt{TMB}} (DM).
% 
% <<tmb-acceleration-hosp-print-table, results='asis', echo = FALSE>>=
% print(table, include.rownames = FALSE)
% @
% 
% TMB also accelerates the likelihood computation time.
% The percentage speed gains from using {\tt{TMB}} (\autoref{table:increase-likelihood-hosp}) are all positive, thus showing that on large datasets, the objective function computation time can be accelerated.
% <<tmb-acceleration-hosp-log, results='asis', echo = FALSE>>=
% incr_mllk_time_hosp <- data.frame(m = integer(),
%                                   "TMB" = numeric(),
%                                   "TMB2" = numeric(),
%                                   "TMB3" = numeric(),
%                                   "TMB4" = numeric())
% for (idx in 1:length(M_LIST_HOSP)) {
%   m <- M_LIST_HOSP[idx]
%   incr_mllk_time_hosp[idx, "m"] <- m
%   boxplot_data <- mllk_times_hosp[[idx]]
% 
%   incr_mllk_time_hosp[idx, "TMB"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB"])) - 1
%   incr_mllk_time_hosp[idx, "TMB2"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB2"])) - 1
%   incr_mllk_time_hosp[idx, "TMB3"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB3"])) - 1
%   incr_mllk_time_hosp[idx, "TMB4"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB4"])) - 1
% }
% incr_mllk_time_hosp[, - 1] <- incr_mllk_time_hosp[, - 1] * 100
% table <- xtable(incr_mllk_time_hosp,
%                 display = c("d", "d", rep("f", 4)),
%                 caption = paste0("Speed percentage increase of using TMB for m state Poisson HMM negative log-likelihood calculation, estimated on the hospital dataset. Each procedure was repeated and timed. Their mean times are compared in this table. When m=1, the mean calculation time with TMB by providing the exact gradient but not the exact hessian (TMB3) was ", round(incr_mllk_time_hosp[1, 4]), "\\% lower than the mean estimation time with direct maximization without TMB (DM). Naming convention is found in \\autoref{table:notation}"),
%                 digits = 0,
%                 label = "table:increase-likelihood-hosp")
% print(table, include.rownames = FALSE)
% @
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Interpretation}
% \label{sec:interpretation}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Looking at a boxplot of the data (\autoref{fig:hourly-hospital}) lets us discover the hourly distribution of the data.
% We choose to use hours instead of days or other time durations because the difference in arrivals between night and day was already obvious to the doctors themselves, and we would likely find at least two distinct regimes there.
% <<hourly-hospital, echo = FALSE, fig.cap = "Hourly amount of patient arrival in the hospital">>=
% full_hosp_data %>%
%   select(HOUR, PATIENTS) %>%
%   mutate(type = ifelse(HOUR >= 9 & HOUR <= 22, "day", "night"),
%          HOUR = as.factor(HOUR)) %>%
%   ggplot(aes(x = HOUR, y = PATIENTS, fill = type)) +
%   geom_boxplot() +
%   theme_Publication() +
%   theme(legend.title = element_blank()) +
%   xlab("hour") +
%   ylab("patients")
% @
% More patients arrive in the day rather than during the night as was guessed, but more patients arrive in the afternoon rather than in the evening.
% Also, most patients show up in the early part of the day and around noon.
% Given the amount of data (87648/24 = 3652 data points for each hour), we can apply a z-test to see if the means are different.
% <<p-value, echo = FALSE>>=
% library(tidyverse)
% hour_0 <- full_hosp_data %>%
%   filter(HOUR == 0) %>%
%   select(PATIENTS)
% hour_0 <- hour_0$PATIENTS
% hour_12 <- full_hosp_data %>%
%   filter(HOUR == 12) %>%
%   select(PATIENTS)
% hour_12 <- hour_12$PATIENTS
% 
% mean_12 <- mean(hour_12)
% mean_0 <- mean(hour_0)
% var_12 <- var(hour_12)
% var_0 <- var(hour_0)
% len <- length(hour_12)
% z <- (mean_12 - mean_0) / sqrt(var_12 / len + var_0 / len)
% pvalue <- pnorm(z, lower.tail = FALSE)
% @
% We find a p-value approximated to $pvalue$, so the test shows that the difference between the average amount of patient arrival between 6 and 6:59am and the one between 10 and 10:59am is statistically significant.
% This agrees with the conclusion of the doctors and shows that there are likely at least 2 different distributions in the hourly arrivals of patients, justifying an educated guess of 2 or more hidden states.
% 
% To choose more accurately the number of hidden states, we tried a few, and looked at the quality of each fit (\autoref{table:model-selection-hosp}).
% According to the AIC, the BIC, and the negative log-likelihood, the model with $m = 4$ is the most appropriate and we therefore pick it.
% <<model-selection, results = 'asis', echo = FALSE>>=
% temp <- mllk_values_hosp[mllk_values_hosp$procedure == "TMB4", - 2]
% temp$m <- as.numeric(temp$m)
% colnames(temp)[2] <- "nll"
% table <- xtable(temp,
%                 caption = "Model selection measures of m state Poisson HMMs estimated on the hospital dataset",
%                 label = "table:model-selection-hosp",
%                 digits = 0)
% print(table, include.rownames = FALSE)
% @
% Once the 4 state HMM is estimated, we can look at the state distribution of a 4 state Poisson HMM fitted on the hourly arrivals (see \autoref{fig:states-hospital}) across the entire hospital dataset.
% This HMM seems to agree with our previous guess.
% It should be noted that for the hours 6 and 23, state 4 appeared 0 times and explains why the number 4 is missing in those columns.
% <<states-hospital, fig.cap = "Hourly state distribution of a 4 state Poisson HMM estimated and fitted on the hospital dataset. The $1^{st}$ row in the horizontal axis denotes the hour, the $2^{nd}$ row denotes the state. The state is also color coded for clarity.", echo = FALSE>>=
% m <- 4
% gamma <- matrix(0.2 / (m - 1), nrow = m, ncol = m)
% diag(gamma) <- 0.8
% lambda <- seq(5, 20, length.out = m)
% 
% working_params <- pois.HMM.pn2pw(m, lambda, gamma)
% TMB_data <- list(x = hosp_data, m = m)
% 
% tmb4 <- TMB.estimate(TMB_data = TMB_data,
%                      parameters = working_params,
%                      gradient = TRUE,
%                      hessian = TRUE)
% 
% dec <- HMM.decode(tmb4$obj)$ldecode
% dec <- enframe(dec) %>%
%   rename(state = value) %>%
%   mutate(state = as.factor(state)) %>%
%   bind_cols(full_hosp_data)
% 
% hour <- dec %>%
%   group_by(HOUR, state) %>%
%   summarize(patients = n(), .groups = "drop")
% ggplot(hour, aes(x = state, y = patients, fill = state)) +
%   geom_bar(stat = "identity") +
%   facet_grid(. ~ HOUR, space ="free_x", scales="free_x", switch="x") +
%   xlab("hour and state") +
%   theme_Publication()
% @
% States (1, 2, 3, 4) have Poisson means (round(tmb4$lambda, 3)) respectively.
% 



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Miscallaneous}
% \label{sec:misc}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\label{txt:all-methods-comparison} All optimization methods\\
Finally, we compare different optimization methods.
The ones we retain are BFGS, Nelder-Mead, L-BFGS-B, nlm, nlminb, and hjn, because the others don't converge in our case.
{\tt{marqLevAlg}} provides an algorithm for least-squares curve fitting, and is therefore included in the comparison.
Exact gradients and hessians are provided by {\tt{TMB}} and fed to each algorithm.
The speed comparisons are in the following \autoref{fig:all-methods-comparison}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[htb]

{\centering \includegraphics[width=\maxwidth]{figure/all-methods-comparison-1} 

}

\caption[Poisson HMM parameter estimation time per optimization method]{Poisson HMM parameter estimation time per optimization method}\label{fig:all-methods-comparison}
\end{figure}


\end{knitrout}

The following tables summarize the estimates and their confidence intervals, for the lamb dataset and for the simulated dataset.
Instead of showing the standard error, we display the lower and upper bounds of the confidence intervals.
This is due to profiling sometimes failing to provide both bounds of the interval.



% latex table generated in R 4.0.5 by xtable 1.8-4 package
% Sat May 01 02:46:24 2021
\begin{table}[ht]
\centering
\begin{tabular}{cccccccccccc}
  &&& \multicolumn{2}{c}{Profile}& \multicolumn{2}{c}{Bootstrap}& \multicolumn{2}{c}{TMB}& \multicolumn{3}{c}{Coverage}\\ \hline
m & Parameter & Estimate & Lower & Upper & Lower & Upper & Lower & Upper & Profile & Bootstrap & TMB \\ 
  \hline
  2 & $\lambda_{1}$ & 0.26 & 0.15 & 0.33 & 0.00 & 0.33 & 0.18 & 0.34 & 0.80 & 0.90 & 0.90 \\ 
    2 & $\lambda_{2}$ & 3.11 & 1.27 & 4.95 & 0.38 & 5.16 & 1.11 & 5.12 & 0.83 & 0.84 & 0.81 \\ 
    2 & $\gamma_{1, 1}$ & 0.99 & 0.93 & 1.00 & 0.54 & 1.00 & 0.97 & 1.01 & 0.97 & 0.97 & 0.96 \\ 
    2 & $\gamma_{2, 1}$ & 0.31 & 0.04 & 0.68 & 0.05 & 1.00 & -0.05 & 0.67 & 0.96 & 1.00 & 0.84 \\ 
    2 & $\gamma_{1, 2}$ & 0.01 & 0.00 & 0.07 & 0.00 & 0.46 & -0.01 & 0.03 & 0.97 & 0.97 & 0.96 \\ 
    2 & $\gamma_{2, 2}$ & 0.69 & 0.32 & 0.96 & 0.00 & 0.95 & 0.33 & 1.05 & 0.96 & 1.00 & 0.84 \\ 
    2 & $\delta_{1}$ & 0.96 &  &  & 0.31 & 0.99 & 0.90 & 1.03 &  & 1.00 & 0.88 \\ 
    2 & $\delta_{2}$ & 0.04 &  &  & 0.01 & 0.69 & -0.03 & 0.10 &  & 1.00 & 0.88 \\ 
   \hline
\end{tabular}
\caption{Estimates and standard errors on the lamb dataset} 
\label{table:lamb_estimates_std_errors}
\end{table}


% latex table generated in R 4.0.5 by xtable 1.8-4 package
% Sat May 01 02:46:25 2021
\begin{table}[ht]
\centering
\scalebox{0.95}{
\begin{tabular}{ccccccccccccc}
  &&&& \multicolumn{2}{c}{Profile}& \multicolumn{2}{c}{Bootstrap}& \multicolumn{2}{c}{TMB}& \multicolumn{3}{c}{Coverage}\\ \hline
m & Parameter & True value & Estimate & Lower & Upper & Lower & Upper & Lower & Upper & Profile & Bootstrap & TMB \\ 
  \hline
  2 & $\lambda_{1}$ & 1.00 & 0.97 & 0.91 & 1.03 & 0.78 & 1.14 & 0.91 & 1.03 & 0.95 & 0.94 & 0.95 \\ 
    2 & $\lambda_{2}$ & 20.00 & 20.07 & 19.79 & 20.35 & 19.32 & 20.92 & 19.79 & 20.34 & 0.95 & 0.96 & 0.95 \\ 
    2 & $\gamma_{1, 1}$ & 0.80 & 0.80 & 0.77 & 0.82 & 0.71 & 0.86 & 0.77 & 0.82 & 0.93 & 0.92 & 0.92 \\ 
    2 & $\gamma_{2, 1}$ & 0.20 & 0.20 & 0.18 & 0.23 & 0.13 & 0.28 & 0.18 & 0.22 & 0.95 & 0.94 & 0.96 \\ 
    2 & $\gamma_{1, 2}$ & 0.20 & 0.20 & 0.18 & 0.23 & 0.14 & 0.29 & 0.18 & 0.23 & 0.93 & 0.92 & 0.92 \\ 
    2 & $\gamma_{2, 2}$ & 0.80 & 0.80 & 0.77 & 0.82 & 0.72 & 0.87 & 0.78 & 0.82 & 0.95 & 0.94 & 0.96 \\ 
    2 & $\delta_{1}$ & 0.50 & 0.50 &  &  & 0.36 & 0.62 & 0.45 & 0.54 &  & 0.92 & 0.93 \\ 
    2 & $\delta_{2}$ & 0.50 & 0.50 &  &  & 0.38 & 0.64 & 0.46 & 0.55 &  & 0.92 & 0.93 \\ 
    3 & $\lambda_{1}$ & 1.00 & 0.95 & 0.87 & 1.02 & 0.71 & 1.18 & 0.87 & 1.02 & 0.95 & 0.95 & 0.95 \\ 
    3 & $\lambda_{2}$ & 10.50 & 10.87 & 10.60 & 11.14 & 10.03 & 11.79 & 10.60 & 11.14 & 0.94 & 0.94 & 0.93 \\ 
    3 & $\lambda_{3}$ & 20.00 & 20.19 & 19.77 & 20.62 & 18.90 & 21.54 & 19.77 & 20.61 & 0.94 & 0.95 & 0.94 \\ 
    3 & $\gamma_{1, 1}$ & 0.80 & 0.82 & 0.78 & 0.86 & 0.71 & 0.89 & 0.79 & 0.85 & 0.99 & 0.92 & 0.95 \\ 
    3 & $\gamma_{2, 1}$ & 0.10 & 0.09 & 0.07 & 0.11 & 0.03 & 0.16 & 0.07 & 0.11 & 0.96 & 0.94 & 0.94 \\ 
    3 & $\gamma_{3, 1}$ & 0.10 & 0.09 & 0.07 & 0.11 & 0.03 & 0.18 & 0.06 & 0.11 & 0.93 & 0.94 & 0.92 \\ 
    3 & $\gamma_{1, 2}$ & 0.10 & 0.10 & 0.08 & 0.12 & 0.04 & 0.19 & 0.08 & 0.12 & 0.90 & 0.92 & 0.96 \\ 
    3 & $\gamma_{2, 2}$ & 0.80 & 0.84 & 0.80 & 0.88 & 0.74 & 0.91 & 0.82 & 0.87 & 1.00 & 0.97 & 0.88 \\ 
    3 & $\gamma_{3, 2}$ & 0.10 & 0.08 & 0.06 & 0.11 & 0.02 & 0.20 & 0.05 & 0.11 & 0.92 & 0.94 & 0.90 \\ 
    3 & $\gamma_{1, 3}$ & 0.10 & 0.08 & 0.06 & 0.10 & 0.02 & 0.15 & 0.05 & 0.10 & 0.97 & 0.97 & 1.00 \\ 
    3 & $\gamma_{2, 3}$ & 0.10 & 0.07 & 0.05 & 0.09 & 0.02 & 0.15 & 0.05 & 0.09 & 0.94 & 0.94 & 0.89 \\ 
    3 & $\gamma_{3, 3}$ & 0.80 & 0.83 & 0.78 & 0.87 & 0.69 & 0.90 & 0.79 & 0.86 & 0.99 & 0.91 & 0.76 \\ 
    3 & $\delta_{1}$ & 0.33 & 0.33 &  &  & 0.19 & 0.49 & 0.28 & 0.39 &  & 0.94 & 0.94 \\ 
    3 & $\delta_{2}$ & 0.33 & 0.37 &  &  & 0.22 & 0.54 & 0.31 & 0.43 &  & 0.93 & 0.90 \\ 
    3 & $\delta_{3}$ & 0.33 & 0.29 &  &  & 0.15 & 0.44 & 0.24 & 0.35 &  & 0.94 & 0.93 \\ 
   \hline
\end{tabular}
}
\caption{Estimates and standard errors on the simulated dataset} 
\label{table:simul_estimates_std_errors}
\end{table}


% latex table generated in R 4.0.5 by xtable 1.8-4 package
% Sat May 01 02:46:25 2021
\begin{table}[ht]
\centering
\begin{tabular}{ccccccccc}
  &&& \multicolumn{2}{c}{Profile}& \multicolumn{2}{c}{Bootstrap}& \multicolumn{2}{c}{TMB}\\ \hline
m & Parameter & Estimate & Lower & Upper & Lower & Upper & Lower & Upper \\ 
  \hline
  2 & $\lambda_{1}$ & 4.87 & 4.84 & 4.90 & 4.41 & 5.33 & 4.84 & 4.90 \\ 
    2 & $\lambda_{2}$ & 13.33 & 13.29 & 13.37 & 12.72 & 14.04 & 13.29 & 13.37 \\ 
    2 & $\gamma_{1, 1}$ & 0.87 & 0.88 & 0.87 & 0.77 & 0.93 & 0.87 & 0.88 \\ 
    2 & $\gamma_{2, 1}$ & 0.10 & 0.10 & 0.10 & 0.05 & 0.17 & 0.10 & 0.10 \\ 
    2 & $\gamma_{1, 2}$ & 0.13 & 0.12 & 0.13 & 0.07 & 0.23 & 0.12 & 0.13 \\ 
    2 & $\gamma_{2, 2}$ & 0.90 & 0.90 & 0.90 & 0.83 & 0.95 & 0.90 & 0.90 \\ 
    2 & $\delta_{1}$ & 0.44 &  &  & 0.28 & 0.61 & 0.43 & 0.45 \\ 
    2 & $\delta_{2}$ & 0.56 &  &  & 0.39 & 0.72 & 0.55 & 0.57 \\ 
    3 & $\lambda_{1}$ & 4.06 & 4.03 & 4.10 & 3.52 & 4.68 & 4.03 & 4.10 \\ 
    3 & $\lambda_{2}$ & 10.47 & 10.40 & 10.54 & 9.55 & 11.22 & 10.40 & 10.54 \\ 
    3 & $\lambda_{3}$ & 17.64 & 17.49 & 17.79 & 15.09 & 19.50 & 17.49 & 17.79 \\ 
    3 & $\gamma_{1, 1}$ & 0.85 &  &  & 0.71 & 0.92 & 0.85 & 0.86 \\ 
    3 & $\gamma_{2, 1}$ & 0.10 &  &  & 0.04 & 0.18 & 0.10 & 0.10 \\ 
    3 & $\gamma_{3, 1}$ & 0.00 &  &  & 0.00 & 0.10 & -0.00 & 0.00 \\ 
    3 & $\gamma_{1, 2}$ & 0.10 &  &  & 0.01 & 0.22 & 0.10 & 0.11 \\ 
    3 & $\gamma_{2, 2}$ & 0.87 &  &  & 0.76 & 0.93 & 0.86 & 0.87 \\ 
    3 & $\gamma_{3, 2}$ & 0.17 &  &  & 0.05 & 0.39 & 0.16 & 0.18 \\ 
    3 & $\gamma_{1, 3}$ & 0.05 &  &  & 0.00 & 0.14 & 0.04 & 0.05 \\ 
    3 & $\gamma_{2, 3}$ & 0.03 &  &  & 0.00 & 0.10 & 0.03 & 0.03 \\ 
    3 & $\gamma_{3, 3}$ & 0.83 &  &  & 0.59 & 0.93 & 0.82 & 0.84 \\ 
    3 & $\delta_{1}$ & 0.33 &  &  & 0.16 & 0.49 & 0.32 & 0.34 \\ 
    3 & $\delta_{2}$ & 0.49 &  &  & 0.32 & 0.64 & 0.48 & 0.50 \\ 
    3 & $\delta_{3}$ & 0.18 &  &  & 0.07 & 0.35 & 0.17 & 0.19 \\ 
   \hline
\end{tabular}
\caption{Estimates and standard errors on the hospital dataset} 
\label{table:hosp_estimates_std_errors}
\end{table}


% \newpage
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Discussion}
% \label{sec:discussion}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% In this paper, an alternative method of computing Poisson HMM parameters and their standard errors is examined.
% Using the language R, the usual method can take a long time, particularly if the standard errors are computed by bootstrapping.
% The approach with {\tt{TMB}} was timed on 3 datasets with different amounts of data, and compared with the times of estimations using R without making use of {\tt{TMB}}.
% For computational reasons, this paper doesn't estimate HMMs on millions or billions of data points, but the performance gains are expected to be large even in these cases.
% As can be seen in \autoref{table:simul_estimates_std_errors} and \autoref{table:lamb_estimates_std_errors}, the standard errors obtained through this method are very similar to the standard errors obtained through profiling the likelihood and bootstrapping while being less computationally intensive.
% The method used in this paper can easily be extended to other distributions.
% 
% It is recommended to use both {\tt{TMB}}'s gradient and hessian since it ensures a more reliable result, although it doesn't make any difference in our computations.
% However, including the hessian can make the optimization slower because of its size, as demonstrated by the difference in time between TMB3 and TMB4 when dealing with a multiple state Poisson HMM on a large amount of data (hospital dataset).
% 
% Although we have not tried, it should be possible to extend this approach to HMMs with random effects for panel data for example.
% Introduced by \citet{altman}, Mixed HMM (MHMM) is a class of HMM that combines fixed and random effects, by using the framework of Generalized Linear Mixed Models.
% {\tt{TMB}} should be usable for MHMMs since the likelihood can be optimized through direct maximization as shown by \citet{altman}, or through the EM algorithm as shown by \citet{maruotti}.
% Both authors list possible approaches to estimate MHMMs.
% \citet[p .~7]{altman} reports a numerical integration taking from 1 second with 1 random effect to several days when 4 random effects are included, and a Monte Carlo method requiring approximately 3 days to estimate a MHMM with 3 random effects.
% In addition, both assume the observations (conditional on the random effects and the hidden states), to be distributed in the exponential family.
% By default, {\tt{TMB}} integrates out the random effects and then uses a Laplace approximation on the negative joint log-likelihood.
% However, if needed (for example because the minimizer with respect to the random effect is not unique) one can set this approximation explicitly in C++.
% 
% 
% \begin{enumerate}
% \item
% To our surprise, TMB3 was the fastest in most cases although we don't understand why.
% It operates only with an exact gradient provided by {\tt{TMB}} but without the exact hessian.
% This happened when we used hidden states ($m > 1$), and also when using other optimizers than {\tt{nlminb}} such as {\tt{nlm}}.
% Although adding the exact hessian on top of the gradient to the optimizer seems to slow down the computation, it might help the optimizer converge in some cases, and could be worth investigating.
% 
% \item
% When providing only {\tt{TMB}}'s hessian (TMB2), optimizers sometimes fail to converge.
% 
% \item
% We have timed this approach on different volumes of data, but we haven't tried very large datasets of sizes in the millions or billions.
% However, we expect the increase in speed to be smaller since the time necessary becomes longer independently of the approach chosen.
% 
% \item Mention the prospect of using {\tt{TMB}} for panel data with random effects, Laplace approximation.
% \end{enumerate}
% 
% 
% \begin{acknowledgement}
% We gratefully thank Dr. Bertrand GALICHON and Dr. Anthony CHAUVIN for their patience and their efforts to provide the hospital dataset along with the necessary authorizations.
% \end{acknowledgement}
% \vspace*{1pc}
% 
% \noindent {\bf{Conflict of Interest}}
% 
% \noindent {\it{The authors have declared no conflict of interest.}}
% 
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{Appendix}
% \label{sec:appendix}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection*{A.1.\enspace R Code}
% \label{subsec:rcode}
% \begin{enumerate}
% 
% \item \label{code:setup_parameters.R} Code to setup global parameters and declare functions used
% % \lstinputlisting[language={R}]{code/setup_parameters.R}
% 
% \item \label{code:packages.R} Packages
% % \lstinputlisting[language={R}]{code/packages.R}
% 
% \item \label{code:utils.R} Functions
% % \lstinputlisting[language={R}]{functions/utils.R}
% 
% \item \label{code:poi_hmm_lamb.R} Code to run estimations and comparisons using the lamb dataset
% % \lstinputlisting[language={R}]{code/poi_hmm_lamb.R}
% 
% \item \label{code:poi_hmm_simul.R} Code to run estimations and comparisons using a simulated dataset
% % \lstinputlisting[language={R}]{code/poi_hmm_simul.R}
% 
% \item \label{code:poi_hmm_hosp.R} Code to run estimations and comparisons using the hospital dataset
% % \lstinputlisting[language={R}]{code/poi_hmm_hosp.R}
% 
% \end{enumerate}
% 
% \subsection*{A.2.\enspace C++ Code}
% \label{subsec:cppcode}
% \begin{enumerate}
% 
% \item \label{code:poi_hmm.cpp} Poisson HMM negative log-likelihood calculation
% This is the file poi\_hmm.cpp which contains the negative likelihood function.
% 
% It should be noted that in the C++ file, testing if the value is missing (i.e. testing for NaN (Not A Number) values) requires a little trick.
% The reason is that the standard test function \texttt{std::isnan()} doesn't currently work on a single data value inside {\tt{TMB}}.
% Cuurently in C++, comparisons involving NaN values are always false, except when testing inequality between 2 NaN values.
% In other words, for a float f, the expression f != f is true if and only if f is a NaN value.
% Similarly, f == f returns false if and only if f is a NaN value.
% \lstinputlisting{code/poi_hmm.cpp}
% 
% \item \label{code:Delta_w2n}
% This optional function is in the file utils.cpp.
% It is only necessary if a stationary distribution is not assumed, and $\bfdelta$ is used as a parameter instead of being derived from the transistion probability matrix $\bgamma$.
% It codes the function to convert the working parameter \texttt{tdelta} into its natural format.
% \lstinputlisting[linerange=//latex_Delta_w2n_start-//latex_Delta_w2n_end]{functions/utils.cpp}
% 
% \item \label{code:Gamma_w2n}
% This function is in the file utils.cpp.
% It transforms the transition probability matrix $\bgamma$ from its working format to its natural format.
% \lstinputlisting[linerange=//latex_Gamma_w2n_start-//latex_Gamma_w2n_end]{functions/utils.cpp}
% 
% \item \label{code:Stat_dist}
% This function is in the file utils.cpp.
% It derives the stationary distribution from the transition probability matrix $\bgamma$.
% \lstinputlisting[linerange=//latex_Stat_dist_start-//latex_Stat_dist_end]{functions/utils.cpp}
% 
% \item \label{code:utils.cpp} Functions used in C++ Poisson HMM code
% % \lstinputlisting{functions/utils.cpp}
% 
% \item \label{code:linreg.cpp} Linear model negative log-likelihood calculation
% % \lstinputlisting{code/linreg.cpp}
% 
% \item \label{code:utils_linreg.cpp} Functions used in C++ linear model code
% % \lstinputlisting{functions/utils_linreg.cpp}
% 
% \end{enumerate}
% 
% % \begin{enumerate}
% % \item simple C acceleration of Zucchini scripts p. 333, A 1.7, A 1.8 with conditional probabilities outside of the forward / backward loop
% % \item Use same order as in Zucchini
% % \item the .cpp file with transformation code
% % \item the .cpp file with likelihood
% % \item an .R file showcasing the use
% % \end{enumerate}
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage %Before bibliography
\bibliographystyle{plainnat}
\bibliography{paper}
% \addcontentsline{toc}{chapter}{References}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
