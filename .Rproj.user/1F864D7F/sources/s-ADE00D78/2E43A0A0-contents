# Confidence intervals

```{r import-files, echo = FALSE, cache = FALSE}
library(knitr)
options(warn = - 1)
setwd(dir = "../")
source("code/main.R")
knitr::read_chunk('functions/utils.R')
```


From the parameters' ML estimates, we generate new data and re-estimate the parameters \Sexpr{BOOTSTRAP_SAMPLES} times.
From that list of new estimates we can get the 2.5th and 97.5th percentiles and get 95\% confidence intervals for the parameters.\\

We show below how we get confidence intervals using bootstrap, based on the 2 state Poisson HMM estimates from above. TIMO: we had a 2-state above, right?

* First, we need a function to generate random data from a HMM.
```{r pois.HMM.generate_sample}
```

* Then, when the model is estimated each time, we don't impose an order for the states.

This can lead to the label switching problem, where states aren't ordered the same way in each model.
To address this, we re-ordered the states by ascending Poisson means.\\
Sorting the means is pretty straightforward.
Re-ordering the TPM is a little trickier.
To do so, we took the permutations of the states given by the sorted Poisson means, and permuted each row index and column index to its new value.\\
The function we used is
```{r pois.HMM.label.order}

```

Let's show an example to understand the process.
For readability, the TPM is filled with row and column indexes instead of probabilities.
```{r relabel}
lambda <- c(30, 10, 20)
gamma <- matrix(c(11, 12, 13,
                  21, 22, 23,
                  31, 32, 33), byrow = TRUE, ncol = 3)
pois.HMM.label.order(m = 3, lambda, gamma)
```

State 1 has been relabeled state 3, state 2 became state 1, and state 3 became state 2.

* Bootstrap code
```{r bootstrap-code, eval = FALSE}
bootstrap_estimates <- data.frame()
DATA_SIZE <- length(lamb_data)
# Set how many parametric bootstrap samples we create
BOOTSTRAP_SAMPLES <- 1000

# ML parameters
ML_working_estimates <- obj_tmb$env$last.par.best
ML_natural_estimates <- obj_tmb$report(ML_working_estimates)
gamma <- ML_natural_estimates$gamma
lambda <- ML_natural_estimates$lambda
delta <- ML_natural_estimates$delta

# Parameters for TMB
cols <- names(ML_working_estimates)
tgamma <- ML_working_estimates[cols == "tgamma"]
tlambda <- ML_working_estimates[cols == "tlambda"]
ML_TMB_parameters <- list(tlambda = tlambda,
                          tgamma = tgamma)

params_names <- c("lambda", "gamma", "delta")
# counter1 <- counter2 <- counter3 <- counter_threshold_gamma <- counter_threshold_lambda <- 0
# conv1 <- c()
# conv2 <- data.frame(message = character(),
#                     min_g = numeric(),
#                     max_g = numeric(),
#                     min_l = numeric(),
#                     max_l = numeric())
# # nll <- c()
# idx_sample <- 1
# myAIC <- myBIC <- data.frame()
for (idx_sample in 1:BOOTSTRAP_SAMPLES) {
  # Loop as long as there is an issue with nlminb
  repeat {
    #simulate the data
    bootstrap_data <- pois.HMM.generate_sample(DATA_SIZE,
                                               list(m = m,
                                                    lambda = lambda,
                                                    gamma = gamma,
                                                    delta = delta))
    
    # # TRUE STATE FILTERING
    # tab <- as.numeric(table(bootstrap_data$state)) / DATA_SIZE
    # if (any(tab >= 0.99)) {
    #   counter = counter + 1
    #   next
    # }
    
    # Parameters for TMB
    TMB_data_bootstrap <- list(x = bootstrap_data$data, m = m)
    
    # Estimate the parameters
    obj <- MakeADFun(TMB_data_bootstrap,
                     ML_TMB_parameters,
                     DLL = "poi_hmm",
                     silent = TRUE)
    
    # Using tryCatch to break the loop if an error happens
    # interrupts the outer (for) loop, not the inner one (repeat)
    mod_bootstrap <- NULL
    try(mod_bootstrap <- nlminb(start = obj$par, objective = obj$fn,
                                gradient = obj$gr, hessian = obj$he),
        silent = TRUE)
    
    # If nlminb doesn't reach any result, retry
    if (is.null(mod_bootstrap)) {
      # conv1 <- c(conv1, mod_bootstrap$convergence)
      # counter1 <- counter1 + 1
      next
    }
    # If nlminb doesn't converge successfully, retry
    if (mod_bootstrap$convergence != 0) {
      # working_parameters <- obj$env$last.par.best
      # natural_parameters <- obj$report(working_parameters)
      # 
      # g <- natural_parameters$gamma
      # conv2[counter2 + 1, ] <- cbind(mod_bootstrap$message,
      #                                round(min(g), 4),
      #                                round(max(g), 4),
      #                                round(min(l), 4),
      #                                round(max(l), 4))
      # if(min(l) <= 1e-3) {
      #   print(min(l), digits = 10)
      # }
      # counter2 <- counter2 + 1
      next
    }
    
    # # SMOOTHING FILTERING
    # smoot <- HMM.decode(obj)$ldecode
    # tab <- as.numeric(table(smoot)) / DATA_SIZE
    # if (any(tab >= 0.99)) {
    #   counter = counter + 1
    #   next
    # }
    
    working_parameters <- obj$env$last.par.best
    natural_parameters <- obj$report(working_parameters)

    l <- natural_parameters$lambda
    g <- natural_parameters$gamma
    d <- natural_parameters$delta
    
    # # VITERBI FILTERING
    # vit <- pois.HMM.viterbi(bootstrap_data$sample, m, lambda = l, gamma = g, delta = d)
    # tab <- as.numeric(table(vit)) / DATA_SIZE
    # if (any(tab >= 0.99)) {
    #   counter = counter + 1
    #   next
    # }
    

    # Label switching
    natural_parameters <- pois.HMM.label.order(m = m, lambda = l,
                                               gamma = g, delta = d)

    # If some parameters are NA for some reason, retry
    if (anyNA(natural_parameters[params_names], recursive = TRUE)) {
      # counter3 <- counter3 + 1
      next
    }
    
    # if(max(g) >= 0.999999) {
    #   counter_threshold_gamma <- counter_threshold_gamma + 1
    # }    
    # if(min(l) <= 1e-3) {
    #   counter_threshold_lambda <- counter_threshold_lambda + 1
    # }
    
    # # ATTEMPT ONE STATE, AIC AND BIC
    # lambda1state <- mean(bootstrap_data$data)
    # gamma1state <- matrix(1, nrow = 1, ncol = 1)
    # # parvect <- pois.HMM.pn2pw(m = 1, lambda = lambda1state, gamma = gamma1state)
    # # mllk <- pois.HMM.mllk(parvect, x_alias = bootstrap_data$data, m_alias = 1)
    # mllk <- -log(prod(dpois(bootstrap_data$data, lambda1state)))
    # # np <- length(unlist(parvect))
    # np <- 1
    # myAIC[idx_sample, "1state"] <- 2 * (mllk + np)
    # n <- sum(!is.na(bootstrap_data$data))
    # myBIC[idx_sample, "1state"] <- 2 * mllk + np * log(n)
    
    
    # # 2 STATE AIC AND BIC
    # mllk <- mod_bootstrap$objective
    # np <- length(mod_bootstrap$par)
    # myAIC[idx_sample, "2state"] <- 2 * (mllk + np)
    # myBIC[idx_sample, "2state"] <- 2 * mllk + np * log(n)
    
    # LOGLIKELIHOOD VALUES
    # nll <- c(nll, mod_bootstrap$objective)
    # nll <- mod_bootstrap$objective
    # if (nll <= 142.1408 | nll >= 215.7791) {
    #   counter = counter + 1
    #   next
    # }
    # if (nll <= 146.7423 | nll >= 208.3097) {
    #   counter = counter + 1
    #   next
    # }
    
    # If everything went well, end the "repeat" loop
    break
  }
  # The values from gamma are taken columnwise
  natural_parameters <- unlist(natural_parameters[params_names])
  len_par <- length(natural_parameters)
  bootstrap_estimates[idx_sample, 1:len_par] <- natural_parameters
}

# Lower and upper (2.5% and 97.5%) bounds
q <- apply(bootstrap_estimates, 2, function(par_estimate) {
  quantile(par_estimate, probs = c(0.025, 0.975))
})

params_names <- paste0(rep("lambda", m), 1:m)
# Get row and column indexes for gamma instead of the default
# columnwise index: the default indexes are 1:m for the 1st column,
# then (m + 1):(2 * m) for the 2nd, etc...
for (gamma_idx in 1:m ^ 2) {
  row <- (gamma_idx - 1) %% m + 1
  col <- (gamma_idx - 1) %/% m + 1
  row_col_idx <- c(row, col)
  params_names <- c(params_names,
                    paste0("gamma",
                           paste0(row_col_idx, collapse = "")))
}
params_names <- c(params_names,
                  paste0(rep("delta", m), 1:m))
bootstrap_CI <- data.frame("Parameter" = params_names,
                           "Estimate" = c(lambda, gamma, delta),
                           "Lower bound" = q[1, ],
                           "Upper bound" = q[2, ])
print(bootstrap_CI, row.names = FALSE)
# print(counter)
# plot(x = 1:BOOTSTRAP_SAMPLES, y = nll)
# hist(nll)
# apply(bootstrap_estimates[, 1:2], 2, function(est) {
#   hist(est, breaks = "FD")
# })
# print(paste("Lack of convergence/missing results", counter2, counter1, "times"))
# myAIC[, "pref"] <- ifelse(myAIC[, "1state"] <= myAIC[, "2state"], 1, 2)
# myBIC[, "pref"] <- ifelse(myBIC[, "1state"] <= myBIC[, "2state"], 1, 2)
# print(paste("AIC prefers 1 state", sum(myAIC[, "pref"] == 1), "times, and 2 states", sum(myAIC[, "pref"] == 2), "times"))
# print(paste("BIC prefers 1 state", sum(myBIC[, "pref"] == 1), "times, and 2 states", sum(myBIC[, "pref"] == 2), "times"))
# 
# 
# print(paste(counter1, "don't reach any result"))
# # print(paste("conv codes:", conv1))
# print(paste(counter2, "don't converge successfully"))
# # print(paste("conv codes:", conv2))
# print(paste(counter3, "parameters are NA"))
# # print(paste("gamma over the threshold", counter_threshold_gamma, "times"))
# print(paste("lambda under the threshold", counter_threshold_lambda, "times"))
```


It should be noted that some estimates can be very large or very small.
One possible reason is that the randomly generated bootstrap sample might contain long chains of the same values, thus causing a probability in the TPM to be almost 0 or almost 1.
However, a large number of bootstrap samples lowers that risk since we leave out 5\% of the most extreme values when computing the 95\% CI.

TIMO: REALLY? So the biologist / MD determines which samples are not convenient, leaves them out, and cites us. Patients might die...

