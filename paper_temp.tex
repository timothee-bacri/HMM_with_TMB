\documentclass[bimj,fleqn]{w-art}\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{times}
\usepackage{w-thm}
\usepackage[authoryear, round]{natbib}
\usepackage{booktabs}
\setlength{\bibsep}{2pt}
\setlength{\bibhang}{2em}
\newcommand{\J}{J\"{o}reskog}
\newcommand{\So}{S\"{o}rbom}
\newcommand{\bcx}{{\bf X}}
\newcommand{\bcy}{{\bf Y}}
\newcommand{\bcz}{{\bf Z}}
\newcommand{\bcu}{{\bf U}}
\newcommand{\bcv}{{\bf V}}
\newcommand{\bcw}{{\bf W}}
\newcommand{\bci}{{\bf I}}
\newcommand{\bch}{{\bf H}}
\newcommand{\bcb}{{\bf B}}
\newcommand{\bcr}{{\bf R}}
\newcommand{\bcm}{{\bf M}}
\newcommand{\bcp}{{\bf P}}
\newcommand{\bcf}{{\bf F}}
\newcommand{\bcg}{{\bf G}}
\newcommand{\bcs}{{\bf S}}
\newcommand{\bct}{{\bf T}}
\newcommand{\bca}{{\bf A}}
\newcommand{\bcd}{{\bf D}}
\newcommand{\bcc}{{\bf C}}
\newcommand{\bce}{{\bf E}}
\newcommand{\ba}{{\bf a}}
\newcommand{\bb}{{\bf b}}
\newcommand{\bc}{{\bf c}}
\newcommand{\bd}{{\bf d}}
\newcommand{\bx}{\mbox{\boldmath x}}
\newcommand{\by}{\mbox{\boldmath y}}
\newcommand{\bz}{{\bf z}}
\newcommand{\bu}{{\bf u}}
\newcommand{\bv}{{\bf v}}
\newcommand{\bh}{{\bf h}}
\newcommand{\bl}{{\bf l}}
\newcommand{\be}{{\bf e}}
\newcommand{\br}{{\bf r}}
\newcommand{\bw}{{\bf w}}
\newcommand{\de}{\stackrel{D}{=}}
\newcommand{\bt}{\bigtriangleup}
\newcommand{\bfequiv}{\mbox{\boldmath $\equiv$}}
\newcommand{\bmu}{\mbox{\boldmath $\mu$}}
\newcommand{\bnu}{\mbox{\boldmath $\nu$}}
\newcommand{\bxi}{\mbox{\boldmath $\xi$}}
\newcommand{\btau}{\mbox{\boldmath $\tau$}}
\newcommand{\bgamma}{\mbox{\boldmath $\Gamma$}}
\newcommand{\bphi}{\mbox{\boldmath $\Phi$}}
\newcommand{\bfphi}{\mbox{\boldmath $\varphi$}}
\newcommand{\bfeta}{\mbox{\boldmath $\eta$}}
\newcommand{\bpi}{\mbox{\boldmath $\Pi$}}
\newcommand{\bequiv}{\mbox{\boldmath $\equiv$}}
\newcommand{\bvarepsilon}{\mbox{\boldmath $\varepsilon$}}
\newcommand{\btriangle}{\mbox{\boldmath $\triangle$}}
\newcommand{\bdelta}{\mbox{\boldmath $\Delta$}}
\newcommand{\beps}{\mbox{\boldmath $\epsilon$}}
\newcommand{\btheta}{\mbox{\boldmath $\theta$}}
\newcommand{\balpha}{\mbox{\boldmath $\alpha$}}
\newcommand{\bfbeta}{\mbox{\boldmath $\beta$}}
\newcommand{\bsphi}{\mbox{\boldmath $\varphi$}}
\newcommand{\bsig}{\mbox{\boldmath $\sigma$}}
\newcommand{\bfpsi}{\mbox{\boldmath $\psi$}}
\newcommand{\bfdelta}{\mbox{\boldmath $\delta$}}
\newcommand{\bsigma}{{\bf \Sigma}}
\newcommand{\bzero}{{\bf 0}}
\newcommand{\bone}{{\bf 1}}
\newcommand{\bpsi}{\mbox{\boldmath $\Psi$}}
\newcommand{\bep}{\mbox{\boldmath $\epsilon$}}
\newcommand{\bomega}{\mbox{\boldmath $\Omega$}}
\newcommand{\bfomega}{\mbox{\boldmath $\omega$}}
\newcommand{\blambda}{\mbox{\boldmath $\Lambda$}}
\newcommand{\bflambda}{\mbox{\boldmath $\lambda$}}
\newcommand{\bfsigma}{\mbox{\boldmath $\sigma$}}
\newcommand{\bfpi}{{\mbox{\boldmath $\pi$}}}
\newcommand{\bupsilon}{\mbox{\boldmath $\upsilon$}}
\newcommand{\obs}{{\rm obs}}
\newcommand{\mis}{{\rm mis}}
\theoremstyle{plain}
\newtheorem{criterion}{Criterion}
\theoremstyle{definition}
\newtheorem{condition}[theorem]{Condition}
\usepackage[]{graphicx}
\chardef\bslash=`\\ % p. 424, TeXbook
\newcommand{\ntt}{\normalfont\ttfamily}
\newcommand{\cn}[1]{{\protect\ntt\bslash#1}}
\newcommand{\pkg}[1]{{\protect\ntt#1}}
\let\fn\pkg
\let\env\pkg
\let\opt\pkg
\hfuzz1pc % Don't bother to report overfull boxes if overage is < 1pc
\newcommand{\envert}[1]{\left\lvert#1\right\rvert}
\let\abs=\envert


% \usepackage[utf8x]{inputenc}
\usepackage{bm}
\usepackage{amsthm,amsmath,amssymb,amsfonts,amssymb}
\usepackage{listings}
% \usepackage{booktabs}
\usepackage{enumerate}
% \usepackage{subfigure}
% \usepackage{caption}
% \usepackage{subcaption}
% \usepackage{capt-of}
\usepackage{enumitem}
\setlist[enumerate,1]{label={(\roman*)}}
\usepackage[figuresright]{rotating}
% \usepackage{lmodern}
% \usepackage[a4paper, left=1in,right=1in,top=1.5in,bottom=1.5in]{geometry}
% \usepackage{babel}
% \usepackage{setspace}
% \usepackage[draft]{fixme}
\usepackage{mathtools}
%\usepackage[nolists,heads,tablesfirst]{endfloat}
% \usepackage{hyphenat}
% \usepackage[section]{placeins}
% \usepackage{xcolor}
\usepackage[dvipsnames]{xcolor}
\usepackage{xurl}
\usepackage{hyperref}
\hypersetup{
colorlinks,
citecolor=black,
filecolor=black,
linkcolor=black,
urlcolor=black
}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\def\subsubsectionautorefname{Section}

\definecolor{backgroundColour}{rgb}{0.97,0.96,0.97}
\lstset {
  language={C++},
  numbers=none,
  frame=none,
  includerangemarker=false,
  basicstyle=\small\ttfamily, % basic font setting
  backgroundcolor=\color{backgroundColour},
% https://tex.stackexchange.com/questions/68091/how-do-i-add-syntax-coloring-to-my-c-source-code-in-beamer
  keywordstyle=\color{BrickRed}\textbf,
  emph={DATA_VECTOR, DATA_INTEGER, PARAMETER, PARAMETER_VECTOR, sum, exp, dnorm, Type, dpois, Gamma_w2n, Stat_dist, setOnes, size,
        ADREPORT, objective_function,
        lambda, tlambda, gamma, tgamma, vector, matrix},
  emphstyle=\color{BrickRed}\textbf,
  emph={[2]a, b, x, y, m, n, i, j, sigma, tsigma, nll},
  emphstyle={[2]\color{Green}},
  commentstyle=\color{Gray}\textit,
  % morecomment=[l][\color{magenta}]{\#}
}

% \onehalfspacing
% \newcommand{\dnorm}[3][x]{\frac{1}{\sqrt{2\pi}#3}\exp\biggl[-\frac{1}{2}\biggl(\frac{#1-#2}{#3}\biggr)^2\biggr]}
% \newcommand{\dnormvar}[3][x]{\frac{1}{\sqrt{2\pi(#3)}}\exp\biggl[-\frac{1}{2}\frac{(#1-#2)^2}{#3}\biggr]}
% 
% \newcommand{\ndist}[1]{\mathcal{N}(#1)} % Normal distribution
% \newcommand{\mn}[1]{\overline{#1}} % Mean: bar or overline?
% \newcommand{\est}[1]{\widehat{#1}} % Estimator/estimate: hat?
% \newcommand{\altest}[1]{\widetilde{#1}} % Estimator/estimate: hat?
% % \mkern2mu\overline{\mkern-2mu F}_i  \overline{F}_i
% 
% \DeclareMathOperator{\LW}{LW}
% \DeclareMathOperator{\Cov}{Cov}         % Covariance
% \DeclareMathOperator{\corr}{corr}       % Correlation
% \DeclareMathOperator{\Var}{Var}         % Variance
% \DeclareMathOperator{\E}{\mathbb{E}}    % Expected value
% \DeclareMathOperator{\diag}{diag}       % Diagonal matrix
% \DeclareMathOperator*{\argmax}{argmax}

% \newcommand*\diff{
% \mathop{}\nobreak
% \mskip-\thinmuskip\nobreak
% \mathrm{d}
% }
% 
% \newcommand*\pdiff{
% \mathop{}\nobreak
% \mskip-\thinmuskip\nobreak
% \partial
% }
% 
% % \newcommand{\abbr}[1]{\textsc{\lowercase{#1}}}
% \newcommand{\abbr}[1]{#1}
% \newcommand{\kort}[1]{\abbr{#1}}
% 
% \providecommand*\ifrac[2]{
% \begingroup #1 \endgroup
% % A Close / so no space before it (unless a punctuation atom is the
% % last item in #1 but unlikely).
% \mathclose{/}%
% % Followed by an empty Open. Thus no space is inserted before the
% % denominator.
% \mathopen{}%
% \begingroup #2 \endgroup
% }
% 
% % Absolute value and norm
% \newcommand{\abs}[1]{\lvert #1 \rvert}
% \newcommand{\eqdef}{\ensuremath{\stackrel{\mathrm{def}}{=}}}


%\bibliographystyle{jf}
%\bibpunct{(}{)}{;}{a}{}{,}

% Bold letters
% \def\balpha{\bm{\alpha}}
% \def\bbeta{\bm{\beta}}
% \def\bdelta{\bm{\delta}}
% \def\bGamma{\bm{\Gamma}}
% \def\btheta{\bm{\theta}}
% \def\bpsi{\bm{\psi}}
% \def\bphi{\bm{\phi}}
% \def\bI{\bm{I}}
% \def\bU{\bm{U}}
% \def\bT{\bm{T}}
% \def\b1{\bm{1}}
% \def\bP{\bm{P}}
% \def\bx{\bm{x}}
% \def\blambda{\bm{\lambda}}

% % Matrices and transpose symbol:
% \newcommand{\matr}[1]{\boldsymbol{#1}}
% \newcommand{\tr}{^{\mathrm{T}}} % Transponert
% \newcommand{\startr}{^{\star{}\mathrm{T}}} % Transponert
% \newcommand{\eps}{\varepsilon}
% 
% 
% \newcommand{\argmin}{\operatornamewithlimits{arg\,min}}
\newcommand{\argmax}{\operatornamewithlimits{arg\,max}}
% \newcommand{\dcon}{\ensuremath{\stackrel{\mathrm{d}}{\to}}}


% \usepackage{url}

% \newenvironment{subfigures}{}{}
% \newenvironment{subtables}{}{}


% \newcommand{\dataset}[1]{\nohyphens{\texttt{#1}}}
% \newcommand{\package}[1]{\dataset{#1}}
% \newcommand{\program}[1]{\dataset{#1}}
% 
% \makeatletter
% \def\maxwidth{.7\textwidth}
% % {%
% % \ifdim\Gin@nat@width>\linewidth
% % \linewidth
% % \else
% % \Gin@nat@width
% % \fi
% % }
% \makeatother

%\usepackage[center]{titlesec}
%\renewcommand{\thetable}{\Roman{table}}
%\renewcommand{\thesection}{\Roman{section}.}
%\renewcommand\thesubsection{\Alph{subsection}.} 

%\hyphenation{Huft-hammer}

% \usepackage{dcolumn}
% \newcolumntype{e}{D{.}{.}{9}}

% \numberwithin{equation}{section}

% \theoremstyle{plain}
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{proposition}{Proposition}[section]
% \newtheorem{lemma}{Lemma}[section]
% 
% \theoremstyle{definition}
% \newtheorem{definition}{Definition}[section]
% \newtheorem{example}{Example}[section]
% 
% \theoremstyle{remark}
% \newtheorem*{remark}{Remark}

% \providecommand{\abs}[1]{\lvert#1\rvert}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}              
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% Import R files here with proper labels
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in TMB::compile("{}./code/poi\_hmm.cpp"{}): Compilation failed}}\end{kframe}
\end{knitrout}

% \author{Timothee Bacri$^{1,*}$\email{timothee.bacri@uib.no} \\
% University of Bergen, 5007 Bergen, Norway
% \and
% Jan Bulla$^{2,**}$\email{jan.bulla@uib.no}\\
% University of Bergen, 5007 Bergen, Norway
% \and
% Geir Drage Berentsen$^{3,***}$\email{geir.berentsen@nhh.no}\\
% Norwegian School of Economics, Helleveien 30, 5045 Bergen, Norway
% }

%\DOIsuffix{bimj.DOIsuffix}
\DOIsuffix{bimj.200100000}
\Volume{52}
\Issue{61}
\Year{2020}
\pagespan{1}{}
\keywords{Hidden Markov Model; TMB; Confidence intervals;\\
\noindent \hspace*{-4pc}
% {\small\it (Up to five keywords are allowed and should be given in alphabetical order. Please capitalize the key}\\
\hspace*{-4pc} %{\small\it words)}
\\[1pc]
\noindent\hspace*{-4.2pc} Supporting Information for this article is available from the author or on the WWW under\break \hspace*{-4pc}
\underline{\url{https://github.com/timothee-bacri/hmm-tmb}} % (please delete if not
% applicable)
}  %%% semicolon and fullpoint added here for keyword style

\title[Running title]{Fast parameter and confidence interval estimation for Hidden Markov Models using Template Model Builder}
%% Information for the first author.
\author[First Author{\it{et al.}}]{Timothee Bacri\footnote{Corresponding author: {\sf{e-mail: timothee.bacri@uib.no}}, Phone: +33-636-775-063}\inst{1}}
\address[\inst{1}]{Department of Statistics, University of Bergen, 5007 Bergen, Norway}
%%%%    Information for the second author
\author[dd]{Jan Bulla\inst{1}}
%%%%    Information for the third author
\author[]{Geir D. Berentsen\inst{2}}
\address[\inst{2}]{Department of Business and Management Science, Norwegian School of Economics, Helleveien 30, 5045 Bergen, Norway}
%%%%    \dedicatory{This is a dedicatory.}
\Receiveddate{zzz} \Reviseddate{zzz} \Accepteddate{zzz}

\begin{abstract}
Hidden Markov Models (HMMs) are a class of models widely used in speech recognition \citep[See e.g.][]{gales, fredkin}.
There are straightforward ways to compute Maximum Likelihood Estimates (MLE) of their parameters, but getting confidence intervals can be more difficult \citep[See e.g.][]{zucchini, lystig}.
In addition, computing MLEs can be time-consuming when the dataset and the complexity become very large.
We show in this paper a way to speed up computation by up to 50 times in the language R when compared to usual optimization approaches, and at the same time retrieve standard errors easily.
In a first part, we see how to optimize HMMs with the {\tt{TMB}} package in R and how to retrieve confidence intervals.
In a second part, we compare different optimizers such as {\tt{nlminb}}, and minimize the negative log-likelihood directly on different datasets: a small one (240 data points) from \citet{leroux}, a medium sized simulated one (2000 data points), and a large one (87648 data points) from a hospital.
% 
% Why we did it
% What problem/question did we address
% What did we find
% What is new (compare with best approaches)
% How did we do it
% 
% 
% 1-2 sentences Basic intro, audience = anyone
% 2-3 intro to audience in related discipline
% 1 general problem in this paper
% 1 "here we show" summarize result
% 2-3 explain what result reveals compared to what was thought/how it adds to previous knowledge
% 1-2 put result in more general context
% 2-3 broader perspective (discussion points)
\end{abstract}

\maketitle

% \tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short paragraph about HMMs and problems with speed and uncertainty evaluation.

Hidden Markov models (HMMs) are a versatile type of model that have been employed in many different situations since their introduction by \citet{baum}.
As an example, \citet{fredkin} applied them in speech recognition, \citet{lystig} to rainfall occurence data, and \citet{schadt} to phylogenetic trees.
\citet{fredkin} and \citet{zucchini} are references in the theory of HMMs.
Evaluating uncertainty and getting confidence intervals in HMMs is uneasy and not necessarily reliable.
Although \citet[][Chapter 12]{cappe} showed it can be achieved using asymptotic normality of the MLEs of the parameters under certain conditions, \citet[p. ~53]{fruhwirth-schnatter} points out that in independent mixture models, ``The regularity conditions are often violated''.
\citet[p. ~68]{mclachlan} adds that ``In particular for mixture models, it is well known that the sample size $n$ has to be very large before the asymptotic theory of maximum likelihood applies.''
\citet{lystig} shows a way to compute the exact Hessian, and \citet{zucchini} shows another way to compute the approximate Hessian and thus confidence intervals but admits that ``the use of the Hessian to compute standard errors (and thence confidence intervals) is unreliable if some of the parameters are on or near the boundary of their parameter space''.

{\tt{TMB}} (Template Model Builder) is an R package for efficient fitting of complex statistical random effect models to data, as described by \citet{kristensen}.
It provides exact calculations of first and second order derivatives of the likelihood of a model by automatic differentiation, which allows for efficient gradient and/or Hessian based optimization of the likelihood as well as uncertainty estimates by means of the Hessian.
The Hessian is not necessarily directly applicable for evaluating parameter uncertainty in HMMs as there are several parameter constraints in these models.
This can be adressed by constraint optimization, and subsequently combining the Hessian with the Jacobian of the constraints to obtain the covariance matrix as shown by \citet{visser}.
Alternatively, \citet{zucchini} shows how the constraints can be imposed by suitable transformations of the parameters.
The covariance matrix of the untransformed (original) parameters can then be retrieved by the delta method, a feature implemented in {\tt{TMB}}.

In this paper, we first show how to optimize Poisson HMMs with the help of {\tt{TMB}} in the language R.
Afterwards, we explain how to make a nested model through an example, how to effortlessly compute confidence intervals, how to fetch interesting probabilities, and we apply a Poisson HMM on a real hospital dataset.
Eventually, we see that {\tt{TMB}} can accelerate traditional optimizers in R by up to approximately 50 times on a fairly large dataset, and can easily output confidence intervals similar to bootstrap and profile methods via its Hessian based approach.

Maximum likelihood estimation can be achieved either by a direct numerical maximization as introduced by \citet{turner} and later detailed with R code by \citet{zucchini}, by an Expectation-Maximization (EM) based algorithm as described by \citet{bauma}, or by a mixture of the 2 as shown by \citet{bulla}.
We decide to use a direct maximization approach instead of the EM algorithm.
The reason is that the direct maximization approach is easier to adapt if one wants to fit different and more complex models.
It also deals easily with missing observations, whereas the EM approach is more complex.

% Selling points to include:
% \begin{enumerate}
% \item Two aspects of the paper: 1) Speedup of estimation 2) Esier and faster evaluation of parameter uncertainty
% \item Speed important when $T$ and $m$ large?
% \item Hessian based uncertainty earlier in the traditional sense not feasable \citep{visser}, and finite-differences must be employed since the computation of the Hessian not feasible.
% \item Bootstrap methods requires speed
% \item Profile methods requires speed
% \end{enumerate}
% 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Principles of using {\tt{TMB}} for Maximum Likelihood Estimation (MLE)}
\label{sec:principles}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In order to keep this tutorial at acceptable length, all sections follow the same concept.
That is, the reader is encouraged to consult our GitHub repository in parallel (https://github.com/(TIMO:MAKE A REPOSITORY)).
This permits to copy-paste or download all the scripts presented in this tutorial for each section.
Moreover, the repository also contains additional explanations, comments, and scripts.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Setup}
\label{sec:setup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Execution of our routines requires the installation of the R-package {\tt{TMB}} and the software {\tt{Rtools}}, where the latter serves for compiling C++ code.
In order to ensure reproducibility of all results involving the generation of random numbers, the \texttt{set.seed} function requires R version number 3.6.0 or greater.
Our scripts were tested on an Intel(R) Core(TM) i7-8700 processor running under Windows 10 Enterprise version 1809.

In particular for beginners, those parts of scripts involving C++ code can be difficult to debug because the code operates using a specific template.
Therefore it is helpful to know that {\tt{TMB}} provides a debugging feature, which can be useful to retrieve diagnostic error messages, in RStudio.
Enabling this feature is optional and can be achieved by the command \texttt{TMB:::setupRStudio()} (requires manual confirmation and re-starting RStudio).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Linear regression example}
\label{sec:linreg}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We begin by demonstrating the principles of {\tt{TMB}}, which we illustrate through the fitting procedure for a simple linear model.
This permits, among other things, to show how to handle parameters subject to constraints, an aspect particularly relevant for HMMs.
A more comprehensive tutorial on {\tt{TMB}} presenting many technical details in more depths is available at\\
\underline{\url{https://kaskr.github.io/adcomp/\_book/Tutorial.html}}.

Let $\bm{x}$ and $\bm{y}$ denote the predictor and response vector, respectively, both of length $n$.
For a simple linear regression model with intercept $a$ and slope $b$, the negative log-likelihood equals
\begin{equation*}
- l(a, b, \sigma^2) = - \sum_{i=1}^n \log(\phi(y_i; a + bx_i, \sigma^2))),
\end{equation*}
where $\phi(\cdot; \mu, \sigma^2)$ corresponds to the density function of the univariate normal distribution with mean $\mu$ and variance $\sigma^2$.

The use of {\tt{TMB}} requires the (negative) log-likelihood function to be coded in C++ under a specific template, which is then loaded into R.
The minimization of this function and other post-processing procedures are all carried out in R.
Therefore, we require two files.
The first file, named \textit{linreg.cpp}, is written in C++ and defines the negative log-likelihood (nll) function of the linear model as follows.\\

\lstinputlisting{code/linreg.cpp}

Note that we define data inputs $x$ and $y$ using the \texttt{DATA\_VECTOR()} declaration in the above code.
Furthermore, we declare the nll as a function of the three parameters a, b and $\log(\sigma)$ using the \texttt{PARAMETER()} declaration.
In order to be able to carry out unconstrained optimization procedures in the following, the nll is parameterized in terms of $\log(\sigma)$.
While the parameter $\sigma$ is constrained to be non-negative, $\log(\sigma)$ can be freely estimated.
Alternatively, constraint optimization methods could be carried out, but we do not investigate such procedures.
The \texttt{ADREPORT()} function is optional but useful for parameter inference at the postprocessing stage.
%This approach avoids the need of using constraint optimization methods.

The second file needed is written in R and serves for compiling the nll function defined above and carrying out the estimation procedure by numerical optimization of the nll function.
The .R file (shown below) carries out the compilation of the C++ file and minimization of the objective function:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Loading TMB package}
\hlkwd{library}\hlstd{(TMB)}
\hlcom{# Compilation. The compiler returns 0 if the compilation of}
\hlcom{# the cpp file was successful}
\hlstd{TMB}\hlopt{::}\hlkwd{compile}\hlstd{(}\hlstr{"code/linreg.cpp"}\hlstd{)}
\end{alltt}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in TMB::compile("{}code/linreg.cpp"{}): Compilation failed}}\begin{alltt}
\hlcom{# Dynamic loading of the compiled cpp file}
\hlkwd{dyn.load}\hlstd{(}\hlkwd{dynlib}\hlstd{(}\hlstr{"code/linreg"}\hlstd{))}
\hlcom{# Generate the data for our test sample}
\hlkwd{set.seed}\hlstd{(}\hlnum{123}\hlstd{)}
\hlstd{data} \hlkwb{<-} \hlkwd{list}\hlstd{(}\hlkwc{y} \hlstd{=} \hlkwd{rnorm}\hlstd{(}\hlnum{20}\hlstd{)} \hlopt{+} \hlnum{1}\hlopt{:}\hlnum{20}\hlstd{,} \hlkwc{x} \hlstd{=} \hlnum{1}\hlopt{:}\hlnum{20}\hlstd{)}
\hlstd{parameters} \hlkwb{<-} \hlkwd{list}\hlstd{(}\hlkwc{a} \hlstd{=} \hlnum{0}\hlstd{,} \hlkwc{b} \hlstd{=} \hlnum{0}\hlstd{,} \hlkwc{tsigma} \hlstd{=} \hlnum{0}\hlstd{)}
\hlcom{# Instruct TMB to create the likelihood function}
\hlstd{obj_linreg} \hlkwb{<-} \hlkwd{MakeADFun}\hlstd{(data, parameters,} \hlkwc{DLL} \hlstd{=} \hlstr{"linreg"}\hlstd{,}
                        \hlkwc{silent} \hlstd{=} \hlnum{TRUE}\hlstd{)}
\hlcom{# Optimization of the objective function with nlminb}
\hlstd{mod_linreg} \hlkwb{<-} \hlkwd{nlminb}\hlstd{(obj_linreg}\hlopt{$}\hlstd{par, obj_linreg}\hlopt{$}\hlstd{fn)}
\hlstd{mod_linreg}\hlopt{$}\hlstd{par}
\end{alltt}
\begin{verbatim}
##           a           b      tsigma 
##  0.31009240  0.98395535 -0.05814659
\end{verbatim}
\end{kframe}
\end{knitrout}

In addition to the core functionality presented above, different types of post-processing of the results are possible as well. For example, the function \texttt{sdreport} returns the estimates and standard errors of the parameters in terms of which the nll is parameterized:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{sdreport}\hlstd{(obj_linreg)}
\end{alltt}
\begin{verbatim}
## sdreport(.) result
##           Estimate Std. Error
## a       0.31009240 0.43829083
## b       0.98395535 0.03658781
## tsigma -0.05814659 0.15811381
## Maximum gradient component: 6.931683e-05
\end{verbatim}
\end{kframe}
\end{knitrout}
The standard errors above are based on the Hessian matrix of the nll.
From a practical perspective, it is usually desirable to obtain standard errors for the constrained variables, in this case $\sigma$. To achieve this, one can run the \texttt{summary} function with argument \texttt{select = "report"}:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{summary}\hlstd{(}\hlkwd{sdreport}\hlstd{(obj_linreg),} \hlkwc{select} \hlstd{=} \hlstr{"report"}\hlstd{)}
\end{alltt}
\begin{verbatim}
##        Estimate Std. Error
## sigma 0.9435116  0.1491822
\end{verbatim}
\end{kframe}
\end{knitrout}
These standard errors result from the generalized delta method described by \citet{kass}, which is implemented within {\tt{TMB}}. Note that full functionality of the \texttt{sdreport} function requires calling the function \texttt{ADREPORT} on the additional parameters of interest (i.e. those including transformed parameters, in our example $\sigma$) in the C++ part.
The \texttt{select} argument restricts the output to variables passed by \texttt{ADREPORT}.
This feature is particularly useful when the likelihood has been reparameterized as above, and is especially relevant for HMMs.
Following \citet{zucchini}, we refer to the original parameters as natural parameters, and to their transformed version as the working parameters.\\
Last, we display the estimation results from the \texttt{lm} function for comparison.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{lm}\hlstd{(y} \hlopt{~} \hlstd{x,} \hlkwc{data} \hlstd{= data)}\hlopt{$}\hlstd{coefficients}
\end{alltt}
\begin{verbatim}
## (Intercept)           x 
##   0.3100925   0.9839554
\end{verbatim}
\end{kframe}
\end{knitrout}
Note that minor deviations from the results of \texttt{lm} originate in the numerical methods involved in the selected optimization procedure, in our case \texttt{nlminb}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Parameter estimation techniques for HMMs}
\label{sec:estimation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section we recall basic concepts underlying parameter estimation for HMMs via direct numerical optization of the likelihood. In terms of notation, we stay as close as possible to \citet{zucchini}, where a more detailed presentation is available.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Basic notation and model setup}
\label{sec:notation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A large variety of modeling approaches is possible with HMMs, ranging from rather simple to highly complex setups.
In a basic HMM, one assumes that the data-generating process corresponds to a time-dependent mixture of the so-called conditional distributions.
More specifically, the mixing process is driven by an unobserved (hidden) homogeneous Markov chain.
In this paper we focus on a Poisson HMM, but only small changes are necessary to adapt our scripts to models with other conditional distributions.
Let $\{X_t: t = 1, \ldots, T\}$ and $\{C_t : t = 1, \ldots, T\}$ denote the observed and hidden process, respectively.
For an $m$-state Poisson HMM, the conditional distributions with parameter $\lambda_i$ are then specified through
\begin{equation*}
p_i(x) = P(X_t = x \vert C_t = i) = \frac{e^{-\lambda_i} \lambda_i^x}{x!},
\end{equation*}
where $i = 1, \ldots, m$. Furthermore, we let $\bgamma = \{\gamma_{ij}\}$ and $\bfdelta$ denote the transition probability matrix (TPM) of the Markov chain and the corresponding stationary distribution, respectively.
It is noteworthy that Markov chains in the context of HMMs are often assumed irreducible and aperiodic.
For example,
\citet[Lemma 6.3.5 on p. ~225 and Theorem 6.4.3 on p. ~227]{grimmett} show that irreducibility ensures the existence of the stationary distribution, and \citet[p. ~394]{feller} describe that aperiodicity implies that a unique limiting distribution exists and corresponds to the stationary distribution.
These results are, however, of limited relevance for most estimation algorithms, because the elements of $\bgamma$ are in general strictly positive. Nevertheless, one should be careful when manually setting selected elements of $\bgamma$ equal to zero.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The likelihood function of an HMM}
\label{sec:hmm_likelihood}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The likelihood function of an HMM requires, in principle, an summation over all possible state sequences. As shown e.g.~by \citet[p.~37]{zucchini}, a computationally convenient representation as a product of matrices is possible. Let $X^{(t)} = \{X_1, \ldots, X_t \}$ and $x^{(t)} = \{x_1, \ldots, x_t \}$ denote the history of the observed process $X_t$ and the observations $x_t$ from time zero up to time $t$. Moreover, let $\btheta = (\gamma_{11}, \ldots, \gamma_{1m}, \ldots, \gamma_{m1}, \ldots, \gamma_{mm}, \lambda_1,\ldots, \lambda_m)$ denote the vector of model parameters. Given these parameters, the likelihood of the observations $\{x_1, \ldots, x_T \}$ can then be expressed as
\begin{equation}
\label{eq:hmm_likelihood}
L_T = \bcp(X^{(T)} = x^{(T)}) = \bfdelta \bcp(x_1) \bgamma \bcp(x_2) \bgamma \bcp(x_3) \ldots \bgamma \bcp(x_T) \bone',
\end{equation}
where
\begin{equation*}
\bcp(x) = \begin{pmatrix}
p_1(x)    &         &         & 0\\
          & p_2(x)  &         &\\
          &         & \ddots  &\\
0         &         &         & p_m(x)
\end{pmatrix}
\end{equation*}
corresponds to a diagonal matrix with the $m$ conditional probability density functions (pdf) evaluated at $x$ (we will use the term density despite the discrete support), and $\bone$ denotes a vector of ones. The first element of the likelhood function, the so-called initial distribution, is given by the stationary distribution $\delta$ here. Alternatively, the initial distribution may be estimated freely, which requires minor changes to the likelihood function discussed in Section \ref{sec:tmb_cpp}.\\
(JAN:) Note that the treatment of missing data is comparably straightforward in this setup. If $x$ is a missing observations, one just has to set $p_i(x) =  1$, thus $\bcp(x)$ reduces to the unity matrix as detailed in \citet[p. ~40]{zucchini}.
\citet[p. ~41]{zucchini} also explains how to adjust the likelihood when entire intervals are missing. Furthermore, this representation of the likelihood is quite natural from an intuitive point of view. From left to right, it can be interpreted as a pass through the observations: one starts with the initial distribution multiplied by the conditional density of $x_1$ collected in $\bcp(x_1)$. This is followed by iterative multiplications with the TPM modeling the transition to the next observation, and yet another multiplication with contributions of the following conditional densities.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Forward algorithm and backward algorithm}
\label{sec:hmm_fwbw}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The pass through the observations described above actually forms the basis for an efficient evaluation of the  likelihood function. More precisely, the so-called ``forward algorithm'' allows for a recursive computation of the likelihood. For setting up this algorithm, we need to define the vector $\balpha_t$ by
\begin{align*}
\balpha_t &= \bfdelta \bcp(x_1)\bgamma \bcp(x_2) \bgamma \bcp(x_3) \ldots \bgamma \bcp(x_t)\\
&= \bfdelta \bcp(x_1) \prod_{s=2}^{t}\bgamma \bcp(x_s)\\
&= \left( \alpha_t(1), \ldots, \alpha_t(m) \right)
\end{align*}
for $t = 1, 2, \ldots, T$. 
 The name forward algorithm originates from the way of calculating $\balpha_t$, i.e.
\begin{gather*}
\balpha_0 = \bfdelta\\
\balpha_t = \balpha_{t-1} \bgamma \bcp(x_t) \text{ for } t = 1, 2, \ldots, T.
\end{gather*}
After a pass through all observations, the likelihood results from
\begin{gather*}
L_T = \balpha_T \bone'.
\end{gather*}
In a similar way, the ``backward algorithm'' also permits the recursive computation of the likelihood, but starting with the last observation. To formulate the backward algorithm, let us define the vector $\bfbeta_t$ for $t = 1, 2,
\ldots, T$ so that
\begin{align*}
\bfbeta'_t &= \bgamma \bcp(x_{t+1}) \bgamma \bcp(x_{t+2}) \ldots \bgamma \bcp(x_T) \ldots \bone'\\
&= \left(\prod_{s=t+1}^{T}\bgamma \bcp(x_s) \right) \bone'\\
&= \left( \beta_t(1), \ldots, \beta_t(m) \right)
\end{align*}
The name backward algorithm results from the way of calculating $\bfbeta_t$, i.e.
\begin{gather*}
\bfbeta_T = \bone'\\
\bfbeta_t = \bgamma \bcp(x_{t+1}) \bfbeta_{t+1} \text{ for } t = T-1, T-2, \ldots, 1.
\end{gather*}
Again, the likelihood can be calculated after a pass through all observations by
\begin{gather*}
L_T = \bfdelta \bfbeta_1.
\end{gather*}
In general, parameter estimation bases on the forward algorithm. The backward algorithm is, however, still useful because the quantities $\balpha_t$ and $\bfbeta_t$ together serve for a couple of interesting tasks. For example, they are the basis for state inference, forecasting,... TIMO: please add the applications and references to the chapter / sections / pages of Zucchini. Last, it is well-known that the execution of the forward (or backward) algorithm may quickly lead to underflow errors, because many (for discrete distributions: all) elements of the vectors and matrices involved take values between zero and one. To avoid these difficulties, a scaling factor can be introduced. We follow the approach suggested by \citet[p. ~48]{zucchini} and implement a scaled version of the forward algorithm, which directly provides the (negative) log-likelihood as result.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Reparameterization of the likelihood function}
\label{sec:hmm_repar}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The representation of the likelihood and the algorithms shown above rely on the data and the set of parameters $\btheta$ as input. The data are subject to several constraints:
\begin{enumerate}
\item Typically there are various constraints of the parameters in the conditional distribution. For the Poisson HMM, all elements of the parameter vector $\bflambda\ = (\lambda_1, \dots, \lambda_m)$ must be non-negative. 
\item In general, the parameters $\gamma_{ij}$ of the TPM $\bgamma$ have to be non-negative, and the rows of $\bgamma$ must sum up to one.
\end{enumerate}
The constraints of the TPM can be difficult to deal with using constrained optimization of the likelihood. A common approach is to reparameterize the log-likelihood in terms of unconstrained ``working'' parameters $\{\bct, \bfeta\}= g^{-1}(\{\bgamma, \bflambda\})$, as follows. A possible reparameterisation of $\bgamma$ is given by 

\begin{equation*}
\gamma_{ij} = \frac{\exp(\tau_{ij})}{1 + \sum_{k \neq i} \tau_{ik}}, \text{ for } i \neq j
\end{equation*}

where $\tau_{ij}$ are the $m(m-1)$ elements of an $m$ times $m$ matrix $\bct$ with no diagonal elements. The diagonal elements of $\bgamma$ follows implicitly from $\sum_j \gamma_{ij} = 1 \;\forall\; i$ \cite[p. ~51]{zucchini}. The corresponding reverse transformation is given by
\begin{equation*}
\tau_{ij} = \log\left(\frac{\gamma_{ij}}{1 - \sum_{k \neq i} \gamma_{ik}}\right) = \log(\gamma_{ij}/\gamma_{ii}), \text{ for } i \neq j
\end{equation*}

For the Poisson HMM the intensities can be reparameterised in terms of $\lambda_i = \exp(\eta_i), i = 1,\dots,m$.
Estimates of the "natural" parameters $\{\bgamma, \bflambda\}$ can then be obtained by maximising the reparameterised likelihood with respect to $\{\bct, \bfeta\}$ and then transforming the estimated working parameters back to natural parameters via the above transformations, i.e. $\{\hat{\bgamma}, \hat{\bflambda}\} = g(\{\hat{\bct}, \hat{\bfeta}\}$.



% As illustrated in the previous section, we focus on unconstrained optimization, similar \citet[p. ~50]{zucchini}.
% Hence, we parameterize the likelihood function as a function of the working parameters.
% The computation of the likelihood then requires to transform the working parameters back to natural parameters before calculating the negative log-likelihood via \autoref{eq:hmm_likelihood}.
% This part is entirely set up in the C++ likelihood function.
% 
% Since the transformation of the transition probability Matrix (TPM) is identical for all HMMs, it is stored as a separate C++ function \texttt{Gamma\_w2n} which is available in the file \textit{utils.cpp} (Appendix A.2 \autoref{code:Gamma_w2n}).
% This file also contains the C++ function \texttt{Delta\_w2n} which, if necessary, we can define to transform the working stationary distribution vector into a natural parameter (Appendix A.2 \autoref{code:Delta_w2n}).
% It is however unneeded for this paper.
% 
% Given a working parameter vector \texttt{log\_lambda}, the C++ function turning the working vector of the Poisson means into a natural parameter is simple:
% \begin{lstlisting}
% vector<Type> lambda = tlambda.exp();
% \end{lstlisting}
% 
% Such a transformation can be adapted to other conditional distributions.
% Note that the parts concerning the Markov chain remain unchanged.
% The working parameters returned then serve as initial values for starting the optimization procedure of the likelihood via {\tt{TMB}}.
% 


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Needed? / to be integrated later}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% 
% We call $\begin{pmatrix}
% p_1(x_1)  & p_2(x_1) & \ldots & p_m(x_1)\\
% p_1(x_2)  & p_2(x_2) & \ldots & p_m(x_2)\\
% \vdots    &  \vdots  & \ddots & \vdots\\
% p_1(x_T)  & p_2(x_T) & \ldots & p_m(x_T)\\
% \end{pmatrix}$ the emission probability matrix.\\
% 
% 
% Similarly to \citet{zucchini}, we choose to minimize the negative log-likelihood for convenience purposes.
% It is therefore better to maximize the log-likelihood instead.
% 
% 
% In summary, we need to define the state-dependent probabilities (from Poisson distributions) and the transformations in R and C++ (depending on the distributions used, but can be easily adapted).
% We show below the next step: defining the likelihood function in C++ and using it to model a dataset in R.
% 
% 
% In practice, as explained in \autoref{sec:principles}, we first create a set of natural parameters and turn them into working parameters before passing them to an optimizer.\\
% 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Using TMB}
\label{sec:tmb}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In the following we show how parameter estimation for HMMs can be carried out  efficiently via TMB. The TMB (R and C++) code used below is available from
https://github.com/(TIMO:MAKE A REPOSITORY) to allow for consistent maintenance of the code. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Likelihood function}
\label{sec:tmb_cpp}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Similar to the linear regression example presented in \ref{sec:linreg}, the first and essential step is to define our objective function to be minimized later in a suitable C++ file. In our case, this function calculates the negative log-likelihood presented in (\citet[p. ~48]{zucchini}), and our C++ code is analog to the R-code presented in (\citet[p. ~331 - 333]{zucchini}). This function, named \textit{poi\_hmm.cpp}, tackles our setting with conditional Poisson distributions only. An extension to for example Gaussian, binomial and exponential conditional distributions is straightforward. It only requires to modify the density function in the \textit{poi\_hmm.cpp} function and the related functions for parameter transformation presented in Section \ref{sec:hmm_repar}. We illustrate the implementation of these cases in the GitHub repository. However, note that the number of possible modelling setups is very large: e.g., the conditional distributions may vary from state to state, the conditional mean may be linked to covariates, or the TPM could depend on covariates - to name only a few. Due to the very large number of possible extensions of the basic HMM, we refrain from implementing an R-package, but prefer to provide a proper guidance to the reader for building custom models suited to a particular application. As a small example, we illustrate how to implement a freely estimated initial distribution in the function \textit{poi\_hmm.cpp}. This modification can be achieved by uncommenting a couple of lines only. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Optimization}
\label{sec:tmb_r}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

With the objective function available in C++, we can carry out the parameter estimation and all pre-/post-processing in R. The following steps describe the steps to be carried out.

\begin{enumerate}
\item Loading of the necessary packages, compilation of the objective function with TMB and subsequent loading, and loading of the auxiliary functions for parameter transformation.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Load TMB and optimization packages}
\hlkwd{library}\hlstd{(TMB)}
\hlkwd{library}\hlstd{(optimr)}
\hlcom{# Run the C++ file containing the TMB code}
\hlstd{TMB}\hlopt{::}\hlkwd{compile}\hlstd{(}\hlstr{"code/poi_hmm.cpp"}\hlstd{)}
\end{alltt}


{\ttfamily\noindent\bfseries\color{errorcolor}{\#\# Error in TMB::compile("{}code/poi\_hmm.cpp"{}): Compilation failed}}\begin{alltt}
\hlcom{# Load it}
\hlkwd{dyn.load}\hlstd{(}\hlkwd{dynlib}\hlstd{(}\hlstr{"code/poi_hmm"}\hlstd{))}
\hlcom{# Load the parameter transformation function}
\hlkwd{source}\hlstd{(}\hlstr{"functions/utils.R"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\item Loading of the observations. We chose the well-known data set presented in \citet{leroux}. It consists of the number of movements by a fetal lamb in 240 consecutive 5-second intervals. These counts are listed in \autoref{table:lamb_data} and are available from the github repository.\\
TIMO: move table here. This part "read from left to right and top to bottom" can also be mentioned in the table
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{load}\hlstd{(}\hlstr{"data/fetal-lamb.RData"}\hlstd{)}
\hlstd{lamb_data} \hlkwb{<-} \hlstd{lamb}
\end{alltt}
\end{kframe}
\end{knitrout}

\item Initialization of the number of states and starting (or initial) values for the optimization. First, the number of states needs to be determined. Usually one would fit models with a different number of states and evaluate them e.g.~in terms of model selection criteria, forecasting performance (TIMO: cite Rolands paper on determining number of states, and maybe others, insert references to bibliography). Since the results reported by \citet{leroux} show that a two-state model is preferred by the BIC, we focus on this model only here - although other choices would be possible, e.g.~the AIC selects a three-state model. The list object ABC contains the data and the number of states.\\
TIMO: how TMB underscore data instead of ABC?
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Model with 2 states}
\hlstd{m} \hlkwb{<-} \hlnum{2}
\hlstd{TMB_data} \hlkwb{<-} \hlkwd{list}\hlstd{(}\hlkwc{x} \hlstd{= lamb_data,} \hlkwc{m} \hlstd{= m)}
\end{alltt}
\end{kframe}
\end{knitrout}
Secondly, initial values for the optimization procedure need to be defined. Although we will apply unconstrained optimization, we initialize the natural parameters, because this is much more intuitive and practical than handling the working parameters. 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Generate initial set of parameters for optimization}
\hlstd{lambda} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{3}\hlstd{)}
\hlstd{gamma} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{0.8}\hlstd{,} \hlnum{0.2}\hlstd{,}
                  \hlnum{0.2}\hlstd{,} \hlnum{0.8}\hlstd{),} \hlkwc{byrow} \hlstd{=} \hlnum{TRUE}\hlstd{,} \hlkwc{nrow} \hlstd{= m)}
\end{alltt}
\end{kframe}
\end{knitrout}

\item Transformation from natural to working parameters. The previously created initial values are transformed and stored in the list {\tt parameters} for the optimization procedure.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Turn them into working parameters}
\hlstd{parameters} \hlkwb{<-} \hlkwd{pois.HMM.pn2pw}\hlstd{(m, lambda, gamma)}
\end{alltt}
\end{kframe}
\end{knitrout}

\item Creation of the {\tt TMB} negative log-likelihood function with its derivatives. This object, stored as {\tt obj\_tmb} requires the data, the initial values, and the previously created the DLL as input. Setting argument \texttt{silent = TRUE} disables tracing information and is only used here to avoid excessive output.\\
TIMO: you used the "backslash tt" command for TMB here, but no-where else. I recall we spoke about this - what is the journal's guideline for expressions such as TMB or R? Or R objects such as obj underscore tmb? 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{obj_tmb} \hlkwb{<-} \hlkwd{MakeADFun}\hlstd{(TMB_data, parameters,}
                     \hlkwc{DLL} \hlstd{=} \hlstr{"poi_hmm"}\hlstd{,} \hlkwc{silent} \hlstd{=} \hlnum{TRUE}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}
This object also contains the previously defined initial values as a vector (\texttt{par}) rather than a list. The negative log-likelihood (\texttt{fn}), its gradient (\texttt{gr}) and hessian (\texttt{he}) are functions of the parameters (on vector form) while the data are considered fixed:   
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{obj_tmb}\hlopt{$}\hlstd{par}
\end{alltt}
\begin{verbatim}
##   tlambda   tlambda    tgamma    tgamma 
##  0.000000  1.098612 -1.386294 -1.386294
\end{verbatim}
\begin{alltt}
\hlstd{obj_tmb}\hlopt{$}\hlkwd{fn}\hlstd{(obj_tmb}\hlopt{$}\hlstd{par)}
\end{alltt}
\begin{verbatim}
## [1] 306.9101
\end{verbatim}
\begin{alltt}
\hlstd{obj_tmb}\hlopt{$}\hlkwd{gr}\hlstd{(obj_tmb}\hlopt{$}\hlstd{par)}
\end{alltt}
\begin{verbatim}
##          [,1]     [,2]      [,3]     [,4]
## [1,] 167.9496 9.876427 -3.664494 40.12559
\end{verbatim}
\begin{alltt}
\hlstd{obj_tmb}\hlopt{$}\hlkwd{he}\hlstd{(obj_tmb}\hlopt{$}\hlstd{par)}
\end{alltt}
\begin{verbatim}
##             [,1]       [,2]       [,3]       [,4]
## [1,] 219.2189228   8.348694 -0.7809128 -0.7262055
## [2,]   8.3486938 -12.646088  6.4622217 14.6170124
## [3,]  -0.7809128   6.462222 -0.2996396 -3.7283669
## [4,]  -0.7262055  14.617012 -3.7283669 30.0355147
\end{verbatim}
\end{kframe}
\end{knitrout}



\item Execution of the optimization. For this step we rely again on the optimizer implemented in the {\tt{nlminb}} function. The arguments, i.e.~ initial values for the parameters and the function to be optimized, are extracted from the previously created TMB object. 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{mod_tmb} \hlkwb{<-} \hlkwd{nlminb}\hlstd{(}\hlkwc{start} \hlstd{= obj_tmb}\hlopt{$}\hlstd{par,} \hlkwc{objective} \hlstd{= obj_tmb}\hlopt{$}\hlstd{fn)}
\hlcom{# Check that it converged successfully}
\hlstd{mod_tmb}\hlopt{$}\hlstd{convergence} \hlopt{==} \hlnum{0}
\end{alltt}
\begin{verbatim}
## [1] TRUE
\end{verbatim}
\end{kframe}
\end{knitrout}
There are alternatives to {\tt{nlminb}}, but we focus on it here because of its good performance in terms of speed. Further details are available in Section \ref{sec:speed} \autoref{txt:all-methods-comparison}.

\item Obtaining the ML estimates of the natural parameters together with their standard errors is possible by using the previously introduced command \texttt{sdreport}. Recall that this requires the parameters of interest to be treated by the \texttt{ADREPORT} statement in the C++ part. It should be noted that the presentation of the set of parameters \texttt{gamma} below results from a column-wise representation of the TPM.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{summary}\hlstd{(}\hlkwd{sdreport}\hlstd{(obj_tmb),} \hlstr{"report"}\hlstd{)}
\end{alltt}
\begin{verbatim}
##          Estimate Std. Error
## lambda 0.25636530 0.04016451
## lambda 3.11475069 1.02131434
## gamma  0.98872130 0.01063573
## gamma  0.31033874 0.18468645
## gamma  0.01127870 0.01063573
## gamma  0.68966126 0.18468645
## delta  0.96493132 0.03181447
## delta  0.03506868 0.03181447
\end{verbatim}
\end{kframe}
\end{knitrout}
The value of the objective function in the minimum found by the optimizer can also be extracted directly from the object {\tt mod\_tmb} by accessing the list element {\tt objective}:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{mod_tmb}\hlopt{$}\hlstd{objective}
\end{alltt}
\begin{verbatim}
## [1] 177.5188
\end{verbatim}
\end{kframe}
\end{knitrout}


\item In the optimization above we already benefited from an increased speed due to the evaluation of the loglikelihood in C++ compared to the forward algorithm being executed entirely in R. However, the use of {\tt{TMB}} also permits to introduce the gradient and/or the Hessian computed by {\tt{TMB}} into the optimization procedure. This is in general advisable, because {\tt{TMB}} provides an exact value of both gradient and Hessian up to machine precision, which is superior to approximations used by optizing procedure. Similar to the objective function, both quantities can be extracted directly from the {\tt{TMB}} object {\tt obj\_tmb}:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# The negative log-likelihood is accessed by the objective}
\hlcom{# attribute of the optimized object}
\hlstd{mod_tmb} \hlkwb{<-} \hlkwd{nlminb}\hlstd{(}\hlkwc{start} \hlstd{= obj_tmb}\hlopt{$}\hlstd{par,} \hlkwc{objective} \hlstd{= obj_tmb}\hlopt{$}\hlstd{fn,}
                  \hlkwc{gradient} \hlstd{= obj_tmb}\hlopt{$}\hlstd{gr,} \hlkwc{hessian} \hlstd{= obj_tmb}\hlopt{$}\hlstd{he)}
\hlstd{mod_tmb}\hlopt{$}\hlstd{objective}
\end{alltt}
\begin{verbatim}
## [1] 177.5188
\end{verbatim}
\end{kframe}
\end{knitrout}
Note that passing the exact gradient and Hessian as provided by {\tt{TMB}} to \texttt{nlminb} leads to the same minimum, i.e.~value of the objective function.\\ TIMO/GEIR: can we say something as the following\\
However, the speed of convergence / number of iterations carried out differs / can differ and may be more important for other data sets and models (see Section ABC for a comparison).

\end{enumerate}

On a minor note, when comparing our estimation results to those reported by \citet{leroux}, some non-negligible differences can be noted. These reasons for this are difficult to determine, but some likely explanations are given in the following. First, differences in the parameter estimates may result e.g.~from the optimizing algorithms used and related setting (e.g.~convergence criterion, number of steps,...). Moreover, \citet{leroux} seem to base their calculations on an altered likelihood, which is reduced by removing the term $\sum_{i=1}^{T} \log(x_{i}!)$ from the log-likelihood (see Github for further details). This modifications my also possess an impact on the behavior of the optimization algorithm, as e.g.~relative convergence criteria and step size could be affected.\\

% TIMO/GEIR: I would put all the following on the GitHub site or into an appendix if we have the space.
% 
% The reason is that their likelihood is altered while ours isn't.
% This becomes clear when comparing the likelihoods with only one state.
% 
% A 1 state Poisson HMM is the same as a Poisson regression model, for which the log-likelihood has the expression
% \begin{align*}
% l(\lambda) &= \log \left(\prod_{i=1}^{T} \frac{\lambda^{x_i} e^{-\lambda}}{x_{i}!} \right)\\
% &= - T \lambda + \log(\lambda) \left( \sum_{i=1}^{T} x_i \right) - \sum_{i=1}^{T} \log(x_{i}!).
% \end{align*}
% The authors find a ML estimate $\lambda = 0.3583$ and a log-likelihood of -174.26.
% In contrast, calculating the log-likelihood explicitly shows a different result.
% <<leroux-likelihood-calculation>>=
% x <- lamb_data
% # We use n instead of T in R code
% n <- length(x)
% l <- 0.3583
% - n * l + log(l) * sum(x) - sum(log(factorial(x)))
% @
% 
% The log-likelihood is different, but when the constant $- \sum_{i=1}^{T} \log(x_{i}!)$ is removed, it matches our result.
% <<leroux-likelihood>>=
% - n * l + log(l) * sum(x)
% @
% In this paper, we use the complete formula for the negative log-likelihood, which therefore differs sligthly from \citet{leroux}.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{GEIR: MOVE ELSEWHERE / GitHub / Appendix? SUGGESTIONS? Computing the stationary distribution}
% \label{sec:stat_dist}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% Within the objective function in \autoref{sec:tmb_cpp}, the stationary distribution of the $m$ state HMM's Markov chain with transition probability matrix $\bgamma$ is calculated.
% In this section, we explain that calculation.
% 
% \citet{zucchini} shows that calculating the stationary distribution can be achieved by solving \autoref{eq:stat-dist} for $\bfdelta$, where $\bci_m$ is the $m*m$ identity matrix, $\bcu$ is a $m*m$ matrix of ones, and $\bone$ is a row vector of ones.
% \begin{equation}
% \bfdelta(\bci_m - \bgamma + \bcu) = \bone
% \label{eq:stat-dist}
% \end{equation}
% 
% An implementation of this in R is shown here
% <<stat.dist>>=
% @
% and used in the supporting information.
% 
% In order to use it in {\tt{TMB}}, an implementation in C++ is necessary under a specific template.
% The file \textit{utils.cpp} (Appendix A.2 \autoref{code:Stat_dist}) shows how to achieve this.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{GEIR: LEAVE HERE? DIFFERENT TITLE EG PARAMETER CONSTRAINS AND NESTED MODELS? Nested model specification}
\label{sec:nested}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Nested models can be useful for multiple reasons.
For example, there can be a need to fix some parameters because of some biological or physical constraints.

Using a nested model solves this issue at the cost of having a worse fit.
The reason is that some parameters are fixed and the others' estimates have different values than in the original model.
Since the estimates can vary, the fit worsens.

To showcase the advantage of nested models, we re-use the 2 state model.

{\tt{TMB}} can be instructed to treat some parameters as constants.
However, only working parameters can be fixed.
Nevertheless, in our situation we can fix a natural parameter ($\lambda$) for which we can easily identify the corresponding working parameter ($\log(\lambda)$) to fix, hence we do that.
Fixing a value in the transition probability matrix might be possible, but is clearly troublesome given the lack of a one-to-one correspondence with the natural parameters, so we fix a Poisson mean instead.

We arbitrarily fix $\lambda_1$ to 1.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Get the previous values, and fix some}
\hlstd{fixed_par_lambda} \hlkwb{<-} \hlstd{lambda}
\hlstd{fixed_par_lambda[}\hlnum{1}\hlstd{]} \hlkwb{<-} \hlnum{1}
\hlstd{fixed_par_gamma} \hlkwb{<-} \hlstd{gamma}
\end{alltt}
\end{kframe}
\end{knitrout}

Now that the nested model's natural parameters are constructed, we transform them into a set of working parameters.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Transform them into working parameters}
\hlstd{new_parameters} \hlkwb{<-} \hlkwd{pois.HMM.pn2pw}\hlstd{(}\hlkwc{m} \hlstd{= m,}
                                 \hlkwc{lambda} \hlstd{= fixed_par_lambda,}
                                 \hlkwc{gamma} \hlstd{= fixed_par_gamma)}
\end{alltt}
\end{kframe}
\end{knitrout}
In order for {\tt{TMB}} to treat parameters as constants, the \texttt{map} argument of the \texttt{MakeADFun} function must be a list.
This list must contain named vectors filled with \texttt{NA} for each fixed parameters, and unique factor levels for the regular parameters.

Equal factor levels are collected to common values, so we want a different factor level for each non-fixed parameter, and NA for each fixed parameter.
We can use increasing numbers to make factors, and replace with NA where necessary.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{map} \hlkwb{<-} \hlkwd{list}\hlstd{(}\hlkwc{tlambda} \hlstd{=} \hlkwd{as.factor}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{NA}\hlstd{,} \hlnum{1}\hlstd{)),}
            \hlkwc{tgamma} \hlstd{=} \hlkwd{as.factor}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{2}\hlstd{,} \hlnum{3}\hlstd{)))}

\hlcom{# The map is fed to the MakeADFun function}
\hlstd{fixed_par_obj_tmb} \hlkwb{<-} \hlkwd{MakeADFun}\hlstd{(TMB_data, new_parameters,}
                               \hlkwc{DLL} \hlstd{=} \hlstr{"poi_hmm"}\hlstd{,}
                               \hlkwc{silent} \hlstd{=} \hlnum{TRUE}\hlstd{,}
                               \hlkwc{map} \hlstd{= map)}
\hlstd{fixed_par_mod_tmb} \hlkwb{<-} \hlkwd{nlminb}\hlstd{(}\hlkwc{start} \hlstd{= fixed_par_obj_tmb}\hlopt{$}\hlstd{par,}
                            \hlkwc{objective} \hlstd{= fixed_par_obj_tmb}\hlopt{$}\hlstd{fn,}
                            \hlkwc{gradient} \hlstd{= fixed_par_obj_tmb}\hlopt{$}\hlstd{gr,}
                            \hlkwc{hessian} \hlstd{= fixed_par_obj_tmb}\hlopt{$}\hlstd{he)}
\end{alltt}
\end{kframe}
\end{knitrout}

The estimates and standard errors vary after fixing the parameters (\autoref{table:nested-model}) and the standard errors for the fixed parameters are zero.
% latex table generated in R 4.0.2 by xtable 1.8-4 package
% Fri Jan 15 12:53:02 2021
\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  & \multicolumn{2}{c}{Original model}& \multicolumn{2}{c}{Nested model}\\ \hline
 & Estimate & Std. Error & Estimate & Std. Error \\ 
  \hline
$\lambda_{1}$ & 0.26 & 0.04 & 1.00 & 0.00 \\ 
  $\lambda_{2}$ & 3.11 & 1.02 & 3.44 & 0.86 \\ 
  $\gamma_{1, 1}$ & 0.99 & 0.01 & 1.00 & 0.00 \\ 
  $\gamma_{2, 1}$ & 0.31 & 0.18 & 0.19 & 0.18 \\ 
  $\gamma_{1, 2}$ & 0.01 & 0.01 & 0.00 & 0.00 \\ 
  $\gamma_{2, 2}$ & 0.69 & 0.18 & 0.81 & 0.18 \\ 
  $\delta_{1}$ & 0.96 & 0.03 & 0.98 & 0.03 \\ 
  $\delta_{2}$ & 0.04 & 0.03 & 0.02 & 0.03 \\ 
   \hline
\end{tabular}
\caption{2 state Poisson HMM before and after fixing $\lambda_1$ to 1 using a nested model} 
\label{table:nested-model}
\end{table}


Since the nested model isn't the optimal model, the likelihood becomes worse, going from
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Original model negative log-likelihood}
\hlstd{mod_tmb}\hlopt{$}\hlstd{objective}
\end{alltt}
\begin{verbatim}
## [1] 177.5188
\end{verbatim}
\end{kframe}
\end{knitrout}

to
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Nested model negative log-likelihood}
\hlstd{fixed_par_mod_tmb}\hlopt{$}\hlstd{objective}
\end{alltt}
\begin{verbatim}
## [1] 264.1636
\end{verbatim}
\end{kframe}
\end{knitrout}

Note that some inconsistencies can happen.

The stationary distribution is a vector of probabilities and should sum to 1.
However, it doesn't behave as expected.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{adrep} \hlkwb{<-} \hlkwd{summary}\hlstd{(}\hlkwd{sdreport}\hlstd{(obj_tmb),} \hlstr{"report"}\hlstd{)}
\hlstd{estimate_delta} \hlkwb{<-} \hlstd{adrep[}\hlkwd{rownames}\hlstd{(adrep)} \hlopt{==} \hlstr{"delta"}\hlstd{,} \hlstr{"Estimate"}\hlstd{]}
\hlkwd{sum}\hlstd{(estimate_delta)}
\end{alltt}
\begin{verbatim}
## [1] 1
\end{verbatim}
\begin{alltt}
\hlkwd{sum}\hlstd{(estimate_delta)} \hlopt{==} \hlnum{1}
\end{alltt}
\begin{verbatim}
## [1] FALSE
\end{verbatim}
\end{kframe}
\end{knitrout}

As noted on \citet[pp. ~159-160]{zucchini}, ``with the specifications of $\bfdelta$, $\bgamma$ and $\bcp(x_t)$ given above, the row sums of $\bgamma$ will only approximately equal 1, and the components of the vector $\bfdelta$ will only approximately total 1. This can be remedied by scaling the vector $\bfdelta$ and each row of $\bgamma$ to total 1''.
The code in this paper doesn't remedy this because it provides no benefit here.

% This is likely due to machine approximations when numbers far apart from each other interact together.
% In R, a small number is not 0 but is treated as 0 when added to a much larger number.
% <<0-diff-0>>=
% 1e-100 == 0
% (1 + 1e-100) == 1
% @
This can result in incoherent findings when checking equality between 2 numbers.
Fortunately, no issue arose from this.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Confidence intervals}
\label{sec:confint}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hessian based confidence intervals}
\label{sec:hessian}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

First, let us consider evaluation of parameter uncertainty via the Hessian.
As the Hessian $\nabla^2\log L(\hat{\psi})$ refers to the transformed parameters $\psi$, the delta-method must be used to obtain an estimate of the covariance matrix of $\hat{\theta}$:
\begin{equation}
\Sigma_{\hat{\theta}} = - \nabla g(\hat{\psi})\left(\nabla^2\log L(\hat{\psi})\right)^{-1}\nabla g(\hat{\psi})^\prime
\label{eq:deltamethod}
\end{equation}
With minimal effort from the user's perspective, {\tt{TMB}} can be instructed to calculate $\Sigma_{\hat{\theta}}$ (by automatic differentiation).
Standard errors of derived parameters, such as the stationary distribution $\bfdelta$ which is a function of $\bgamma$, can be calculated by the delta-method similarly.

{\tt{TMB}} provides an easy way to retrieve these.
Following the example above, as we saw in \autoref{sec:tmb_r},
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Get all standard errors}
\hlstd{adrep} \hlkwb{<-} \hlkwd{summary}\hlstd{(}\hlkwd{sdreport}\hlstd{(obj_tmb),} \hlstr{"report"}\hlstd{)}
\hlstd{adrep}
\end{alltt}
\begin{verbatim}
##          Estimate Std. Error
## lambda 0.25636541 0.04016445
## lambda 3.11475432 1.02131181
## gamma  0.98872128 0.01063571
## gamma  0.31033853 0.18468648
## gamma  0.01127872 0.01063571
## gamma  0.68966147 0.18468648
## delta  0.96493123 0.03181445
## delta  0.03506877 0.03181445
\end{verbatim}
\begin{alltt}
\hlcom{# More help with ?summary.sdreport}
\end{alltt}
\end{kframe}
\end{knitrout}
It should be noted that although the estimates can be found in the optimization variable \texttt{opt} in their working form, they are also sent to the \texttt{MakeADFun} object \texttt{obj} and can be retrieved as shown above.
\texttt{sdreport} shows information about the working parameters' estimates, whereas the summary of that report also shows information about the variables reported by the function \texttt{ADREPORT}.

The next part shows how to get Wald confidence intervals as defined by \citet{wald}, using {\tt{TMB}}.
With the previous 2 state Poisson HMM, the $100(1-\alpha)\%$ confidence interval for $a$ is $a \pm z_{1-\alpha/2} * \sigma_{a}$ where $z_{x}$ is the $x$-percentile of the standard normal distribution, and $\sigma_a$ is the standard error of $a$.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{adrep} \hlkwb{<-} \hlkwd{summary}\hlstd{(}\hlkwd{sdreport}\hlstd{(obj_tmb),} \hlstr{"report"}\hlstd{)}

\hlcom{# Get the 97.5 percentile of the standard normal distribution}
\hlstd{q95_norm} \hlkwb{<-} \hlkwd{qnorm}\hlstd{(}\hlnum{1} \hlopt{-} \hlnum{0.05} \hlopt{/} \hlnum{2}\hlstd{)}

\hlcom{# Create the confidence interval}
\hlcom{# Extract the values}
\hlstd{estimates} \hlkwb{<-} \hlstd{adrep[,} \hlstr{"Estimate"}\hlstd{]}
\hlstd{std_errors} \hlkwb{<-} \hlstd{adrep[,} \hlstr{"Std. Error"}\hlstd{]}
\hlcom{# Create the bounds}
\hlstd{lower_bound} \hlkwb{<-} \hlstd{estimates} \hlopt{-} \hlstd{q95_norm} \hlopt{*} \hlstd{std_errors}
\hlstd{upper_bound} \hlkwb{<-} \hlstd{estimates} \hlopt{+} \hlstd{q95_norm} \hlopt{*} \hlstd{std_errors}
\hlcom{# Show the CI}
\hlkwd{cbind}\hlstd{(lower_bound, upper_bound)}
\end{alltt}
\begin{verbatim}
##         lower_bound upper_bound
## lambda  0.177644535  0.33508628
## lambda  1.113019955  5.11648869
## gamma   0.967875672  1.00956689
## gamma  -0.051640322  0.67231737
## gamma  -0.009566886  0.03212433
## gamma   0.327682625  1.05164032
## delta   0.902576056  1.02728641
## delta  -0.027286406  0.09742394
\end{verbatim}
\end{kframe}
\end{knitrout}

Estimates of $\btheta$ and $\bfdelta$, with accompanying confidence intervals are displayed in \autoref{table:lamb_estimates_std_errors} and \autoref{table:simul_estimates_std_errors}.
% For comparison, we have included the corresponding standard deviations resulting from replacing $\nabla^2\log L(\hat{\psi})$ in (\autoref{eq:deltamethod}) with traditional numerical approximations using the R functions nlm (nlmb) and fdHess (nlme).
% The difference when using these approximations are of order $1e-05$ or less, however one would still have to calculate $\nabla g(\hat{\psi})$ for these approximations to be useful.

As mentioned earlier, for a larger amount of hidden states, {\tt{TMB}} may be unable to give some or any standard standard errors because some variables are close to their boundaries.
In that situation, using a nested model might solve this issue.
We refer the reader to \autoref{sec:nested} for information on how to specify a nested model using {\tt{TMB}}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Likelihood profile based confidence intervals}
\label{sec:likelihood}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Next, we consider evaluating uncertainty using likelihood-profiles.
Assuming normality of the maximum likelihood (ML) estimators, confidence intervals for the parameters can be obtained using the above estimates of standard deviations.
For a fixed sample size, the validity of the normality assumption can often be violated and in these cases a likelihood-based CI is more robust (reference).
Let $\eta$ be single parameter in a model with parameters $(\eta, \theta)$ and likelihood $L(\eta, \theta)$, and let $L_p(\eta)$ be the profile likelihood defined by $L_p(\eta)=\max_{\theta} L(\eta, \theta)$.
Then a likelihood-based CI for $\eta$ is given by
\begin{equation}
\left\{\eta: 2\log(\frac{L(\hat{\eta},\hat{\theta})}{L_p(\eta)} < \chi^{2}_{1, (1-\alpha)}\right\}
\label{eq:profileCI}
\end{equation}
where $\chi^{2}_{1, (1-\alpha)}$ is the $1-\alpha$ quantile of a $\chi^2$ distribution with $1$ degree of freedom.
{\tt{TMB}} allows for very efficient computation of both the profile likelihood $L(\eta)$ and the CI given by \autoref{eq:profileCI}, and this has been used to produce \autoref{table:lamb_estimates_std_errors} and \autoref{table:simul_estimates_std_errors} which display the profile log-likelihood and the corresponding likelihood-based CI's in the model for the lamb and the simulated dataset.
Note that the problem of transformed parameters is easier to deal with when making likelihood-based CI's, since $\{g(\eta), L_p(g(\eta))\}=\{g(\eta), L_p(\eta)\}$ for any one-to-one function $g$ (invariance principle).


Again, {\tt{TMB}} provides an easy way to obtain those intervals.
The \texttt{name} argument allows to profile parameter.
The \texttt{trace} argument indicates how much information on the optimization you want the function to show.
Following the HMM example, if we wish to profile the $2^{nd}$ working parameter \texttt{log\_lambda2}, we have to feed its position into the \texttt{name} argument:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{profile} \hlkwb{<-} \hlkwd{tmbprofile}\hlstd{(}\hlkwc{obj} \hlstd{= obj_tmb,}
                      \hlkwc{name} \hlstd{=} \hlnum{2}\hlstd{,}
                      \hlkwc{trace} \hlstd{=} \hlnum{FALSE}\hlstd{)}

\hlcom{# Confidence interval of lambda}
\hlkwd{exp}\hlstd{(}\hlkwd{confint}\hlstd{(profile))}
\end{alltt}
\begin{verbatim}
##            lower    upper
## tlambda 1.265339 4.947733
\end{verbatim}
\end{kframe}
\end{knitrout}
It should be noted that the argument \texttt{lincomb} allows to profile any linear combination of parameters by using a vector of the linear coefficients in the same order as the parameters.\\
This method can sometimes give NA values, usually when a ML estimate is close to a boundary, but not only.
For example, the first working parameter \texttt{log\_lambda} is pretty low ($-4.47$).
It becomes too difficult to profile the likelihood so the function fails to provide a meaningful confidence interval.

Another important issue is about profiling with a univariate model.
With only 1 (hidden) state, a Poisson HMM becomes a univariate Poisson regression model.
Therefore, the profile becomes a plot of the likelihood when the Poisson mean varies.
However, \texttt{tmbprofile} fails to provide a confidence interval.
The reason is likely that it tries to optimize the likelihood despite the lack of parameters to change, and therefore fails.
\citet{visser}, \citet{meeker} and \citet{venzon} provide more details on profiling likelihoods.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bootstrap based confidence intervals}
\label{sec:bootstrapping}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Finally, we consider evaluating uncertainty using bootstrap.
There are many ways to bootstrap.
Non-parametric bootstraping of time series is possible, as \citet{hardle} has shown.
However our focus is not on bootstrapping techniques, so we choose a parametric approach.
See \citet{efron} for more details on bootstrapping.


From the parameters' ML estimates, we generate new data and re-estimate the parameters 500 times.
From that list of new estimates we can get the 2.5th and 97.5th percentiles and get 95\% confidence intervals for the parameters.\\

We show below how we get confidence intervals using bootstrap, based on the 3 state Poisson HMM estimates from above.

\begin{enumerate}

\item
First, we need a function to generate random data from a HMM.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Generate a random sample from a HMM}
\hlstd{pois.HMM.generate_sample}  \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{ns}\hlstd{,} \hlkwc{mod}\hlstd{) \{}
  \hlstd{mvect} \hlkwb{<-} \hlnum{1}\hlopt{:}\hlstd{mod}\hlopt{$}\hlstd{m}
  \hlstd{state} \hlkwb{<-} \hlkwd{numeric}\hlstd{(ns)}
  \hlstd{state[}\hlnum{1}\hlstd{]} \hlkwb{<-} \hlkwd{sample}\hlstd{(mvect,} \hlnum{1}\hlstd{,} \hlkwc{prob} \hlstd{= mod}\hlopt{$}\hlstd{delta)}
  \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{2}\hlopt{:}\hlstd{ns) \{}
    \hlstd{state[i]} \hlkwb{<-} \hlkwd{sample}\hlstd{(mvect,} \hlnum{1}\hlstd{,} \hlkwc{prob} \hlstd{= mod}\hlopt{$}\hlstd{gamma[state[i} \hlopt{-} \hlnum{1}\hlstd{], ])}
  \hlstd{\}}
  \hlstd{x} \hlkwb{<-} \hlkwd{rpois}\hlstd{(ns,} \hlkwc{lambda} \hlstd{= mod}\hlopt{$}\hlstd{lambda[state])}
  \hlkwd{return}\hlstd{(x)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

\item
Then, when the model is estimated each time, we don't impose an order for the states.
This can lead to the label switching problem, where states aren't ordered the same way in each model.
To address this, we re-ordered the states by ascending Poisson means.\\
Sorting the means is pretty straightforward.
Re-ordering the TPM is a little trickier.
To do so, we took the permutations of the states given by the sorted Poisson means, and permuted each row index and column index to its new value.\\
The function we used is
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Relabel states by increasing Poisson means}
\hlstd{pois.HMM.label.order} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{m}\hlstd{,} \hlkwc{lambda}\hlstd{,} \hlkwc{gamma}\hlstd{,} \hlkwc{delta} \hlstd{=} \hlkwa{NULL}\hlstd{) \{}
  \hlcom{# Get the indexes of the sorted states}
  \hlcom{# according to ascending lambda}
  \hlstd{sorted_lambda} \hlkwb{<-} \hlkwd{sort}\hlstd{(lambda,} \hlkwc{index.return} \hlstd{=} \hlnum{TRUE}\hlstd{)}\hlopt{$}\hlstd{ix}
  \hlcom{# Re-order the TPM according to the switched states}
  \hlcom{# in the sorted lambda}
  \hlstd{ordered_gamma} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{0}\hlstd{,} \hlkwc{nrow} \hlstd{= m,} \hlkwc{ncol} \hlstd{= m)}
  \hlkwa{for} \hlstd{(col} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{m) \{}
    \hlstd{new_col} \hlkwb{<-} \hlkwd{which}\hlstd{(sorted_lambda} \hlopt{==} \hlstd{col)}
    \hlkwa{for} \hlstd{(row} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{m) \{}
      \hlstd{new_row} \hlkwb{<-} \hlkwd{which}\hlstd{(sorted_lambda} \hlopt{==} \hlstd{row)}
      \hlstd{ordered_gamma[row, col]} \hlkwb{<-} \hlstd{gamma[new_row, new_col]}
    \hlstd{\}}
  \hlstd{\}}
  \hlcom{# Re-order the stationary distribution if it was provided}
  \hlcom{# Generate it otherwise}
  \hlkwa{if} \hlstd{(}\hlkwd{is.null}\hlstd{(delta)) \{}
    \hlstd{delta} \hlkwb{<-} \hlkwd{stat.dist}\hlstd{(ordered_gamma)}
  \hlstd{\}} \hlkwa{else} \hlstd{\{}
    \hlstd{delta} \hlkwb{<-} \hlstd{delta[sorted_lambda]}
  \hlstd{\}}
  \hlkwd{return}\hlstd{(}\hlkwd{list}\hlstd{(}\hlkwc{lambda} \hlstd{=} \hlkwd{sort}\hlstd{(lambda),}
              \hlkwc{gamma} \hlstd{= ordered_gamma,}
              \hlkwc{delta} \hlstd{= delta))}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

Let's show an example to understand the process.
For readability, the TPM is filled with row and column indexes instead of probabilities.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{lambda} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{30}\hlstd{,} \hlnum{10}\hlstd{,} \hlnum{20}\hlstd{)}
\hlstd{gamma} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{11}\hlstd{,} \hlnum{12}\hlstd{,} \hlnum{13}\hlstd{,}
                  \hlnum{21}\hlstd{,} \hlnum{22}\hlstd{,} \hlnum{23}\hlstd{,}
                  \hlnum{31}\hlstd{,} \hlnum{32}\hlstd{,} \hlnum{33}\hlstd{),} \hlkwc{byrow} \hlstd{=} \hlnum{TRUE}\hlstd{,} \hlkwc{ncol} \hlstd{=} \hlnum{3}\hlstd{)}
\hlkwd{pois.HMM.label.order}\hlstd{(}\hlkwc{m} \hlstd{=} \hlnum{3}\hlstd{, lambda, gamma)}
\end{alltt}
\begin{verbatim}
## $lambda
## [1] 10 20 30
## 
## $gamma
##      [,1] [,2] [,3]
## [1,]   33   31   32
## [2,]   13   11   12
## [3,]   23   21   22
## 
## $delta
## [1] -0.032786885  0.016393443 -0.008196721
\end{verbatim}
\end{kframe}
\end{knitrout}
State 1 has been relabeled state 3, state 2 became state 1, and state 3 became state 2.

\item
Bootstrap code
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{bootstrap_estimates} \hlkwb{<-} \hlkwd{data.frame}\hlstd{()}
\hlstd{DATA_SIZE} \hlkwb{<-} \hlkwd{length}\hlstd{(lamb_data)}
\hlcom{# Set how many parametric bootstrap samples we create}
\hlstd{BOOTSTRAP_SAMPLES} \hlkwb{<-} \hlnum{10}

\hlcom{# ML parameters}
\hlstd{ML_working_estimates} \hlkwb{<-} \hlstd{obj_tmb}\hlopt{$}\hlstd{env}\hlopt{$}\hlstd{last.par.best}
\hlstd{ML_natural_estimates} \hlkwb{<-} \hlstd{obj_tmb}\hlopt{$}\hlkwd{report}\hlstd{(ML_working_estimates)}
\hlstd{gamma} \hlkwb{<-} \hlstd{ML_natural_estimates}\hlopt{$}\hlstd{gamma}
\hlstd{lambda} \hlkwb{<-} \hlstd{ML_natural_estimates}\hlopt{$}\hlstd{lambda}
\hlstd{delta} \hlkwb{<-} \hlstd{ML_natural_estimates}\hlopt{$}\hlstd{delta}

\hlcom{# Parameters for TMB}
\hlstd{cols} \hlkwb{<-} \hlkwd{names}\hlstd{(ML_working_estimates)}
\hlstd{tgamma} \hlkwb{<-} \hlstd{ML_working_estimates[cols} \hlopt{==} \hlstr{"tgamma"}\hlstd{]}
\hlstd{tlambda} \hlkwb{<-} \hlstd{ML_working_estimates[cols} \hlopt{==} \hlstr{"tlambda"}\hlstd{]}
\hlstd{ML_TMB_parameters} \hlkwb{<-} \hlkwd{list}\hlstd{(}\hlkwc{tlambda} \hlstd{= tlambda,}
                          \hlkwc{tgamma} \hlstd{= tgamma)}

\hlstd{params_names} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"lambda"}\hlstd{,} \hlstr{"gamma"}\hlstd{,} \hlstr{"delta"}\hlstd{)}

\hlkwa{for} \hlstd{(idx_sample} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{BOOTSTRAP_SAMPLES) \{}
  \hlcom{# Loop as long as there is an issue with nlminb}
  \hlkwa{repeat} \hlstd{\{}
    \hlcom{#simulate the data}
    \hlstd{bootstrap_data} \hlkwb{<-} \hlkwd{pois.HMM.generate_sample}\hlstd{(DATA_SIZE,}
                                               \hlkwd{list}\hlstd{(}\hlkwc{m} \hlstd{= m,}
                                                    \hlkwc{lambda} \hlstd{= lambda,}
                                                    \hlkwc{gamma} \hlstd{= gamma,}
                                                    \hlkwc{delta} \hlstd{= delta))}

    \hlcom{# Parameters for TMB}
    \hlstd{TMB_data_bootstrap} \hlkwb{<-} \hlkwd{list}\hlstd{(}\hlkwc{x} \hlstd{= bootstrap_data,} \hlkwc{m} \hlstd{= m)}

    \hlcom{# Estimate the parameters}
    \hlstd{obj} \hlkwb{<-} \hlkwd{MakeADFun}\hlstd{(TMB_data_bootstrap,}
                     \hlstd{ML_TMB_parameters,}
                     \hlkwc{DLL} \hlstd{=} \hlstr{"poi_hmm"}\hlstd{,}
                     \hlkwc{silent} \hlstd{=} \hlnum{TRUE}\hlstd{)}

    \hlcom{# Using tryCatch to break the loop if an error happens}
    \hlcom{# interrupts the outer (for) loop, not the inner one (repeat)}
    \hlstd{mod_temp} \hlkwb{<-} \hlkwa{NULL}
    \hlkwd{try}\hlstd{(mod_temp} \hlkwb{<-} \hlkwd{nlminb}\hlstd{(}\hlkwc{start} \hlstd{= obj}\hlopt{$}\hlstd{par,} \hlkwc{objective} \hlstd{= obj}\hlopt{$}\hlstd{fn,}
                           \hlkwc{gradient} \hlstd{= obj}\hlopt{$}\hlstd{gr,} \hlkwc{hessian} \hlstd{= obj}\hlopt{$}\hlstd{he),}
        \hlkwc{silent} \hlstd{=} \hlnum{TRUE}\hlstd{)}

    \hlcom{# If nlminb doesn't reach any result, retry}
    \hlkwa{if} \hlstd{(}\hlkwd{is.null}\hlstd{(mod_temp)) \{}
      \hlkwa{next}
    \hlstd{\}}
    \hlcom{# If nlminb doesn't converge successfully, retry}
    \hlkwa{if} \hlstd{(mod_temp}\hlopt{$}\hlstd{convergence} \hlopt{!=} \hlnum{0}\hlstd{) \{}
      \hlkwa{next}
    \hlstd{\}}
    \hlstd{working_parameters} \hlkwb{<-} \hlstd{obj}\hlopt{$}\hlstd{env}\hlopt{$}\hlstd{last.par.best}
    \hlstd{natural_parameters} \hlkwb{<-} \hlstd{obj}\hlopt{$}\hlkwd{report}\hlstd{(working_parameters)}

    \hlstd{l} \hlkwb{<-} \hlstd{natural_parameters}\hlopt{$}\hlstd{lambda}
    \hlstd{g} \hlkwb{<-} \hlstd{natural_parameters}\hlopt{$}\hlstd{gamma}
    \hlstd{d} \hlkwb{<-} \hlstd{natural_parameters}\hlopt{$}\hlstd{delta}

    \hlcom{# Label switching}
    \hlstd{natural_parameters} \hlkwb{<-} \hlkwd{pois.HMM.label.order}\hlstd{(m, l, g, d)}

    \hlcom{# If some parameters are NA for some reason, retry}
    \hlkwa{if} \hlstd{(}\hlkwd{anyNA}\hlstd{(natural_parameters[params_names],} \hlkwc{recursive} \hlstd{=} \hlnum{TRUE}\hlstd{)) \{}
      \hlkwa{next}
    \hlstd{\}}

    \hlcom{# If everything went well, end the "repeat" loop}
    \hlkwa{break}
  \hlstd{\}}
  \hlcom{# The values from gamma are taken columnwise}
  \hlstd{natural_parameters} \hlkwb{<-} \hlkwd{unlist}\hlstd{(natural_parameters[params_names])}
  \hlstd{len_par} \hlkwb{<-} \hlkwd{length}\hlstd{(natural_parameters)}
  \hlstd{bootstrap_estimates[idx_sample,} \hlnum{1}\hlopt{:}\hlstd{len_par]} \hlkwb{<-} \hlstd{natural_parameters}
\hlstd{\}}

\hlcom{# Lower and upper 95% bounds}
\hlstd{q} \hlkwb{<-} \hlkwd{apply}\hlstd{(bootstrap_estimates,} \hlnum{2}\hlstd{,} \hlkwa{function}\hlstd{(}\hlkwc{par_estimate}\hlstd{) \{}
  \hlkwd{quantile}\hlstd{(par_estimate,} \hlkwc{probs} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{0.025}\hlstd{,} \hlnum{0.975}\hlstd{))}
\hlstd{\})}

\hlstd{params_names} \hlkwb{<-} \hlkwd{paste0}\hlstd{(}\hlkwd{rep}\hlstd{(}\hlstr{"lambda"}\hlstd{, m),} \hlnum{1}\hlopt{:}\hlstd{m)}
\hlcom{# Get row and column indexes for gamma instead of the default}
\hlcom{# columnwise index: the default indexes are 1:m for the 1st column,}
\hlcom{# then (m + 1):(2 * m) for the 2nd, etc...}
\hlkwa{for} \hlstd{(gamma_idx} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{m} \hlopt{^} \hlnum{2}\hlstd{) \{}
  \hlstd{row} \hlkwb{<-} \hlstd{(gamma_idx} \hlopt{-} \hlnum{1}\hlstd{)} \hlopt{%%} \hlstd{m} \hlopt{+} \hlnum{1}
  \hlstd{col} \hlkwb{<-} \hlstd{(gamma_idx} \hlopt{-} \hlnum{1}\hlstd{)} \hlopt{%/%} \hlstd{m} \hlopt{+} \hlnum{1}
  \hlstd{row_col_idx} \hlkwb{<-} \hlkwd{c}\hlstd{(row, col)}
  \hlstd{params_names} \hlkwb{<-} \hlkwd{c}\hlstd{(params_names,}
                    \hlkwd{paste0}\hlstd{(}\hlstr{"gamma"}\hlstd{,}
                           \hlkwd{paste0}\hlstd{(row_col_idx,} \hlkwc{collapse} \hlstd{=} \hlstr{""}\hlstd{)))}
\hlstd{\}}
\hlstd{params_names} \hlkwb{<-} \hlkwd{c}\hlstd{(params_names,}
                  \hlkwd{paste0}\hlstd{(}\hlkwd{rep}\hlstd{(}\hlstr{"delta"}\hlstd{, m),} \hlnum{1}\hlopt{:}\hlstd{m))}
\hlstd{bootstrap_CI} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlstr{"Parameter"} \hlstd{= params_names,}
                           \hlstr{"Estimate"} \hlstd{=} \hlkwd{c}\hlstd{(lambda, gamma, delta),}
                           \hlstr{"Lower bound"} \hlstd{= q[}\hlnum{1}\hlstd{, ],}
                           \hlstr{"Upper bound"} \hlstd{= q[}\hlnum{2}\hlstd{, ])}
\hlkwd{print}\hlstd{(bootstrap_CI,} \hlkwc{row.names} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\end{alltt}
\begin{verbatim}
##  Parameter   Estimate  Lower.bound Upper.bound
##    lambda1 0.25636541 7.117339e-09   0.2954041
##    lambda2 3.11475432 3.565353e-01   3.4379317
##    gamma11 0.98872128 4.444232e-01   0.9947810
##    gamma21 0.31033853 9.937752e-02   0.5693026
##    gamma12 0.01127872 5.219006e-03   0.5555768
##    gamma22 0.68966147 4.306974e-01   0.9006225
##     delta1 0.96493123 2.635814e-01   0.9830019
##     delta2 0.03506877 1.699805e-02   0.7364186
\end{verbatim}
\end{kframe}
\end{knitrout}

It should be noted that some estimates can be very large or small.
This can happen when the randomly generated bootstrap sample contains long chains of the same values.
However, a large number of bootstrap samples lowers that risk since we "leave out" 5\% of the extreme values in the CI.
It could be useful to skip the bootstrap samples where the estimates are too far out of expectations.

\end{enumerate}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{State inference}
% \label{sec:state_inference}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% \subsection{Setup}
% Given an optimized \texttt{MakeADFun} object \texttt{obj}, we need to setup some variables to compute the probabilities detailed below.
% <<init-decoding>>=
% # Retrieve the objects at ML value
% adrep <- obj_tmb$report(obj_tmb$env$last.par.best)
% delta <- adrep$delta
% gamma <- adrep$gamma
% emission_probs <- adrep$emission_probs
% n <- adrep$n
% m <- length(delta)
% mllk <- adrep$mllk
% @
% 
% \subsection{Log-forward probabilities}
% The forward probabilities have been detailed in \autoref{sec:hmm_likelihood}.
% We show here a way to compute the log of the forward probabilities, using a scaling scheme defined by \citet{zucchini}.
% <<log-forward>>=
% # Compute log-forward probabilities (scaling used)
% lalpha <- matrix(NA, m, n)
% foo <- delta * emission_probs[1, ]
% sumfoo <- sum(foo)
% lscale <- log(sumfoo)
% foo <- foo / sumfoo
% lalpha[, 1] <- log(foo) + lscale
% for (i in 2:n) {
%   foo <- foo %*% gamma * emission_probs[i, ]
%   sumfoo <- sum(foo)
%   lscale <- lscale + log(sumfoo)
%   foo <- foo / sumfoo
%   lalpha[, i] <- log(foo) + lscale
% }
% # lalpha contains n=240 columns, so we only display 5 for readability
% lalpha[, 1:5]
% @
% 
% \subsection{Log-backward probabilities}
% The backward probabilities have been defined in the same section.
% <<log-backward>>=
% # Compute log-backwards probabilities (scaling used)
% lbeta <- matrix(NA, m, n)
% lbeta[, n] <- rep(0, m)
% foo <- rep (1 / m, m)
% lscale <- log(m)
% for (i in (n - 1):1) {
%   foo <- gamma %*% (emission_probs[i + 1, ] * foo)
%   lbeta[, i] <- log(foo) + lscale
%   sumfoo <- sum(foo)
%   foo <- foo / sumfoo
%   lscale <- lscale + log(sumfoo)
% }
% # lbeta contains n=240 columns, so we only display 4 for readability
% lbeta[, 1:4]
% @
% 
% \subsection{Smoothing probabilities}
% The smoothing probabilities are defined in \citet{zucchini} as
% $P(C_t = i \vert X^{(n)} = x^{(n)}) = \frac{\alpha_t(i) \beta_t(i)}{L_n}$.
% <<smoothing>>=
% # Compute conditional state probabilities, smoothing probabilities
% stateprobs <- matrix(NA, ncol = n, nrow = m)
% llk <- - mllk
% for(i in 1:n) {
%   stateprobs[, i] <- exp(lalpha[, i] + lbeta[, i] - llk)
% }
% 
% # Most probable states
% ldecode <- rep(NA, n)
% for (i in 1:n) {
%   ldecode[i] <- which.max(stateprobs[, i])
% }
% ldecode
% @
% 
% \subsection{Forecast, h-step-ahead-probabilities}
% The forecast distribution or h-step-ahead-probabilities as well as its implementation in R is detailed in \citet{zucchini}.\\
% Let $\bfphi_T = \frac{\balpha_T}{\balpha_T \bone'}$.\\
% Then,
% \begin{equation*}
% P(X_{T+h} = x \vert X^{(T)} = x^{(T)}) = \frac{\balpha_T \bgamma^h \bcp(x) \bone'}{\balpha_T \bone'} = \bfphi_T \bgamma^h \bcp(x) \bone'.
% \end{equation*}
% An implementation of this, using a scaling scheme is
% <<forecast>>=
% # Number of steps
% h <- 1
% # Values for which we want the forecast probabilities
% xf <- 0:50
% 
% nxf <- length(xf)
% dxf <- matrix(0, nrow = h, ncol = nxf)
% foo <- delta * emission_probs[1, ]
% sumfoo <- sum(foo)
% lscale <- log(sumfoo)
% foo <- foo / sumfoo
% for (i in 2:n) {
%   foo <- foo %*% gamma * emission_probs[i, ]
%   sumfoo <- sum( foo)
%   lscale <- lscale + log(sumfoo)
%   foo <- foo / sumfoo
% }
% emission_probs_xf <- get.emission.probs(xf, lambda)
% for (i in 1:h) {
%   foo <- foo %*% gamma
%   for (j in 1:m) {
%     dxf[i, ] <- dxf[i, ] + foo[j] * emission_probs_xf[, j]
%   }
% }
% # dxf contains n=240 columns, so we only display 4 for readability
% dxf[, 1:4]
% @
% 
% \subsection{Global decoding using the Viterbi algorithm}
% The Viterbi algorithm is detailed in \citet{zucchini}.
% It calculates the sequence of states $(i_1^*, \ldots, i_T^*)$ which maximizes the conditional probability of all states simultaneously, i.e.
% \begin{equation*}
% (i_1^*, \ldots, i_T^*) = \argmax_{i_1, \ldots, i_T \in \{1, \ldots, m \}} P(C_1 = i_1, \ldots, C_T = i_T \vert X^{(T)} = x^{(T)}).
% \end{equation*}
% An implementation of it is
% <<global>>=
% xi <- matrix(0, n, m)
% foo <- delta * emission_probs[1, ]
% xi[1, ] <- foo / sum(foo)
% for (i in 2:n) {
%   foo <- apply(xi[i - 1, ] * gamma, 2, max) * emission_probs[i, ]
%   xi[i, ] <- foo / sum(foo)
% }
% iv <- numeric(n)
% iv[n] <- which.max(xi[n, ])
% for (i in (n - 1):1){
%   iv[i] <- which.max(gamma[, iv[i + 1]] * xi[i, ])
% }
% iv
% @
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Application to hospital data}
% \label{sec:application_hospital}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Hospital data description}
% \label{sec:hosp_data_description}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% We timed $m$ states HMM parameter estimations using different parameters on a dataset provided by a hospital from Assistance Publique – Hôpitaux de Paris, a french hospital trust, with $m = M_LIST_HOSP$.
% Other datasets are considered in \autoref{sec:speed_evaluation}.
% 
% The dataset contains the number of patients who entered a hospital each hour between the first hour of 2010 and the last hour of 2019.
% The data was anonymized and grouped using the R package {\tt{dplyr}}.
% That data contains DATA_SIZE_HOSP data points.\\
% The column PATIENTS represents the number of people arriving at the hospital between the hour mentioned and the next.
% The zeroes there indicate that no patient arrived at the hospital during the hour.\\
% The column DATE is a datetime item useful for sorting and plotting the data.\\
% The column WDAY represents the number of the day in the week: 1 is Monday and 7 is Sunday.\\
% The columns YEAR is an integer taking values 2010, 2011, \ldots, 2019.\\
% For example, the dataset starts on Januray $1^{st}$ 2010 on the hour 0 with an amount of patients of 6, indicating that 6 people arrived between 00:00 and 00:59.
% 
% See the supporting information for details on importing the data.
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Speed comparison}
% \label{sec:speed_hosp}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% Different numbers of hidden states $m$ were used: (($M_LIST_LAMB$) for the lamb dataset, ($M_LIST_SIMUL$) for the simulated dataset, and ($M_LIST_HOSP$) for the hospital dataset) to compare speeds.
% Each estimation was timed $BENCHMARK_TRIALS$ times, allowing for graphical comparisons through boxplots.
% The approaches with {\tt{TMB}} were much faster than without, thus making it necessary to display the figures on a log scale for time.
% The lamb dataset and the simulated one are used in \autoref{sec:speed_evaluation}, whereas the current section focuses on the dataset about hospital patient arrivals.
% 
% For readability, DM denotes direct maximization without using {\tt{TMB}}, whereas TMB1, TMB2, TMB3, and TMB4 denote direct maximization using {\tt{TMB}} with and without making use of the exact gradient and hessian that {\tt{TMB}} can provide.
% \autoref{table:notation} summarizes the notation.
% 
% \begin{table}[h!]
% \begin{center}
% \caption{Naming of TMB parameters}
% \label{table:notation}
% \begin{tabular}{l|cccc}
% \hline
%                         & TMB1  & TMB2  & TMB3  & TMB4\\
% \hline
% Exact gradient is used  & No    & No    & Yes   & Yes\\
% Exact hessian is used   & No    & Yes   & No    & Yes\\
% \hline
% \end{tabular}
% \end{center}
% \end{table}
% 
% We timed the parameter estimation of $m$ states Poisson HMMs using the approaches DM, TMB1, TMB2, TMB3, and TMB4, with $m = M_LIST_HOSP$ and found that using {\tt{TMB}} accelerates the estimation in all cases as can be seen in \autoref{fig:tmb-acceleration-hosp} by the drop in time from DM to any estimation using {\tt{TMB}}.
% The times can be found in the supporting information.
% As could have been expected, estimating a model's parameters takes a longer time as the complexity of the model ($m$) increases.
% <<tmb-acceleration-hosp, fig.cap = 'm state Poisson HMM estimation time with and without using TMB, estimated on the hospital dataset. Each procedure was repeated and timed. Boxplots are shown for visual comparison. The naming convention is found in \\autoref{table:notation}.', echo = FALSE>>=
% # include_graphics(paste0("Plots/hosp_m=", max(M_LIST_HOSP), ".png"))
% # title <- paste0("Hospital data, size = ", DATA_SIZE_HOSP)
% # ggplot(benchmarks_df_hosp, aes(x = procedure, y = time)) +
% #   geom_boxplot() +
% #   facet_grid(. ~ m, space ="free_x", scales="free_x", switch="x") +
% #   theme_Publication() +
% #   xlab("Exact/inexact gradient and hessian") +
% #   ylab("Time (seconds)") +
% #   ggtitle(title, "Parameter estimation time") +
% #   scale_y_log10()
% plots <- list()
% for (i in M_LIST_HOSP) {
%   subtitle <- paste0("m = ", i)
%   plots[[i]] <- benchmarks_df_hosp %>%
%     filter(m == i) %>%
%     ggplot(aes(x = procedure, y = time)) +
%     geom_boxplot() +
%     theme_Publication() +
%     xlab("Exact/inexact gradient and hessian") +
%     ylab("Time (seconds)") +
%     ggtitle("Parameter estimation time", subtitle) +
%     scale_y_log10()
% }
% ggarrange(plotlist = plots, ncol = 2, nrow = 2)
% @
% Interestingly, for $m > 1$, TMB3 shows a clear acceleration when compared to other estimations using {\tt{TMB}}.
% The reason is unclear.
% <<tmb-acceleration-hosp-table, echo = FALSE>>=
% incr_hosp <- data.frame(m = integer(),
%                         "TMB1" = numeric(),
%                         "TMB2" = numeric(),
%                         "TMB3" = numeric(),
%                         "TMB4" = numeric())
% for (idx in 1:length(M_LIST_HOSP)) {
%   m <- M_LIST_HOSP[idx]
%   incr_hosp[idx, "m"] <- m
%   boxplot_data <- benchmarks_hosp[[idx]]
% 
%   incr_hosp[idx, "TMB1"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB1"])) - 1
%   incr_hosp[idx, "TMB2"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB2"])) - 1
%   incr_hosp[idx, "TMB3"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB3"])) - 1
%   incr_hosp[idx, "TMB4"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB4"])) - 1
% }
% incr_hosp[, - 1] <- incr_hosp[, - 1] * 100
% table <- xtable(incr_hosp,
%                 display = c("d", "d", rep("f", 4)),
%                 caption = paste0("Speed percentage increase of using TMB for m state Poisson HMM parameter estimation, estimated on the hospital dataset. Each procedure was repeated and timed. Their mean times are compared in this table. When m=1, the mean estimation time with TMB by providing the exact gradient but not the exact hessian (TMB3) was ", round(incr_hosp[1, 4]), "\\% lower than the mean estimation time with direct maximization without TMB (DM). The naming convention is found in \\autoref{table:notation}."),
%                 label = "table:speed-hospital",
%                 digits = 0)
% @
% 
% Each box of the boxplots summarizes $BENCHMARK_TRIALS$ data points but shows little variation.
% A percentage mean speed increase of TMB1, ..., TMB4 when compared to the mean times of DM summarizes these graphs nicely (see \autoref{table:speed-hospital}).
% For example, the average 4 state Poisson HMM estimation speed when providing {\tt{TMB}}'s exact gradient to {\tt{nlminb}} and not the hessian (TMB3) is $round(incr_hosp[incr_hosp$m == 4, "TMB3"])$\% faster than without making use of {\tt{TMB}} (DM).
% 
% <<tmb-acceleration-hosp-print-table, results='asis', echo = FALSE>>=
% print(table, include.rownames = FALSE)
% @
% 
% TMB also accelerates the likelihood computation time.
% The percentage speed gains from using {\tt{TMB}} (\autoref{table:increase-likelihood-hosp}) are all positive, thus showing that on large datasets, the objective function computation time can be accelerated.
% <<tmb-acceleration-hosp-log, results='asis', echo = FALSE>>=
% incr_mllk_time_hosp <- data.frame(m = integer(),
%                                   "TMB1" = numeric(),
%                                   "TMB2" = numeric(),
%                                   "TMB3" = numeric(),
%                                   "TMB4" = numeric())
% for (idx in 1:length(M_LIST_HOSP)) {
%   m <- M_LIST_HOSP[idx]
%   incr_mllk_time_hosp[idx, "m"] <- m
%   boxplot_data <- mllk_times_hosp[[idx]]
% 
%   incr_mllk_time_hosp[idx, "TMB1"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB1"])) - 1
%   incr_mllk_time_hosp[idx, "TMB2"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB2"])) - 1
%   incr_mllk_time_hosp[idx, "TMB3"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB3"])) - 1
%   incr_mllk_time_hosp[idx, "TMB4"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB4"])) - 1
% }
% incr_mllk_time_hosp[, - 1] <- incr_mllk_time_hosp[, - 1] * 100
% table <- xtable(incr_mllk_time_hosp,
%                 display = c("d", "d", rep("f", 4)),
%                 caption = paste0("Speed percentage increase of using TMB for m state Poisson HMM negative log-likelihood calculation, estimated on the hospital dataset. Each procedure was repeated and timed. Their mean times are compared in this table. When m=1, the mean calculation time with TMB by providing the exact gradient but not the exact hessian (TMB3) was ", round(incr_mllk_time_hosp[1, 4]), "\\% lower than the mean estimation time with direct maximization without TMB (DM). Naming convention is found in \\autoref{table:notation}"),
%                 digits = 0,
%                 label = "table:increase-likelihood-hosp")
% print(table, include.rownames = FALSE)
% @
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Interpretation}
% \label{sec:interpretation}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Looking at a boxplot of the data (\autoref{fig:hourly-hospital}) lets us discover the hourly distribution of the data.
% We choose to use hours instead of days or other time durations because the difference in arrivals between night and day was already obvious to the doctors themselves, and we would likely find at least two distinct regimes there.
% <<hourly-hospital, echo = FALSE, fig.cap = "Hourly amount of patient arrival in the hospital">>=
% full_hosp_data %>%
%   select(HOUR, PATIENTS) %>%
%   mutate(type = ifelse(HOUR >= 9 & HOUR <= 22, "day", "night"),
%          HOUR = as.factor(HOUR)) %>%
%   ggplot(aes(x = HOUR, y = PATIENTS, fill = type)) +
%   geom_boxplot() +
%   theme_Publication() +
%   theme(legend.title = element_blank()) +
%   xlab("hour") +
%   ylab("patients")
% @
% More patients arrive in the day rather than during the night as was guessed, but more patients arrive in the afternoon rather than in the evening.
% Also, most patients show up in the early part of the day and around noon.
% Given the amount of data (87648/24 = 3652 data points for each hour), we can apply a z-test to see if the means are different.
% <<p-value, echo = FALSE>>=
% library(tidyverse)
% hour_0 <- full_hosp_data %>%
%   filter(HOUR == 0) %>%
%   select(PATIENTS)
% hour_0 <- hour_0$PATIENTS
% hour_12 <- full_hosp_data %>%
%   filter(HOUR == 12) %>%
%   select(PATIENTS)
% hour_12 <- hour_12$PATIENTS
% 
% mean_12 <- mean(hour_12)
% mean_0 <- mean(hour_0)
% var_12 <- var(hour_12)
% var_0 <- var(hour_0)
% len <- length(hour_12)
% z <- (mean_12 - mean_0) / sqrt(var_12 / len + var_0 / len)
% pvalue <- pnorm(z, lower.tail = FALSE)
% @
% We find a p-value approximated to $pvalue$, so the test shows that the difference between the average amount of patient arrival between 6 and 6:59am and the one between 10 and 10:59am is statistically significant.
% This agrees with the conclusion of the doctors and shows that there are likely at least 2 different distributions in the hourly arrivals of patients, justifying an educated guess of 2 or more hidden states.
% 
% To choose more accurately the number of hidden states, we tried a few, and looked at the quality of each fit (\autoref{table:model-selection-hosp}).
% According to the AIC, the BIC, and the negative log-likelihood, the model with $m = 4$ is the most appropriate and we therefore pick it.
% <<model-selection, results = 'asis', echo = FALSE>>=
% temp <- mllk_values_hosp[mllk_values_hosp$procedure == "TMB4", - 2]
% temp$m <- as.numeric(temp$m)
% colnames(temp)[2] <- "nll"
% table <- xtable(temp,
%                 caption = "Model selection measures of m state Poisson HMMs estimated on the hospital dataset",
%                 label = "table:model-selection-hosp",
%                 digits = 0)
% print(table, include.rownames = FALSE)
% @
% Once the 4 state HMM is estimated, we can look at the state distribution of a 4 state Poisson HMM fitted on the hourly arrivals (see \autoref{fig:states-hospital}) across the entire hospital dataset.
% This HMM seems to agree with our previous guess.
% It should be noted that for the hours 6 and 23, state 4 appeared 0 times and explains why the number 4 is missing in those columns.
% <<states-hospital, fig.cap = "Hourly state distribution of a 4 state Poisson HMM estimated and fitted on the hospital dataset. The $1^{st}$ row in the horizontal axis denotes the hour, the $2^{nd}$ row denotes the state. The state is also color coded for clarity.", echo = FALSE>>=
% m <- 4
% gamma <- matrix(0.2 / (m - 1), nrow = m, ncol = m)
% diag(gamma) <- 0.8
% lambda <- seq(5, 20, length.out = m)
% 
% working_params <- pois.HMM.pn2pw(m, lambda, gamma)
% TMB_data <- list(x = hosp_data, m = m)
% 
% tmb4 <- TMB.estimate(TMB_data = TMB_data,
%                      parameters = working_params,
%                      gradient = TRUE,
%                      hessian = TRUE)
% 
% dec <- HMM.decode(tmb4$obj)$ldecode
% dec <- enframe(dec) %>%
%   rename(state = value) %>%
%   mutate(state = as.factor(state)) %>%
%   bind_cols(full_hosp_data)
% 
% hour <- dec %>%
%   group_by(HOUR, state) %>%
%   summarize(patients = n(), .groups = "drop")
% ggplot(hour, aes(x = state, y = patients, fill = state)) +
%   geom_bar(stat = "identity") +
%   facet_grid(. ~ HOUR, space ="free_x", scales="free_x", switch="x") +
%   xlab("hour and state") +
%   theme_Publication()
% @
% States (1, 2, 3, 4) have Poisson means (round(tmb4$lambda, 3)) respectively.
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Speed evaluation}
% \label{sec:speed_evaluation}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Lamb and simulation dataset description}
% \label{sec:lamb_simul_dataset_description}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% We timed estimations of HMMs using different parameters on a dataset provided by \citet{leroux} and on a simulated dataset.
% 
% The first dataset is the number of movements by a fetal lamb in 240 consecutive 5-second intervals.
% The counts are listed here in \autoref{table:lamb_data} (read from left to right and top to bottom) and are available in the supporting information.
% <<lamb-table, results = 'asis', echo = FALSE>>=
% temp <- paste(lamb_data, collapse = " ")
% temp <- as.data.frame(temp)
% temp <- xtable(temp,
%                align = "lp{15cm}",
%                caption = "Numbers of movements by a fetal lamb in 240 consecutive 5-second intervals. Read from left to right, and from top to bottom.",
%                label = "table:lamb_data")
% print(temp, include.rownames = FALSE, include.colnames = FALSE,
%       hline.after = c(0, nrow(temp)))
% @
% 
% The second dataset is a sequence of random numbers generated by a $m$-state Poisson HMM.
% We used the code below to generate that simulated dataset, with $m=M_LIST_SIMUL$.
% <<generate-data-code, eval = FALSE>>=
% DATA_SIZE_SIMUL <- 2000
% m <- 2
% # Generate parameters
% lambda <- seq(1, 7, length.out = m)
% # Create the transition probability matrix with 0.8 on its diagonal
% gamma <- matrix(0.2 / (m - 1), nrow = m, ncol = m)
% diag(gamma) <- 0.8
% delta <- stat.dist(gamma)
% 
% #simulate the data
% simul_data <- pois.HMM.generate_sample(ns = DATA_SIZE_SIMUL,
%                                        mod = list(m = m,
%                                                   lambda = lambda,
%                                                   gamma = gamma,
%                                                   delta = delta))
% @
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Speed comparison}
% \label{sec:speed}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{enumerate}
% \item Speed of {\tt{TMB}}\\
% To compare speeds with and without using {\tt{TMB}}, it is important to test multiple sets of parameters.
% Hence, we use the R package {\tt{microbenchmark}}.
% 
% \autoref{fig:tmb-acceleration-lamb} shows the time acceleration when using {\tt{TMB}} on the lamb dataset from \citet{leroux} through boxplots.
% {\tt{TMB}} accelerates the estimation time for the HMMs, as with the hospital dataset.
% <<tmb-acceleration-lamb, fig.height = 3, fig.cap = 'm state Poisson HMMs estimation time with and without using TMB, estimated on the lamb dataset. Each procedure was repeated and timed. Boxplots are shown for visual comparison. The naming convention is found in \\autoref{table:notation}.', echo = FALSE>>=
% # include_graphics(paste0("Plots/lamb_m=", max(M_LIST_LAMB), ".png"))
% # title <- paste0("Lamb data, size = ", DATA_SIZE_LAMB)
% # ggplot(benchmarks_df_lamb, aes(x = procedure, y = time)) +
% #   geom_boxplot() +
% #   facet_grid(. ~ m, space ="free_x", scales="free_x", switch="x") +
% #   theme_Publication() +
% #   xlab("Exact/inexact gradient and hessian") +
% #   ylab("Time (seconds)") +
% #   ggtitle(title, "Parameter estimation time") +
% #   scale_y_log10()
% plots <- list()
% for (i in M_LIST_LAMB) {
%   subtitle <- paste0("m = ", i)
%   plots[[i]] <- benchmarks_df_lamb %>%
%     filter(m == i) %>%
%     ggplot(aes(x = procedure, y = time)) +
%     geom_boxplot() +
%     theme_Publication() +
%     xlab("Exact/inexact gradient and hessian") +
%     ylab("Time (seconds)") +
%     ggtitle("Parameter estimation time", subtitle) +
%     scale_y_log10()
% }
% ggarrange(plotlist = plots, ncol = 2, nrow = 1)
% @
% 
% The percentage average speed increases from using {\tt{TMB}} on HMMs estimated on the lamb dataset are all positive (\autoref{table:speed-lamb}), thus showing that estimation using {\tt{TMB}} is faster than a regular estimation procedure without using {\tt{TMB}} even on a small dataset.
% <<tmb-acceleration-lamb-table, results='asis', echo = FALSE>>=
% library(xtable)
% incr_lamb <- data.frame(m = integer(),
%                         "TMB1" = numeric(),
%                         "TMB2" = numeric(),
%                         "TMB3" = numeric(),
%                         "TMB4" = numeric())
% for (idx in 1:length(M_LIST_LAMB)) {
%   m <- M_LIST_LAMB[idx]
%   incr_lamb[idx, "m"] <- m
%   boxplot_data <- benchmarks_lamb[[idx]]
% 
%   incr_lamb[idx, "TMB1"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB1"])) - 1
%   incr_lamb[idx, "TMB2"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB2"])) - 1
%   incr_lamb[idx, "TMB3"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB3"])) - 1
%   incr_lamb[idx, "TMB4"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB4"])) - 1
% }
% incr_lamb[, - 1] <- incr_lamb[, - 1] * 100
% table <- xtable(incr_lamb,
%                 display = c("d", "d", rep("f", 4)),
%                 caption = paste0("Speed percentage increase of using TMB for m state Poisson HMM parameter estimation, estimated on the lamb dataset. Each procedure was repeated and timed. Their mean times are compared in this table. When m=1, the mean estimation time with TMB by providing the exact gradient but not the exact hessian (TMB3) was ", round(incr_lamb[1, 4]), "\\% lower than the mean estimation time with direct maximization without TMB (DM). The naming convention is found in \\autoref{table:notation}."),
%                 label = "table:speed-lamb",
%                 digits = 0)
% print(table, include.rownames = FALSE)
% @
% \autoref{fig:tmb-acceleration-simul} shows the time acceleration when using {\tt{TMB}} on a simulated dataset through boxplots.
% Similarly to the other datasets, it shows that using {\tt{TMB}} results in faster estimations of Poisson HMMs' parameters.
% <<tmb-acceleration-simul, fig.cap = 'm state Poisson HMM estimation time with and without using TMB, simulated data. Naming convention is found in \\autoref{table:notation}', echo = FALSE>>=
% # include_graphics(paste0("Plots/simul_m=", max(M_LIST_SIMUL), ".png"))
% # title <- paste0("Simulated data, size = ", DATA_SIZE_SIMUL)
% # ggplot(benchmarks_df_simul, aes(x = procedure, y = time)) +
% #   geom_boxplot() +
% #   facet_grid(. ~ m, space ="free_x", scales="free_x", switch="x") +
% #   theme_Publication() +
% #   xlab("Exact/inexact gradient and hessian") +
% #   ylab("Time (seconds)") +
% #   ggtitle(title, "Parameter estimation time") +
% #   scale_y_log10()
% plots <- list()
% for (i in M_LIST_SIMUL) {
%   title <- paste0("Simulated data, m = ", i)
%   plots[[i]] <- benchmarks_df_simul %>%
%     filter(m == i) %>%
%     ggplot(aes(x = procedure, y = time)) +
%     geom_boxplot() +
%     theme_Publication() +
%     xlab("Exact/inexact gradient and hessian") +
%     ylab("Time (seconds)") +
%     ggtitle(title, "Parameter estimation time") +
%     scale_y_log10()
% }
% ggarrange(plotlist = plots, ncol = 2, nrow = 2)
% @
% The average speed increases using a simulated dataset (\autoref{table:speed-simul}) shows similar results to the speed gains from the other datasets, showing that {\tt{TMB}} is useful for estimating Poisson HMMs on medium sized datasets.
% <<speed-percent-simul-table, results='asis', echo = FALSE>>=
% incr_simul <- data.frame(m = integer(),
%                          "TMB1" = numeric(),
%                          "TMB2" = numeric(),
%                          "TMB3" = numeric(),
%                          "TMB4" = numeric())
% for (idx in 1:length(M_LIST_SIMUL)) {
%   m <- M_LIST_SIMUL[idx]
%   incr_simul[idx, "m"] <- m
%   boxplot_data <- benchmarks_simul[[idx]]
% 
%   incr_simul[idx, "TMB1"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB1"])) - 1
%   incr_simul[idx, "TMB2"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB2"])) - 1
%   incr_simul[idx, "TMB3"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB3"])) - 1
%   incr_simul[idx, "TMB4"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB4"])) - 1
% }
% incr_simul[, - 1] <- incr_simul[, - 1] * 100
% table <- xtable(incr_simul,
%                 display = c("d", "d", rep("f", 4)),
%                 caption = paste0("Speed percentage increase of using TMB for m state Poisson HMM parameter estimation, estimated on the simulated dataset. Each procedure was repeated and timed. Their mean times are compared in this table. When m=1, the mean estimation time with TMB by providing the exact gradient but not the exact hessian (TMB3) was ", round(incr_simul[1, 4]), "\\% lower than the mean estimation time with direct maximization without TMB (DM). The naming convention is found in \\autoref{table:notation}."),
%                 label = "table:speed-simul",
%                 digits = 0)
% print(table, include.rownames = FALSE)
% @
% 
% \item Log-likelihoods\\
% Optimizing with {\tt{TMB}} gives the same estimates as optimizing without (see \autoref{table:2-state-estimates}) when estimating a 2 state Poisson HMM on the lamb dataset.
% Similar conclusions are obtained for Poisson HMMs estimated on the hospital dataset and simulated datasets.
% <<lamb-estimates, results='asis', echo = FALSE>>=
% idx <- 2
% # Parameters and covariates
% m <- M_LIST_LAMB[idx]
% if (m == 1) {
%   gamma <- matrix(1)
% } else {
%   gamma <- matrix(0.2 / (m - 1), nrow = m, ncol = m)
%   diag(gamma) <- 0.8
% }
% lambda <- seq(0.3, 4, length.out = m)
% delta <- stat.dist(gamma)
% 
% # Parameters & covariates for TMB
% working_params <- pois.HMM.pn2pw(m, lambda, gamma)
% TMB_data <- list(x = lamb_data, m = m)
% obj <- MakeADFun(TMB_data, working_params, DLL = "poi_hmm", silent = TRUE)
% 
% # Estimation
% dm <- DM.estimate(x = lamb_data,
%                   m = m,
%                   lambda0 = lambda,
%                   gamma0 = gamma)
% tmb1 <- TMB.estimate(TMB_data = TMB_data,
%                      parameters = working_params,
%                      MakeADFun_obj = obj)
% tmb2 <- TMB.estimate(TMB_data = TMB_data,
%                      parameters = working_params,
%                      MakeADFun_obj = obj,
%                      hessian = TRUE)
% tmb3 <- TMB.estimate(TMB_data = TMB_data,
%                      parameters = working_params,
%                      MakeADFun_obj = obj,
%                      gradient = TRUE)
% tmb4 <- TMB.estimate(TMB_data = TMB_data,
%                      parameters = working_params,
%                      MakeADFun_obj = obj,
%                      gradient = TRUE,
%                      hessian = TRUE)
% 
% # Table
% estimates_df <- data.frame("DM" = numeric(),
%                            "TMB1" = numeric(),
%                            "TMB2" = numeric(),
%                            "TMB3" = numeric(),
%                            "TMB4" = numeric())
% 
% estimates_df[1:(m + m ^ 2 + m), "DM"] <- c(dm$lambda, as.numeric(dm$gamma), dm$delta)
% estimates_df[1:(m + m ^ 2 + m), "TMB1"] <- c(tmb1$lambda, as.numeric(tmb1$gamma), tmb1$delta)
% estimates_df[1:(m + m ^ 2 + m), "TMB2"] <- c(tmb2$lambda, as.numeric(tmb2$gamma), tmb2$delta)
% estimates_df[1:(m + m ^ 2 + m), "TMB3"] <- c(tmb3$lambda, as.numeric(tmb3$gamma), tmb3$delta)
% estimates_df[1:(m + m ^ 2 + m), "TMB4"] <- c(tmb4$lambda, as.numeric(tmb4$gamma), tmb4$delta)
% 
% row_names_latex <- paste0(rep("$\\lambda_{", m), 1:m, "}$")
% for (gamma_idx in 1:m ^ 2) {
%   row_col_idx <- matrix.col.idx.to.rowcol(gamma_idx, m)
%   row_names_latex <- c(row_names_latex,
%                        paste0("$\\gamma_{", toString(row_col_idx), "}$"))
% }
% row_names_latex <- c(row_names_latex,
%                      paste0(rep("$\\delta_{", m), 1:m, "}$"))
% row.names(estimates_df) <- row_names_latex
% 
% table <- xtable(estimates_df,
%                 caption = "Estimates of 2 state Poisson HMM with and without using TMB, estimated on the lamb dataset. Naming convention is found in \\autoref{table:notation}",
%                 label = "table:2-state-estimates")
% print(table,
%       sanitize.rownames.function = identity)
% @
% 
% Moreover, the percentage speed gains from using {\tt{TMB}} (\autoref{table:increase-likelihood-lamb} and \autoref{table:increase-likelihood-simul}) are all positive, thus showing that on small and medium sized datasets, the objective function computation time can be accelerated.
% <<tmb-acceleration-lamb-table-log, results='asis', echo = FALSE>>=
% incr_mllk_time_lamb <- data.frame(m = integer(),
%                                   "TMB1" = numeric(),
%                                   "TMB2" = numeric(),
%                                   "TMB3" = numeric(),
%                                   "TMB4" = numeric())
% for (idx in 1:length(M_LIST_LAMB)) {
%   m <- M_LIST_LAMB[idx]
%   incr_mllk_time_lamb[idx, "m"] <- m
%   boxplot_data <- mllk_times_lamb[[idx]]
% 
%   incr_mllk_time_lamb[idx, "TMB1"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB1"])) - 1
%   incr_mllk_time_lamb[idx, "TMB2"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB2"])) - 1
%   incr_mllk_time_lamb[idx, "TMB3"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB3"])) - 1
%   incr_mllk_time_lamb[idx, "TMB4"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB4"])) - 1
% }
% incr_mllk_time_lamb[, - 1] <- incr_mllk_time_lamb[, - 1] * 100
% table <- xtable(incr_mllk_time_lamb,
%                 display = c("d", "d", rep("f", 4)),
%                 caption = paste0("Speed percentage increase of using TMB for m state Poisson HMM negative log-likelihood calculation, estimated on the lamb dataset. Each procedure was repeated and timed. Their mean times are compared in this table. When m=1, the mean calculation time with TMB by providing the exact gradient but not the exact hessian (TMB3) was ", round(incr_mllk_time_lamb[1, 4]), "\\% lower than the mean estimation time with direct maximization without TMB (DM). Naming convention is found in \\autoref{table:notation}"),
%                 label = "table:increase-likelihood-lamb",
%                 digits = 0)
% print(table, include.rownames = FALSE)
% 
% 
% incr_mllk_time_simul <- data.frame(m = integer(),
%                                    "TMB1" = numeric(),
%                                    "TMB2" = numeric(),
%                                    "TMB3" = numeric(),
%                                    "TMB4" = numeric())
% for (idx in 1:length(M_LIST_SIMUL)) {
%   m <- M_LIST_SIMUL[idx]
%   incr_mllk_time_simul[idx, "m"] <- m
%   boxplot_data <- mllk_times_simul[[idx]]
% 
%   incr_mllk_time_simul[idx, "TMB1"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB1"])) - 1
%   incr_mllk_time_simul[idx, "TMB2"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB2"])) - 1
%   incr_mllk_time_simul[idx, "TMB3"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB3"])) - 1
%   incr_mllk_time_simul[idx, "TMB4"] <- mean(with(boxplot_data, time[expr == "DM"])) /
%     mean(with(boxplot_data, time[expr == "TMB4"])) - 1
% }
% incr_mllk_time_simul[, - 1] <- incr_mllk_time_simul[, - 1] * 100
% table <- xtable(incr_mllk_time_simul,
%                 display = c("d", "d", rep("f", 4)),
%                 caption = paste0("Speed percentage increase of using TMB for m state Poisson HMM negative log-likelihood calculation, estimated on the simulated dataset. Each procedure was repeated and timed. Their mean times are compared in this table. When m=1, the mean calculation time with TMB by providing the exact gradient but not the exact hessian (TMB3) was ", round(incr_mllk_time_simul[1, 4]), "\\% lower than the mean estimation time with direct maximization without TMB (DM). Naming convention is found in \\autoref{table:notation}"),
%                 label = "table:increase-likelihood-simul",
%                 digits = 0)
% print(table, include.rownames = FALSE)
% @
% 
% \item \label{txt:all-methods-comparison} All optimization methods\\
% Finally, we compare different optimization methods.
% The ones we retain are BFGS, Nelder-Mead, L-BFGS-B, nlm, nlminb, and hjn, because the others don't converge in our case.
% {\tt{marqLevAlg}} provides an algorithm for least-squares curve fitting, and is therefore included in the comparison.
% Exact gradients and hessians are provided by {\tt{TMB}} and fed to each algorithm.
% The speed comparisons are in the following \autoref{fig:all-methods-comparison}.
% 
% <<all-methods-comparison, fig.cap="Poisson HMM parameter estimation time per optimization method", fig.height = 3, echo = FALSE>>=
% # include_graphics(c(paste0("Plots/lamb_all_methods_m=", 2, ".png"),
% #                    paste0("Plots/simul_all_methods_m=", 3, ".png")))
% title <- paste0("Lamb data, size = ", DATA_SIZE_LAMB, ", m = 2")
% boxplot_data <- method_comparison_lamb[[1]]
% boxplot_data$time <- boxplot_data$time / 10 ^ 6
% plot1 <- ggplot(boxplot_data, aes(x = expr, y = time)) +
%   geom_boxplot() +
%   theme_Publication() +
%   xlab("Method") +
%   ylab("Time (milliseconds)") +
%   ggtitle(title, "Parameter estimation time") +
%   scale_y_log10()
% 
% title <- paste0("Simulated data, size = ", DATA_SIZE_SIMUL, ", m = 3")
% boxplot_data <- method_comparison_simul[[1]]
% boxplot_data$time <- boxplot_data$time / 10 ^ 6
% plot2 <- ggplot(boxplot_data, aes(x = expr, y = time)) +
%   geom_boxplot() +
%   theme_Publication() +
%   xlab("Exact/inexact gradient and hessian") +
%   ylab("Time (milliseconds)") +
%   ggtitle(title, "Parameter estimation time") +
%   scale_y_log10()
% ggarrange(plot1, plot2, ncol = 2, nrow = 1)
% @
% 
% The following tables summarize the estimates and their confidence intervals, for the lamb dataset and for the simulated dataset.
% Instead of showing the standard error, we display the lower and upper bounds of the confidence intervals.
% The reason is that sometimes, only one bound of the interval is known when using the profile likelihood method and hence doesn't obtain a standard error.
% <<lamb-estimates-table, results = 'asis', echo = FALSE>>=
% print(xtable(conf_int_lamb[conf_int_lamb$m <= 2, ],
%              caption = "Estimates and standard errors on the lamb dataset",
%              label = "table:lamb_estimates_std_errors",
%              align = rep("c", 10)),
%       include.rownames = FALSE,
%       scalebox = '0.8',
%       sanitize.text.function = identity)
% @
% 
% <<simul-estimates-table, results = 'asis', echo = FALSE>>=
% print(xtable(conf_int_simul[conf_int_simul$m <= 3, ],
%              caption = "Estimates and standard errors on the simulated dataset",
%              label = "table:simul_estimates_std_errors",
%              align = rep("c", 11)),
%       include.rownames = FALSE,
%       scalebox = '0.75',
%       sanitize.text.function = identity)
% @
% \end{enumerate}
% 
% All the code used to produce those results is available in the supporting information.
% 
% \newpage
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Discussion}
% \label{sec:discussion}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% In this paper, an alternative method of computing Poisson HMM parameters and their standard errors is examined.
% Using the language R, the usual method can take a long time, particularly if the standard errors are computed by bootstrapping.
% The approach with {\tt{TMB}} was timed on 3 datasets with different amounts of data, and compared with the times of estimations using R without making use of {\tt{TMB}}.
% For computational reasons, this paper doesn't estimate HMMs on millions or billions of data points, but the performance gains are expected to be large even in these cases.
% As can be seen in \autoref{table:simul_estimates_std_errors} and \autoref{table:lamb_estimates_std_errors}, the standard errors obtained through this method are very similar to the standard errors obtained through profiling the likelihood and bootstrapping while being less computationally intensive.
% The method used in this paper can easily be extended to other distributions.
% 
% It is recommended to use both {\tt{TMB}}'s gradient and hessian since it ensures a more reliable result, although it doesn't make any difference in our computations.
% However, including the hessian can make the optimization slower because of its size, as demonstrated by the difference in time between TMB3 and TMB4 when dealing with a multiple state Poisson HMM on a large amount of data (hospital dataset).
% 
% Although we have not tried, it should be possible to extend this approach to HMMs with random effects for panel data for example.
% Introduced by \citet{altman}, Mixed HMM (MHMM) is a class of HMM that combines fixed and random effects, by using the framework of Generalized Linear Mixed Models.
% {\tt{TMB}} should be usable for MHMMs since the likelihood can be optimized through direct maximization as shown by \citet{altman}, or through the EM algorithm as shown by \citet{maruotti}.
% Both authors list possible approaches to estimate MHMMs.
% \citet[p .~7]{altman} reports a numerical integration taking from 1 second with 1 random effect to several days when 4 random effects are included, and a Monte Carlo method requiring approximately 3 days to estimate a MHMM with 3 random effects.
% In addition, both assume the observations (conditional on the random effects and the hidden states), to be distributed in the exponential family.
% By default, {\tt{TMB}} integrates out the random effects and then uses a Laplace approximation on the negative joint log-likelihood.
% However, if needed (for example because the minimizer with respect to the random effect is not unique) one can set this approximation explicitly in C++.
% 
% 
% \begin{enumerate}
% \item
% To our surprise, TMB3 was the fastest in most cases although we don't understand why.
% It operates only with an exact gradient provided by {\tt{TMB}} but without the exact hessian.
% This happened when we used hidden states ($m > 1$), and also when using other optimizers than {\tt{nlminb}} such as {\tt{nlm}}.
% Although adding the exact hessian on top of the gradient to the optimizer seems to slow down the computation, it might help the optimizer converge in some cases, and could be worth investigating.
% 
% \item
% When providing only {\tt{TMB}}'s hessian (TMB2), optimizers sometimes fail to converge.
% 
% \item
% We have timed this approach on different volumes of data, but we haven't tried very large datasets of sizes in the millions or billions.
% However, we expect the increase in speed to be smaller since the time necessary becomes longer independently of the approach chosen.
% 
% \item Mention the prospect of using {\tt{TMB}} for panel data with random effects, Laplace approximation.
% \end{enumerate}
% 
% 
% \begin{acknowledgement}
% We gratefully thank Dr. Bertrand GALICHON and Dr. Anthony CHAUVIN for their patience and their efforts to provide the hospital dataset along with the necessary authorizations.
% \end{acknowledgement}
% \vspace*{1pc}
% 
% \noindent {\bf{Conflict of Interest}}
% 
% \noindent {\it{The authors have declared no conflict of interest.}}
% 
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{Appendix}
% \label{sec:appendix}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection*{A.1.\enspace R Code}
% \label{subsec:rcode}
% \begin{enumerate}
% 
% \item \label{code:setup_parameters.R} Code to setup global parameters and declare functions used
% % \lstinputlisting[language={R}]{code/setup_parameters.R}
% 
% \item \label{code:packages.R} Packages
% % \lstinputlisting[language={R}]{code/packages.R}
% 
% \item \label{code:utils.R} Functions
% % \lstinputlisting[language={R}]{functions/utils.R}
% 
% \item \label{code:poi_hmm_lamb.R} Code to run estimations and comparisons using the lamb dataset
% % \lstinputlisting[language={R}]{code/poi_hmm_lamb.R}
% 
% \item \label{code:poi_hmm_simul.R} Code to run estimations and comparisons using a simulated dataset
% % \lstinputlisting[language={R}]{code/poi_hmm_simul.R}
% 
% \item \label{code:poi_hmm_hosp.R} Code to run estimations and comparisons using the hospital dataset
% % \lstinputlisting[language={R}]{code/poi_hmm_hosp.R}
% 
% \end{enumerate}
% 
% \subsection*{A.2.\enspace C++ Code}
% \label{subsec:cppcode}
% \begin{enumerate}
% 
% \item \label{code:poi_hmm.cpp} Poisson HMM negative log-likelihood calculation
% This is the file poi\_hmm.cpp which contains the negative likelihood function.
% 
% It should be noted that in the C++ file, testing if the value is missing (i.e. testing for NaN (Not A Number) values) requires a little trick.
% The reason is that the standard test function \texttt{std::isnan()} doesn't currently work on a single data value inside {\tt{TMB}}.
% Cuurently in C++, comparisons involving NaN values are always false, except when testing inequality between 2 NaN values.
% In other words, for a float f, the expression f != f is true if and only if f is a NaN value.
% Similarly, f == f returns false if and only if f is a NaN value.
% \lstinputlisting{code/poi_hmm.cpp}
% 
% \item \label{code:Delta_w2n}
% This optional function is in the file utils.cpp.
% It is only necessary if a stationary distribution is not assumed, and $\bfdelta$ is used as a parameter instead of being derived from the transistion probability matrix $\bgamma$.
% It codes the function to convert the working parameter \texttt{tdelta} into its natural format.
% \lstinputlisting[linerange=//latex_Delta_w2n_start-//latex_Delta_w2n_end]{functions/utils.cpp}
% 
% \item \label{code:Gamma_w2n}
% This function is in the file utils.cpp.
% It transforms the transition probability matrix $\bgamma$ from its working format to its natural format.
% \lstinputlisting[linerange=//latex_Gamma_w2n_start-//latex_Gamma_w2n_end]{functions/utils.cpp}
% 
% \item \label{code:Stat_dist}
% This function is in the file utils.cpp.
% It derives the stationary distribution from the transition probability matrix $\bgamma$.
% \lstinputlisting[linerange=//latex_Stat_dist_start-//latex_Stat_dist_end]{functions/utils.cpp}
% 
% \item \label{code:utils.cpp} Functions used in C++ Poisson HMM code
% % \lstinputlisting{functions/utils.cpp}
% 
% \item \label{code:linreg.cpp} Linear model negative log-likelihood calculation
% % \lstinputlisting{code/linreg.cpp}
% 
% \item \label{code:utils_linreg.cpp} Functions used in C++ linear model code
% % \lstinputlisting{functions/utils_linreg.cpp}
% 
% \end{enumerate}
% 
% % \begin{enumerate}
% % \item simple C acceleration of Zucchini scripts p. 333, A 1.7, A 1.8 with conditional probabilities outside of the forward / backward loop
% % \item Use same order as in Zucchini
% % \item the .cpp file with transformation code
% % \item the .cpp file with likelihood
% % \item an .R file showcasing the use
% % \end{enumerate}
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage %Before bibliography
\bibliographystyle{plainnat}
\bibliography{paper}
% \addcontentsline{toc}{chapter}{References}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
