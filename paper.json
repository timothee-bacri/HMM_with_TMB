[
  {"id":"altman","abstract":"Hidden Markov models (HMMs) are a useful tool for capturing the behavior of overdispersed, autocorrelated data. These models have been applied to many different problems, including speech recognition, precipitation modeling, and gene finding and profiling. Typically, HMMs are applied to individual stochastic processes; HMMs for simultaneously modeling multiple processes—as in the longitudinal data setting—have not been widely studied. In this article I present a new class of models, mixed HMMs (MHMMs), where I use both covariates and random effects to capture differences among processes. I define the models using the framework of generalized linear mixed models and discuss their interpretation. I then provide algorithms for parameter estimation and illustrate the properties of the estimators via a simulation study. Finally, to demonstrate the practical uses of MHMMs, I provide an application to data on lesion counts in multiple sclerosis patients. I show that my model, while parsimonious, can describe the heterogeneity among such patients.","accessed":{"date-parts":[[2020,2,25]]},"author":[{"family":"Altman","given":"Rachel MacKay"}],"container-title":"Journal of the American Statistical Association","container-title-short":"Journal of the American Statistical Association","DOI":"10.1198/016214506000001086","ISSN":"0162-1459","issue":"477","issued":{"date-parts":[[2007,3,1]]},"page":"201-210","source":"Taylor and Francis+NEJM","title":"Mixed Hidden Markov Models","type":"article-journal","URL":"https://doi.org/10.1198/016214506000001086","volume":"102"},
  {"id":"bartolucci","author":[{"family":"Bartolucci","given":"F."},{"family":"Farcomeni","given":"A."},{"family":"Pennoni","given":"F."}],"collection-title":"Chapman & Hall/CRC statistics in the social and behavioral sciences","ISBN":"978-1-4398-1708-7","issued":{"date-parts":[[2012]]},"publisher":"Taylor & Francis","title":"Latent markov models for longitudinal data","type":"book","URL":"https://books.google.no/books?id=X6aMQAAACAAJ"},
  {"id":"baum","accessed":{"date-parts":[[2020,8,18]]},"author":[{"family":"Baum","given":"Leonard E."},{"family":"Petrie","given":"Ted"}],"container-title":"The Annals of Mathematical Statistics","container-title-short":"Ann. Math. Statist.","DOI":"10.1214/aoms/1177699147","ISSN":"0003-4851","issue":"6","issued":{"date-parts":[[1966,12]]},"language":"en","page":"1554-1563","source":"DOI.org (Crossref)","title":"Statistical Inference for Probabilistic Functions of Finite State Markov Chains","type":"article-journal","URL":"http://projecteuclid.org/euclid.aoms/1177699147","volume":"37"},
  {"id":"bauma","accessed":{"date-parts":[[2020,8,18]]},"archive":"JSTOR","author":[{"family":"Baum","given":"Leonard E."},{"family":"Petrie","given":"Ted"},{"family":"Soules","given":"George"},{"family":"Weiss","given":"Norman"}],"container-title":"The Annals of Mathematical Statistics","ISSN":"00034851","issue":"1","issued":{"date-parts":[[1970]]},"page":"164-171","publisher":"Institute of Mathematical Statistics","title":"A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains","type":"article-journal","URL":"www.jstor.org/stable/2239727","volume":"41"},
  {"id":"bulla","abstract":"The parameters of a hidden Markov model (HMM) can be estimated by numerical maximization of the log-likelihood function or, more popularly, using the expectation–maximization (EM) algorithm. In its standard implementation the latter is unsuitable for fitting stationary hidden Markov models (HMMs). We show how it can be modified to achieve this. We propose a hybrid algorithm that is designed to combine the advantageous features of the two algorithms and compare the performance of the three algorithms using simulated data from a designed experiment, and a real data set. The properties investigated are speed of convergence, stability, dependence on initial values, different parameterizations. We also describe the results of an experiment to assess the true coverage probability of bootstrap-based confidence intervals for the parameters.","author":[{"family":"Bulla","given":"Jan"},{"family":"Berzel","given":"Andreas"}],"container-title":"Computational Statistics","container-title-short":"Computational Statistics","DOI":"10.1007/s00180-007-0063-y","ISSN":"1613-9658","issue":"1","issued":{"date-parts":[[2008,1,1]]},"page":"1-18","title":"Computational issues in parameter estimation for stationary hidden Markov models","type":"article-journal","URL":"https://doi.org/10.1007/s00180-007-0063-y","volume":"23"},
  {"id":"cappe","abstract":"Hidden Markov models have become a widely used class of statistical models with applications in diverse areas such as communications engineering, bioinformatics, finance and many more. This book is a comprehensive treatment of inference for hidden Markov models, including both algorithms and statistical theory. Topics range from filtering and smoothing of the hidden Markov chain to parameter estimation, Bayesian methods and estimation of the number of states. In a unified way the book covers both models with finite state spaces, which allow for exact algorithms for filtering, estimation etc. and models with continuous state spaces (also called state-space models) requiring approximate simulation-based algorithms that are also described in detail. Simulation in hidden Markov models is addressed in five different chapters that cover both Markov chain Monte Carlo and sequential Monte Carlo approaches. Many examples illustrate the algorithms and theory. The book also carefully treats Gaussian linear state-space models and their extensions and it contains a chapter on general Markov chain theory and probabilistic aspects of hidden Markov models. This volume will suit anybody with an interest in inference for stochastic processes, and it will be useful for researchers and practitioners in areas such as statistics, signal processing, communications engineering, control theory, econometrics, finance and more. The algorithmic parts of the book do not require an advanced mathematical background, while the more theoretical parts require knowledge of probability theory at the measure-theoretical level. From the reviews: \"By providing an overall survey of results obtained so far in a very readable manner, and also presenting some new ideas, this well-written book will appeal to academic researchers in the field of HMMs, with PhD students working on related topics included. It will also appeal to practitioners and researchers from other fields by guiding them through the computational steps needed for making inference HMMs and/or by providing them with the relevant underlying statistical theory. In the reviewer's opinion this book will shortly become a reference work in its field.\" MathSciNet \"This monograph is a valuable resource. It provides a good literature review, an excellent account of the state of the art research on the necessary theory and algorithms, and ample illustrations of numerous applications of HMM. It goes much beyond the earlier resources on HMM...I anticipate this work to serve well many Technometrics readers in the coming years.\" Haikady N. Nagaraja for Technometrics, November 2006","author":[{"family":"Cappé","given":"Olivier"},{"family":"Moulines","given":"Eric"},{"family":"Ryden","given":"Tobias"}],"ISBN":"978-0-387-28982-3","issued":{"date-parts":[[2006,4,18]]},"language":"en","number-of-pages":"656","publisher":"Springer Science & Business Media","source":"Google Books","title":"Inference in Hidden Markov Models","type":"book"},
  {"id":"casella","abstract":"This book builds theoretical statistics from the first principles of probability theory. Starting from the basics of probability, the authors develop the theory of statistical inference using techniques, definitions, and concepts that are statistical and are natural extensions and consequences of previous concepts. Intended for first-year graduate students, this book can be used for students majoring in statistics who have a solid mathematics background. It can also be used in a way that stresses the more practical uses of statistical theory, being more concerned with understanding basic statistical concepts and deriving reasonable statistical procedures for a variety of situations, and less concerned with formal optimality investigations.Important Notice: Media content referenced within the product description or the product text may not be available in the ebook version.","author":[{"family":"Casella","given":"George"},{"family":"Berger","given":"Roger L."}],"ISBN":"978-0-357-75313-2","issued":{"date-parts":[[2021,1,26]]},"language":"en","number-of-pages":"690","publisher":"Cengage Learning","source":"Google Books","title":"Statistical Inference","type":"book"},
  {"id":"celeux","abstract":"The problem of estimating the number of hidden states in a hidden Markov model is considered. Emphasis is placed on cross-validated likelihood criteria. Using cross-validation to assess the number of hidden states allows to circumvent the well-documented technical difficulties of the order identification problem in mixture models. Moreover, in a predictive perspective, it does not require that the sampling distribution belongs to one of the models in competition. However, computing cross-validated likelihood for hidden Markov models for which only one training sample is available, involves difficulties since the data are not independent. Two approaches are proposed to compute cross-validated likelihood for a hidden Markov model. The first one consists of using a deterministic half-sampling procedure, and the second one consists of an adaptation of the EM algorithm for hidden Markov models, to take into account randomly missing values induced by cross-validation. Numerical experiments on both simulated and real data sets compare different versions of cross-validated likelihood criterion and penalised likelihood criteria, including BIC and a penalised marginal likelihood criterion. Those numerical experiments highlight a promising behaviour of the deterministic half-sampling criterion.","accessed":{"date-parts":[[2021,1,18]]},"author":[{"family":"Celeux","given":"Gilles"},{"family":"Durand","given":"Jean-Baptiste"}],"container-title":"Computational Statistics","container-title-short":"Comput Stat","DOI":"10.1007/s00180-007-0097-1","ISSN":"1613-9658","issue":"4","issued":{"date-parts":[[2008,10,1]]},"language":"en","page":"541-564","source":"Springer Link","title":"Selecting hidden Markov model state number with cross-validated likelihood","type":"article-journal","URL":"https://doi.org/10.1007/s00180-007-0097-1","volume":"23"},
  {"id":"drton","abstract":"We consider approximate Bayesian model choice for model selection problems that involve models whose Fisher information matrices may fail to be invertible along other competing submodels. Such singular models do not obey the regularity conditions underlying the derivation of Schwarz's Bayesian information criterion BIC and the penalty structure in BIC generally does not reflect the frequentist large sample behaviour of the marginal likelihood. Although large sample theory for the marginal likelihood of singular models has been developed recently, the resulting approximations depend on the true parameter value and lead to a paradox of circular reasoning. Guided by examples such as determining the number of components in mixture models, the number of factors in latent factor models or the rank in reduced rank regression, we propose a resolution to this paradox and give a practical extension of BIC for singular model selection problems.","accessed":{"date-parts":[[2021,5,13]]},"author":[{"family":"Drton","given":"Mathias"},{"family":"Plummer","given":"Martyn"}],"container-title":"Journal of the Royal Statistical Society: Series B (Statistical Methodology)","DOI":"https://doi.org/10.1111/rssb.12187","ISSN":"1467-9868","issue":"2","issued":{"date-parts":[[2017]]},"language":"en","note":"_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12187","page":"323-380","source":"Wiley Online Library","title":"A Bayesian information criterion for singular models","type":"article-journal","URL":"https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12187","volume":"79"},
  {"id":"efron","author":[{"family":"Efron","given":"Bradley"},{"family":"Tibshirani","given":"Robert J"}],"event-place":"New York, N.Y.; London","ISBN":"978-0-412-04231-7","issued":{"date-parts":[[1993]]},"language":"English","note":"OCLC: 797437299","publisher":"Chapman & Hall","publisher-place":"New York, N.Y.; London","source":"Open WorldCat","title":"An introduction to the bootstrap","type":"book"},
  {"id":"feller","abstract":"Major changes in this edition include the substitution of probabilistic arguments for combinatorial artifices, and the addition of new sections on branching processes, Markov chains, and the De Moivre-Laplace theorem.","author":[{"family":"Feller","given":"William"}],"ISBN":"978-0-471-25708-0","issued":{"date-parts":[[1968]]},"language":"en","number-of-pages":"534","publisher":"Wiley","source":"Google Books","title":"An introduction to probability theory and its applications","type":"book"},
  {"id":"fredkin","abstract":"[The technique of patch clamp recording makes possible the measurement of current flowing through a single ion channel in a cell membrane. Examination of such recordings suggests that the current is quantal in nature, alternating in a seemingly random manner between \"on\" and \"off,\" but the recordings are corrupted by noise from a variety of sources. In this paper we propose and illustrate methods for restoring the underlying quantal signal from such noisy measurements. The methods use a Markov chain prior distribution for the underlying quantal process and base the restoration on the resulting posterior distribution.]","accessed":{"date-parts":[[2020,5,8]]},"archive":"JSTOR","author":[{"family":"Fredkin","given":"Donald R."},{"family":"Rice","given":"John A."}],"container-title":"Biometrics","DOI":"10.2307/2532301","ISSN":"0006341X, 15410420","issue":"2","issued":{"date-parts":[[1992]]},"page":"427-448","publisher":"[Wiley, International Biometric Society]","title":"Bayesian Restoration of Single-Channel Patch Clamp Recordings","type":"article-journal","URL":"www.jstor.org/stable/2532301","volume":"48"},
  {"id":"fruhwirth-schnatter","abstract":"The prominence of finite mixture modelling is greater than ever. Many important statistical topics like clustering data, outlier treatment, or dealing with unobserved heterogeneity involve finite mixture models in some way or other. The area of potential applications goes beyond simple data analysis and extends to regression analysis and to non-linear time series analysis using Markov switching models. For more than the hundred years since Karl Pearson showed in 1894 how to estimate the five parameters of a mixture of two normal distributions using the method of moments, statistical inference for finite mixture models has been a challenge to everybody who deals with them. In the past ten years, very powerful computational tools emerged for dealing with these models which combine a Bayesian approach with recent Monte simulation techniques based on Markov chains. This book reviews these techniques and covers the most recent advances in the field, among them bridge sampling techniques and reversible jump Markov chain Monte Carlo methods. It is the first time that the Bayesian perspective of finite mixture modelling is systematically presented in book form. It is argued that the Bayesian approach provides much insight in this context and is easily implemented in practice. Although the main focus is on Bayesian inference, the author reviews several frequentist techniques, especially selecting the number of components of a finite mixture model, and discusses some of their shortcomings compared to the Bayesian approach. The aim of this book is to impart the finite mixture and Markov switching approach to statistical modelling to a wide-ranging community. This includes not only statisticians, but also biologists, economists, engineers, financial agents, market researcher, medical researchers or any other frequent user of statistical models. This book should help newcomers to the field to understand how finite mixture and Markov switching models are formulated, what structures they imply on the data, what they could be used for, and how they are estimated. Researchers familiar with the subject also will profit from reading this book. The presentation is rather informal without abandoning mathematical correctness. Previous notions of Bayesian inference and Monte Carlo simulation are useful but not needed. Sylvia Frühwirth-Schnatter is Professor of Applied Statistics and Econometrics at the Department of Applied Statistics of the Johannes Kepler University in Linz, Austria. She received her Ph.D. in mathematics from the University of Technology in Vienna in 1988. She has published in many leading journals in applied statistics and econometrics on topics such as Bayesian inference, finite mixture models, Markov switching models, state space models, and their application in marketing, economics and finance.","accessed":{"date-parts":[[2021,5,12]]},"author":[{"family":"Frühwirth-Schnatter","given":"Sylvia"}],"collection-title":"Springer Series in Statistics","DOI":"10.1007/978-0-387-35768-3","event-place":"New York","ISBN":"978-0-387-32909-3","issued":{"date-parts":[[2006]]},"language":"en","publisher":"Springer-Verlag","publisher-place":"New York","source":"www.springer.com","title":"Finite Mixture and Markov Switching Models","type":"book","URL":"https://www.springer.com/gp/book/9780387329093"},
  {"id":"gales","abstract":"The Application of Hidden Markov Models in Speech Recognition","accessed":{"date-parts":[[2020,5,8]]},"author":[{"family":"Gales","given":"Mark"},{"family":"Young","given":"Steve"}],"container-title":"Foundations and Trends® in Signal Processing","container-title-short":"SIG","DOI":"10.1561/2000000004","ISSN":"1932-8346, 1932-8354","issue":"3","issued":{"date-parts":[[2008,2,20]]},"language":"English","page":"195-304","publisher":"Now Publishers, Inc.","source":"www.nowpublishers.com","title":"The Application of Hidden Markov Models in Speech Recognition","type":"article-journal","URL":"https://www.nowpublishers.com/article/Details/SIG-004","volume":"1"},
  {"id":"grimmett","abstract":"The third edition of this successful text gives a rigorous introduction to probability theory and the discussion of the most important random processes in some depth. It includes various topics which are suitable for undergraduate courses, but are not routinely taught. It is suitable to the beginner, and provides a taste and encouragement for more advanced work. There are four main aims: 1) to provide a thorough but straightforward account of basic probability, giving the reader a natural feel for the subject unburdened by oppressive technicalities, 2) to discuss important random processes in depth with many examples. 3) to cover a range of important but less routine topics, 4) to impart to the beginner the flavour of more advanced work. The books begins with basic ideas common to many undergraduate courses in mathematics, statistics and the sciences; in concludes with topics usually found at graduate level. The ordering and numbering of material in this third edition has been mostly preserved from the second. Minor alterations and additions have been added for clearer exposition. Highlights include new sections on sampling and Markov chain Monte Carlo, geometric probability, coupling and Poisson approximation, large deviations, spatial Poisson processes, renewal-reward, queueing networks, stochastic calculus, Itô's formula and option pricing in the Black-Scholes model for financial markets. In addition there are many (nearly 400) new exercises and problems that are entertaining and instructive; their solutions can be found in the companion volume 'One Thousand Exercises in Probability', (OUP 2001).","author":[{"family":"Grimmett","given":"Geoffrey"},{"family":"Grimmett","given":"Geoffrey R."},{"family":"Grimmett","given":"Professor of Mathematical Statistics Geoffrey"},{"family":"Stirzaker","given":"David"},{"family":"Stirzaker","given":"Mathematical Institute David R."}],"ISBN":"978-0-19-857222-0","issued":{"date-parts":[[2001,5,31]]},"language":"en","number-of-pages":"626","publisher":"OUP Oxford","source":"Google Books","title":"Probability and Random Processes","type":"book"},
  {"id":"hardle","abstract":"The bootstrap is a method for estimating the distribution of an estimator or test statistic by resampling one's data or a model estimated from the data. The methods that are available for implementing the bootstrap and the accuracy of bootstrap estimates depend on whether the data are an independent random sample or a time series. This paper is concerned with the application of the bootstrap to time-series data when one does not have a finite-dimensional parametric model that reduces the data generation process to independent random sampling. We review the methods that have been proposed for implementing the bootstrap in this situation and discuss the accuracy of these methods relative to that of first-order asymptotic approximations. We argue that methods for implementing the bootstrap with time-series data are not as well understood as methods for data that are independent random samples. Although promising bootstrap methods for time series are available, there is a considerable need for further research in the application of the bootstrap to time series. We describe some of the important unsolved problems. Résumé Le bootstrap est une méthode pour estimer la distribution d'un estimateur en rééchantillonnant ses données ou un modéle estiméà partir des données. Les méthodes disponibles pour mettre en oeuvre le bootstrap et la précision des estimateurs de bootstrap dépendent de ce que les données proviennent d'un échantillon aléatoire indépendant ou d'une série temporelle. Cet article concerne l'application du bootstrap aux données des séries temporelles quand on ne dispose pas de modéle paramétrique de dimension finie qui réduise le processus de génération des données à l'échantillonnage aléatoire indépendent. Nous examinons les méthodes qui ont été proposées pour mettre en oeuvre le bootstrap dans cette situation et discutons la precision de ces méthodes comparativement à celle des approximations asymptotiques de premier ordre. Nous montrons que les méthodes pour mettre en oeuvre le bootstrap avec les données des séries temporelles ne sont pas aussi bien comprises que les méthodes pour les données des échantillons aléatoires indépendants. Bien que des méthodes de bootstrap prometteuses pour les séries temporelles soient disponibles, il y a un besoin considérable de recherche supplémental re dans leur application. Nous décrivons quelques problémes importants non résolus.","accessed":{"date-parts":[[2020,2,13]]},"author":[{"family":"Härdle","given":"Wolfgang"},{"family":"Horowitz","given":"Joel"},{"family":"Kreiss","given":"Jens-Peter"}],"container-title":"International Statistical Review","DOI":"10.1111/j.1751-5823.2003.tb00485.x","ISSN":"1751-5823","issue":"2","issued":{"date-parts":[[2003]]},"language":"en","page":"435-459","source":"Wiley Online Library","title":"Bootstrap Methods for Time Series","type":"article-journal","URL":"https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1751-5823.2003.tb00485.x","volume":"71"},
  {"id":"juang","author":[{"family":"Juang","given":"B. H."},{"family":"Rabiner","given":"L. R."}],"container-title":"Technometrics","container-title-short":"Technometrics","DOI":"10.1080/00401706.1991.10484833","ISSN":"0040-1706","issue":"3","issued":{"date-parts":[[1991,8,1]]},"page":"251-272","publisher":"Taylor & Francis","title":"Hidden Markov Models for Speech Recognition","type":"article-journal","URL":"https://www.tandfonline.com/doi/abs/10.1080/00401706.1991.10484833","volume":"33"},
  {"id":"kass","abstract":"We consider two-stage models of the kind used in parametric empirical Bayes (PEB) methodology, calling them conditionally independent hierarchical models. We suppose that there are k “units,” which may be experimental subjects, cities, study centers, etcetera. At the first stage, the observation vectors Yi for units i = 1, …, k are independently distributed with densities p(yi | θi ), or more generally, p(yi | θi, λ). At the second stage, the unit-specific parameter vectors θi are iid with densities p(θi | λ). The PEB approach proceeds by regarding the second-stage distribution as a prior and noting that, if λ were known, inference about θ could be based on its posterior. Since λ is not known, the simplest PEB methods estimate the parameter λ by maximum likelihood or some variant, and then treat λ as if it were known to be equal to this estimate. Although this procedure is sometimes satisfactory, a well-known defect is that it neglects the uncertainty due to the estimation of λ. In this article we suggest that approximate Bayesian inference can provide simple and manageable solutions to this problem. In Bayesian inferences, a prior density π(·) on λ is introduced, the posterior p(λ | y) is calculated, and the posterior density of θi is then equal to the expectation, with respect to p(λ | y), of the conditional posterior p(θi | yi, λ). From the Bayesian point of view, the PEB estimate is of interest because it is a first-order approximation to the posterior mean [having an error of order O(k −1)]. Letting Eλ and Vλ denote the expectation and variance with respect to p(λ | y), we may write the posterior variance of θi as V(θi | y) = Eλ V(θi | yi, λ) + Vλ E(θi | yi, λ). The conditional posterior variance , where is the maximum likelihood estimator, approximates only the first term. When we include an approximation to the second term we obtain a first-order approximation to the posterior variance itself. In many examples, this elementary method, incorporating approximations to both terms, will substantially account for the estimation of λ. We briefly consider second-order approximations, noting that the work of Deely and Lindley (1981) may be extended using expansions derived by Lindley (1980), Mosteller and Wallace (1964), Tierney and Kadane (1986), and Tierney, Kass, and Kadane (1989). We suggest that second-order approximations provide rough and, often, easily computed assessments of accuracy of first-order approximations. Although we confine our data-analytical examples to simple models, we believe the methods will be useful in general settings. An important area of application is longitudinal data analysis.","accessed":{"date-parts":[[2020,3,22]]},"author":[{"family":"Kass","given":"Robert E."},{"family":"Steffey","given":"Duane"}],"container-title":"Journal of the American Statistical Association","DOI":"10.1080/01621459.1989.10478825","ISSN":"0162-1459","issue":"407","issued":{"date-parts":[[1989,9,1]]},"page":"717-726","source":"Taylor and Francis+NEJM","title":"Approximate Bayesian Inference in Conditionally Independent Hierarchical Models (Parametric Empirical Bayes Models)","type":"article-journal","URL":"https://doi.org/10.1080/01621459.1989.10478825","volume":"84"},
  {"id":"kristensen","author":[{"family":"Kristensen","given":"Kasper"},{"family":"Nielsen","given":"Anders"},{"family":"Berg","given":"Casper W."},{"family":"Skaug","given":"Hans"},{"family":"Bell","given":"Brad"}],"container-title":"arXiv preprint arXiv:1509.00660","issued":{"date-parts":[[2015]]},"source":"Google Scholar","title":"TMB: automatic differentiation and Laplace approximation","title-short":"TMB","type":"article-journal"},
  {"id":"lagona","author":[{"family":"Lagona","given":"Francesco"},{"family":"Jdanov","given":"Dmitri"},{"family":"Shkolnikova","given":"Maria"}],"container-title":"Statistics in Medicine","DOI":"10.1002/sim.6220","ISSN":"1097-0258","issue":"23","issued":{"date-parts":[[2014]]},"page":"4116-4134","title":"Latent time-varying factors in longitudinal analysis: a linear mixed hidden Markov model for heart rates","type":"article-journal","URL":"http://dx.doi.org/10.1002/sim.6220","volume":"33"},
  {"id":"langeheine","abstract":"When sparse data have to be fitted to a log-linear or latent class model, one cannot use the theoretical chi-square distribution to evaluate model fit, because with sparse data the observed cross-table has too many cells in relation to the number of observations to use a distribution that only holds asymptotically. The choice of a theoretical distribution is also difficult when model-expected frequencies are 0 or when model probabilities are estimated 0 or 1. The authors propose to solve these problems by estimating the distribution of a fit measure, using bootstrap methods. An algorithm is presented for estimating this distribution by drawing bootstrap samples from the model-expected proportions, the so-called nonnaive bootstrap method. For the first time the method is applied to empirical data of varying sparseness, from five different data sets. Results show that the asymptotic chi-square distribution is not at all valid for sparse data.","author":[{"family":"Langeheine","given":"Rolf"},{"family":"Pannekoek","given":"Jeroen"},{"family":"Pol","given":"Frank","non-dropping-particle":"van de"}],"container-title":"Sociological Methods & Research","container-title-short":"Sociological Methods & Research","DOI":"10.1177/0049124196024004004","issued":{"date-parts":[[1996,5,1]]},"page":"492-516","source":"ResearchGate","title":"Bootstrapping Goodness-of-Fit Measures in Categorical Data Analysis","type":"article-journal","volume":"24"},
  {"id":"leroux","abstract":"This paper concerns the use and implementation of maximum-penalized-likelihood procedures for choosing the number of mixing components and estimating the parameters in independent and Markov-dependent mixture models. Computation of the estimates is achieved via algorithms for the automatic generation of starting values for the EM algorithm. Computation of the information matrix is also discussed. Poisson mixture models are applied to a sequence of counts of movements by a fetal lamb in utero obtained by ultrasound. The resulting estimates are seen to provide plausible mechanisms for the physiological process.","accessed":{"date-parts":[[2019,11,19]]},"archive":"JSTOR","author":[{"family":"Leroux","given":"Brian G."},{"family":"Puterman","given":"Martin L."}],"container-title":"Biometrics","DOI":"10.2307/2532308","ISSN":"0006-341X","issue":"2","issued":{"date-parts":[[1992]]},"page":"545-558","source":"JSTOR","title":"Maximum-Penalized-Likelihood Estimation for Independent and Markov- Dependent Mixture Models","type":"article-journal","URL":"www.jstor.org/stable/2532308","volume":"48"},
  {"id":"lystig","abstract":"This article describes a new algorithm for exact computation of the observed information matrix in hidden Markov models that may be performed in a single pass through the data. The score vector and log-likelihood are computed in the same pass. The new algorithm is derived from the forward–backward algorithm traditionally used to evaluate the likelihood in hidden Markov models. Our result is discussed in the context of previous approaches that have been used to obtain approximate standard errors of parameter estimates in these models. Implications for parameter estimation are also discussed. An application of the proposed methods to rainfall occurrence data is provided.","accessed":{"date-parts":[[2020,5,8]]},"author":[{"family":"Lystig","given":"Theodore C"},{"family":"Hughes","given":"James P"}],"container-title":"Journal of Computational and Graphical Statistics","container-title-short":"Journal of Computational and Graphical Statistics","DOI":"10.1198/106186002402","ISSN":"1061-8600","issue":"3","issued":{"date-parts":[[2002,9,1]]},"page":"678-689","publisher":"Taylor & Francis","source":"amstat.tandfonline.com (Atypon)","title":"Exact Computation of the Observed Information Matrix for Hidden Markov Models","type":"article-journal","URL":"https://amstat.tandfonline.com/doi/abs/10.1198/106186002402","volume":"11"},
  {"id":"maruotti","author":[{"family":"Maruotti","given":"Antonello"}],"container-title":"International Statistical Review","DOI":"10.1111/j.1751-5823.2011.00160.x","ISSN":"1751-5823","issue":"3","issued":{"date-parts":[[2011]]},"page":"427-454","title":"Mixed hidden markov models for longitudinal data: an overview","type":"article-journal","URL":"http://dx.doi.org/10.1111/j.1751-5823.2011.00160.x","volume":"79"},
  {"id":"mclachlan","abstract":"An up-to-date, comprehensive account of major issues in finite mixture modeling This volume provides an up-to-date account of the theory and applications of modeling via finite mixture distributions. With an emphasis on the applications of mixture models in both mainstream analysis and other areas such as unsupervised pattern recognition, speech recognition, and medical imaging, the book describes the formulations of the finite mixture approach, details its methodology, discusses aspects of its implementation, and illustrates its application in many common statistical contexts. Major issues discussed in this book include identifiability problems, actual fitting of finite mixtures through use of the EM algorithm, properties of the maximum likelihood estimators so obtained, assessment of the number of components to be used in the mixture, and the applicability of asymptotic theory in providing a basis for the solutions to some of these problems. The author also considers how the EM algorithm can be scaled to handle the fitting of mixture models to very large databases, as in data mining applications. This comprehensive, practical guide: * Provides more than 800 references-40% published since 1995 * Includes an appendix listing available mixture software * Links statistical literature with machine learning and pattern recognition literature * Contains more than 100 helpful graphs, charts, and tables Finite Mixture Models is an important resource for both applied and theoretical statisticians as well as for researchers in the many areas in which finite mixture models can be used to analyze data.","author":[{"family":"McLachlan","given":"Geoffrey J."},{"family":"Peel","given":"David"}],"ISBN":"978-0-471-65406-3","issued":{"date-parts":[[2004,3,22]]},"language":"en","number-of-pages":"450","publisher":"John Wiley & Sons","source":"Google Books","title":"Finite Mixture Models","type":"book"},
  {"id":"meeker","abstract":"Maximum likelihood (ML) provides a powerful and extremely general method for making inferences over a wide range of data/model combinations. The likelihood function and likelihood ratios have clear intuitive meanings that make it easy for students to grasp the important concepts. Modern computing technology has made it possible to use these methods over a wide range of practical applications. However, many mathematical statistics textbooks, particularly those at the Senior/Masters level, do not give this important topic coverage commensurate with its place in the world of modern applications. Similarly, in nonlinear estimation problems, standard practice (as reflected by procedures available in the popular commercial statistical packages) has been slow to recognize the advantages of likelihood-based confidence regions/intervals over the commonly use “normal-theory” regions/intervals based on the asymptotic distribution of the “Wald statistic.” In this note we outline our approach for presenting, to students, confidence regions/intervals based on ML estimation.","accessed":{"date-parts":[[2020,2,13]]},"author":[{"family":"Meeker","given":"William Q."},{"family":"Escobar","given":"Luis A."}],"container-title":"The American Statistician","DOI":"10.1080/00031305.1995.10476112","ISSN":"0003-1305","issue":"1","issued":{"date-parts":[[1995,2,1]]},"page":"48-53","source":"Taylor and Francis+NEJM","title":"Teaching about Approximate Confidence Regions Based on Maximum Likelihood Estimation","type":"article-journal","URL":"https://www.tandfonline.com/doi/abs/10.1080/00031305.1995.10476112","volume":"49"},
  {"id":"pohle","abstract":"We discuss the notorious problem of order selection in hidden Markov models, that is of selecting an adequate number of states, highlighting typical pitfalls and practical challenges arising when analyzing real data. Extensive simulations are used to demonstrate the reasons that render order selection particularly challenging in practice despite the conceptual simplicity of the task. In particular, we demonstrate why well-established formal procedures for model selection, such as those based on standard information criteria, tend to favor models with numbers of states that are undesirably large in situations where states shall be meaningful entities. We also offer a pragmatic step-by-step approach together with comprehensive advice for how practitioners can implement order selection. Our proposed strategy is illustrated with a real-data case study on muskox movement.","accessed":{"date-parts":[[2021,1,18]]},"author":[{"family":"Pohle","given":"Jennifer"},{"family":"Langrock","given":"Roland"},{"family":"Beest","given":"Floris M.","non-dropping-particle":"van"},{"family":"Schmidt","given":"Niels Martin"}],"container-title":"Journal of Agricultural, Biological and Environmental Statistics","container-title-short":"JABES","DOI":"10.1007/s13253-017-0283-8","ISSN":"1537-2693","issue":"3","issued":{"date-parts":[[2017,9,1]]},"language":"en","page":"270-293","source":"Springer Link","title":"Selecting the Number of States in Hidden Markov Models: Pragmatic Solutions Illustrated Using Animal Movement","title-short":"Selecting the Number of States in Hidden Markov Models","type":"article-journal","URL":"https://doi.org/10.1007/s13253-017-0283-8","volume":"22"},
  {"id":"pohlea","abstract":"We discuss the notorious problem of order selection in hidden Markov models, i.e. of selecting an adequate number of states, highlighting typical pitfalls and practical challenges arising when analyzing real data. Extensive simulations are used to demonstrate the reasons that render order selection particularly challenging in practice despite the conceptual simplicity of the task. In particular, we demonstrate why well-established formal procedures for model selection, such as those based on standard information criteria, tend to favor models with numbers of states that are undesirably large in situations where states shall be meaningful entities. We also offer a pragmatic step-by-step approach together with comprehensive advice for how practitioners can implement order selection. Our proposed strategy is illustrated with a real-data case study on muskox movement.","accessed":{"date-parts":[[2021,1,18]]},"author":[{"family":"Pohle","given":"Jennifer"},{"family":"Langrock","given":"Roland"},{"family":"Beest","given":"Floris","non-dropping-particle":"van"},{"family":"Schmidt","given":"Niels Martin"}],"container-title":"arXiv:1701.08673 [q-bio, stat]","issued":{"date-parts":[[2017,4,14]]},"source":"arXiv.org","title":"Selecting the Number of States in Hidden Markov Models - Pitfalls, Practical Challenges and Pragmatic Solutions","type":"article-journal","URL":"http://arxiv.org/abs/1701.08673"},
  {"id":"probst","abstract":"It is well established that emotions influence tinnitus, but the role of emotion dynamics remains unclear. The present study investigated emotion dynamics in N = 306 users of the “TrackYourTinnitus” application who completed the Mini-Tinnitus Questionnaire (Mini-TQ) at one assessment point and provided complete data on at least five assessment points for the following state variables: tinnitus loudness, tinnitus distress, arousal, valence. The repeated arousal and valence ratings were used for two operationalizations of emotion dynamics: intra-individual variability of affect intensity (pulse) as well as intra-individual variability of affect quality (spin). Pearson correlation coefficients showed that the Mini-TQ was positively correlated with pulse (r = 0.19; p < 0.05) as well as with spin (r = 0.12; p < 0.05). Multilevel models revealed the following results: increases in tinnitus loudness were more strongly associated with increases in tinnitus distress at higher levels of pulse as well as at higher levels of spin (both p < 0.05), whereby increases in tinnitus loudness correlated even stronger with increases in tinnitus distress when both pulse as well as spin were high (p < 0.05). Moreover, increases in spin were associated with a less favorable time course of tinnitus loudness (p < 0.05). To conclude, equilibrating emotion dynamics might be a potential target in the prevention and treatment of tinnitus.","accessed":{"date-parts":[[2021,2,17]]},"author":[{"family":"Probst","given":"Thomas"},{"family":"Pryss","given":"Rüdiger"},{"family":"Langguth","given":"Berthold"},{"family":"Schlee","given":"Winfried"}],"container-title":"Scientific Reports","DOI":"10.1038/srep31166","ISSN":"2045-2322","issue":"1","issued":{"date-parts":[[2016,8,4]]},"language":"en","number":"1","page":"31166","publisher":"Nature Publishing Group","source":"www.nature.com","title":"Emotion dynamics and tinnitus: Daily life data from the “TrackYourTinnitus” application","title-short":"Emotion dynamics and tinnitus","type":"article-journal","URL":"https://www.nature.com/articles/srep31166","volume":"6"},
  {"id":"pryss","abstract":"Many highly prevalent diseases (e.g., tinnitus, migraine, chronic pain) are difficult to treat and universally effective treatments are missing. Available treatments are only effective in patient subgroups, i.e., medical doctors and patients have to figure out which therapy might be helpful in the patient's situation. Sufficiently large and qualitative longitudinal data sets, however, would be desirable to facilitate evidence-based treatment decisions for individual patients. On one hand, traditional sensing techniques (i.e., clinical trials) have many merits enabling evidence-based medicine. On the other, they have inherent limitations. First, clinical trials are very cost- and labour-intensive. Second, the traditional approach aims at reducing ecological heterogeneity to enable the investigation of homogeneous subsamples. Recently, a new paradigm emerged that offers promising perspectives for collecting large amounts of longitudinal patient data – Mobile Crowd Sensing. By utilizing smart mobile devices of a large number of patients, health information can be gathered from large patient collections as well as at many different time points and in various real life environmental situations. In the Track Your Tinnitus project, we implemented such a mobile crowd sensing platform to reveal new medical aspects about tinnitus with a particular focus on the variability of tinnitus over time depending on the environmental situation. In this paper, the current project status as well as first lessons learned from running the mobile application for twelve months are presented. In turn, the lessons learned are discussed in the context of the new perspectives offered by mobile crowd sensing in the medical field.","author":[{"family":"Pryss","given":"R."},{"family":"Reichert","given":"M."},{"family":"Herrmann","given":"J."},{"family":"Langguth","given":"B."},{"family":"Schlee","given":"W."}],"container-title":"2015 IEEE 28th International Symposium on Computer-Based Medical Systems","DOI":"10.1109/CBMS.2015.26","event":"2015 IEEE 28th International Symposium on Computer-Based Medical Systems","ISSN":"2372-9198","issued":{"date-parts":[[2015,6]]},"page":"23-24","source":"IEEE Xplore","title":"Mobile Crowd Sensing in Clinical and Psychological Trials – A Case Study","type":"paper-conference"},
  {"id":"pryssa","abstract":"Tinnitus, the phantom sensation of sound, is a highly prevalent disorder that is difficult to treat, i.e., Available treatments are only effective for patient subgroups. Sufficiently large and qualitative longitudinal data sets, which aggregate the individuals' demographic and clinical characteristics, together with their response to specific therapeutic interventions, would therefore facilitate evidence-based treatment suggestions for individual patients. Currently, clinical trials are the standard instrument for realizing evidence-based medicine. However, the related information gathering is limited. For example, clinical trials try to reduce the complexity of the individual case by generating homogeneous groups to obtain significant results. From the latter, individual treatment decisions are inferred. A complementary approach would be to assess the effect of specific interventions in large samples considering the individual peculiarity of each subject. This allows providing individualized treatment decisions. Recently, mobile crowd sensing emerged as an approach for collecting large and ecological valid datasets at rather low costs. By providing mobile crowd sensing services to large numbers of patients, large datasets can be gathered cheaply on a daily basis. In the TrackYourTinnitus project, we implemented a mobile crowd sensing platform to reveal new medical aspects on tinnitus and its treatment. Additionally, we work on mobile services exploring approaches for understanding tinnitus and for improving its diagnostic and therapeutic management. We present the TrackYourTinnitus platform as well as its goals, architecture and preliminary results. Overall, the platform and its mobile services offer promising perspectives for tinnitus research and treatment.","author":[{"family":"Pryss","given":"R."},{"family":"Reichert","given":"M."},{"family":"Langguth","given":"B."},{"family":"Schlee","given":"W."}],"container-title":"2015 IEEE International Conference on Mobile Services","DOI":"10.1109/MobServ.2015.55","event":"2015 IEEE International Conference on Mobile Services","ISSN":"2329-6453","issued":{"date-parts":[[2015,6]]},"page":"352-359","source":"IEEE Xplore","title":"Mobile Crowd Sensing Services for Tinnitus Assessment, Therapy, and Research","type":"paper-conference"},
  {"id":"schadt","abstract":"We have developed a generalization of Kimura’s Markov chain model for base substitution at a single nucleotide site. This generalized model incorporates more flexible transition rates and consequently allows irreversible as well as reversible chains. Because the model embodies just the right amount of symmetry, it permits explicit calculation of finite-time transition probabilities and equilibrium distributions. The model also meshes well with maximum likelihood methods for phylogenetic analysis. Quick calculation of likelihoods and their derivatives can be carried out by adapting Baum’s forward and backward algorithms from the theory of hidden Markov chains. Analysis of HIV sequence data illustrates the speed of the algorithms on trees with many contemporary taxa. Analysis of some of Lake’s data on the origin of the eukaryotic nucleus contrasts the reversible and irreversible versions of the model.","accessed":{"date-parts":[[2020,5,8]]},"author":[{"family":"Schadt","given":"Eric E."},{"family":"Sinsheimer","given":"Janet S."},{"family":"Lange","given":"Kenneth"}],"container-title":"Genome Research","container-title-short":"Genome Res.","DOI":"10.1101/gr.8.3.222","ISSN":"1088-9051, 1549-5469","issue":"3","issued":{"date-parts":[[1998,1,3]]},"language":"en","page":"222-233","PMID":"9521926","publisher":"Cold Spring Harbor Lab","source":"genome.cshlp.org","title":"Computational Advances in Maximum Likelihood Methods for Molecular Phylogeny","type":"article-journal","URL":"http://genome.cshlp.org/content/8/3/222","volume":"8"},
  {"id":"schliehe","author":[{"family":"Schliehe-Diecks","given":"S."},{"family":"Kappeler","given":"P. M."},{"family":"Langrock","given":"R."}],"container-title":"Interface Focus","DOI":"10.1098/rsfs.2011.0077","ISSN":"2042-8898","issue":"2","issued":{"date-parts":[[2012]]},"page":"180-189","title":"On the application of mixed hidden Markov models to multiple behavioural time series","type":"article-journal","URL":"http://rsfs.royalsocietypublishing.org/content/2/2/180","volume":"2"},
  {"id":"turner","abstract":"Ever since the introduction of hidden Markov models by Baum and his co-workers, the method of choice for fitting such models has been maximum likelihood via the EM algorithm. In recent years it has been noticed that the gradient and Hessian of the log likelihood of hidden Markov and related models may be calculated in parallel with a filtering process by which the likelihood may be calculated. Various authors have used, or suggested the use of, this idea in order to maximize the likelihood directly, without using the EM algorithm. In this paper we discuss an implementation of such an approach. We have found that a straightforward implementation of Newton’s method sometimes works but is unreliable. A form of the Levenberg–Marquardt algorithm appears to provide excellent reliability. Two rather complex examples are given for applying this algorithm to the fitting of hidden Markov models. In the first a better than 6-fold increase in speed over the EM algorithm was achieved. The second example turned out to be problematic (somewhat interestingly) in that the maximum likelihood estimator appears to be inconsistent. Whatever its merit, this estimator is calculated much faster by Levenberg–Marquardt than by EM. We also compared the Levenberg–Marquardt algorithm, applied to the first example, with a generic numerical maximization procedure. The Levenberg–Marquardt algorithm appeared to perform almost three times better than the generic procedure, even when analytic derivatives were provided, and 19 times better when they were not provided.","author":[{"family":"Turner","given":"Rolf"}],"container-title":"Computational Statistics & Data Analysis","container-title-short":"Computational Statistics & Data Analysis","DOI":"10.1016/j.csda.2008.01.029","ISSN":"0167-9473","issue":"9","issued":{"date-parts":[[2008,5,15]]},"page":"4147-4160","title":"Direct maximization of the likelihood of a hidden Markov model","type":"article-journal","URL":"http://www.sciencedirect.com/science/article/pii/S0167947308000200","volume":"52"},
  {"id":"venzon","abstract":"The method of constructing confidence regions based on the generalised likelihood ratio statistic is well known for parameter vectors. A similar construction of a confidence interval for a single entry of a vector can be implemented by repeatedly maximising over the other parameters. We present an algorithm for finding these confidence interval endpoints that requires less computation. It employs a modified Newton-Raphson iteration to solve a system of equations that defines the endpoints.","accessed":{"date-parts":[[2020,2,13]]},"author":[{"family":"Venzon","given":"D. J."},{"family":"Moolgavkar","given":"S. H."}],"container-title":"Journal of the Royal Statistical Society: Series C (Applied Statistics)","DOI":"10.2307/2347496","ISSN":"1467-9876","issue":"1","issued":{"date-parts":[[1988]]},"language":"en","page":"87-94","source":"Wiley Online Library","title":"A Method for Computing Profile-Likelihood-Based Confidence Intervals","type":"article-journal","URL":"https://rss.onlinelibrary.wiley.com:443/doi/abs/10.2307/2347496","volume":"37"},
  {"id":"visser","author":[{"family":"Visser","given":"Ingmar"},{"family":"Raijmakers","given":"Maartje E. J."},{"family":"Molenaar","given":"Peter C. M."}],"container-title":"British Journal of Mathematical and Statistical Psychology","DOI":"10.1348/000711000159240","ISSN":"2044-8317","issue":"2","issued":{"date-parts":[[2000]]},"page":"317-327","title":"Confidence intervals for hidden Markov model parameters","type":"article-journal","URL":"http://dx.doi.org/10.1348/000711000159240","volume":"53"},
  {"id":"wald","accessed":{"date-parts":[[2020,6,15]]},"archive":"JSTOR","author":[{"family":"Wald","given":"Abraham"}],"container-title":"Transactions of the American Mathematical Society","DOI":"10.2307/1990256","ISSN":"0002-9947","issue":"3","issued":{"date-parts":[[1943]]},"page":"426-482","publisher":"American Mathematical Society","source":"JSTOR","title":"Tests of Statistical Hypotheses Concerning Several Parameters When the Number of Observations is Large","type":"article-journal","URL":"https://www.jstor.org/stable/1990256","volume":"54"},
  {"id":"zucchini","author":[{"family":"Zucchini","given":"W."},{"family":"MacDonald","given":"I.L."},{"family":"Langrock","given":"R."}],"collection-title":"Chapman & Hall/CRC monographs on statistics & applied probability","ISBN":"978-1-4822-5384-9","issued":{"date-parts":[[2016]]},"publisher":"CRC Press","title":"Hidden markov models for time series: an introduction using r, second edition","type":"book","URL":"https://books.google.no/books?id=KlWzDAAAQBAJ"}
]
