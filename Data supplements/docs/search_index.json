[["confidence-intervals.html", "Chapter 5 Confidence intervals 5.1 Wald-type 5.2 Parametric bootstrap", " Chapter 5 Confidence intervals From the parameters ML estimates, we generate new data and re-estimate the parameters times. From that list of new estimates we can get the 2.5th and 97.5th percentiles and get 95% confidence intervals for the parameters.\\ We show below how to derive confidence intervals using TMB, a likelihood profile based method, and parametric bootstrap, based on the 2 state Poisson HMM estimates. For all three methods, we require a model, so we generate a 2-state Poisson HMM based on the TYT dataset set.seed(123) library(TMB) TMB::compile(&quot;code/poi_hmm.cpp&quot;) ## [1] 0 dyn.load(dynlib(&quot;code/poi_hmm&quot;)) source(&quot;functions/utils.R&quot;) m &lt;- 2 load(&quot;data/tinnitus.RData&quot;) TMB_data &lt;- list(x = tinn_data, m = m) # Initial set of parameters lambda_init &lt;- c(1, 3) gamma_init &lt;- matrix(c(0.8, 0.2, 0.2, 0.8), byrow = TRUE, nrow = m) # Turn them into working parameters parameters &lt;- pois.HMM.pn2pw(m, lambda_init, gamma_init) # Build the TMB object obj_tmb &lt;- MakeADFun(TMB_data, parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE) # Optimize mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn, gradient = obj_tmb$gr, hessian = obj_tmb$he) # MLE ML_working_estimates &lt;- obj_tmb$env$last.par.best ML_natural_estimates &lt;- obj_tmb$report(ML_working_estimates) lambda &lt;- ML_natural_estimates$lambda gamma &lt;- ML_natural_estimates$gamma delta &lt;- ML_natural_estimates$delta 5.1 Wald-type Now that we have a model estimated via TMB, we can derive Wald-type (Wald 1943) confidence intervals. For example, the \\((1 - \\alpha) \\%\\) CI for \\(\\lambda_1\\) is given by \\(\\lambda_1 \\pm z_{1-\\alpha/2} * \\sigma_{\\lambda_1}\\) where \\(z_{x}\\) is the \\(x\\)-percentile of the standard normal distribution, and \\(\\sigma_{\\lambda_1}\\) is the standard error of \\(\\lambda_1\\) obtained via the delta method. First, we require the standard errors. We can retrieve them from the MakeADFun object. The standard errors of the working parameters tlambda and tgamma can be retrieved without needing to add ADREPORT in the C++ file. However, it is usually more interesting to access the standard errors of the natural parameters lambda, gamma and delta. This requires adding a few lines to the C++ file to produce these standard errors, as detailed in Getting started with a linear regression. Be careful: adrep lists gamma column-wise adrep &lt;- summary(sdreport(obj_tmb), &quot;report&quot;) adrep ## Estimate Std. Error ## lambda 1.63641100 0.27758296 ## lambda 5.53309576 0.31876147 ## gamma 0.94980209 0.04374676 ## gamma 0.02592204 0.02088688 ## gamma 0.05019791 0.04374676 ## gamma 0.97407796 0.02088688 ## delta 0.34054200 0.23056437 ## delta 0.65945800 0.23056437 Standard errors for \\(\\hat{{\\boldsymbol\\lambda}}\\) rows &lt;- rownames(adrep) == &quot;lambda&quot; lambda &lt;- adrep[rows, &quot;Estimate&quot;] lambda_std_error &lt;- adrep[rows, &quot;Std. Error&quot;] lambda ## lambda lambda ## 1.636411 5.533096 lambda_std_error ## lambda lambda ## 0.2775830 0.3187615 Standard errors for \\(\\hat{{\\boldsymbol\\Gamma}}\\) rows &lt;- rownames(adrep) == &quot;gamma&quot; gamma &lt;- adrep[rows, &quot;Estimate&quot;] gamma &lt;- matrix(gamma, ncol = m) gamma_std_error &lt;- adrep[rows, &quot;Std. Error&quot;] gamma_std_error &lt;- matrix(gamma_std_error, nrow = m, ncol = m) gamma ## [,1] [,2] ## [1,] 0.94980209 0.05019791 ## [2,] 0.02592204 0.97407796 gamma_std_error ## [,1] [,2] ## [1,] 0.04374676 0.04374676 ## [2,] 0.02088688 0.02088688 Standard errors for \\(\\hat{{\\boldsymbol\\delta}}\\) rows &lt;- rownames(adrep) == &quot;delta&quot; delta &lt;- adrep[rows, &quot;Estimate&quot;] delta_std_error &lt;- adrep[rows, &quot;Std. Error&quot;] delta ## delta delta ## 0.340542 0.659458 delta_std_error ## delta delta ## 0.2305644 0.2305644 5.2 Parametric bootstrap 5.2.1 Generating data In order to perform a parametric bootstrap, we need to be able to generate data from a set of parameters. For readability and code maintenance, it is conventional to store procedures that will be used more than once into functions. The data-generating function is defined in . pois.HMM.generate_sample &lt;- function(ns,mod) { mvect &lt;- 1:mod$m state &lt;- numeric(ns) state[1] &lt;- sample(mvect, 1, prob = mod$delta) for (i in 2:ns) { state[i] &lt;- sample(mvect, 1, prob = mod$gamma[state[i - 1], ]) } x &lt;- rpois(ns, lambda = mod$lambda[state]) return(x) } Further, one can retrieve the state sequence used to generate the data by performing an adjustment: # Generate a random sample from a HMM pois.HMM.generate_sample &lt;- function(ns, mod) { mvect &lt;- 1:mod$m state &lt;- numeric(ns) state[1] &lt;- sample(mvect, 1, prob = mod$delta) for (i in 2:ns) { state[i] &lt;- sample(mvect, 1, prob = mod$gamma[state[i - 1], ]) } x &lt;- rpois(ns, lambda = mod$lambda[state]) return(list(data = x, state = state)) } The results are returned in a list to simplify usage, as the intuitive way (c(x, state)) would append the state sequence to the data. In practice, HMMs sometimes cannot be estimated on generated samples. To deal with this, we can generate a new sample as long as HMMs cannot be estimated on it with the help of this more robust function which can easily be adapted to different needs. Natural parameter estimates are returned for convenience. The argument test_marqLevAlg decides if convergence of marqLevAlg is required. The argument std_error decides if standard errors are returned along with TMBs estimates. This function relies on the custom function TMB_estimate which is a wrapper of nlminb made for Poisson HMM estimation via TMB. # Generate a random sample from a HMM pois.HMM.generate_estimable_sample &lt;- function(ns, mod, testing_params, params_names = PARAMS_NAMES, test_marqLevAlg = FALSE, std_error = FALSE, label_switch = FALSE) { if(anyNA(c(ns, mod, testing_params))) { stop(&quot;Some parameters are missing in pois.HMM.generate_estimable_sample&quot;) } # Count occurrences for each error failure &lt;- c(&quot;state_number&quot; = 0, &quot;TMB_null&quot; = 0, &quot;TMB_converge&quot; = 0, &quot;TMB_G_null&quot; = 0, &quot;TMB_G_converge&quot; = 0, &quot;TMB_H_null&quot; = 0, &quot;TMB_H_converge&quot; = 0, &quot;TMG_GH_null&quot; = 0, &quot;TMG_GH_converge&quot; = 0, &quot;marqLevAlg_null&quot; = 0, &quot;marqLevAlg_converge&quot; = 0, &quot;NA_value&quot; = 0) m &lt;- mod$m # Loop as long as there is an issue with nlminb repeat { mod_temp &lt;- NULL #simulate the data new_data &lt;- pois.HMM.generate_sample(ns = ns, mod = mod) # If the number of states generated is different from m, discard the data if (length(unique(new_data$state)) != m) { failure[&quot;state_number&quot;] &lt;- failure[&quot;state_number&quot;] + 1 next } TMB_benchmark_data &lt;- list(x = new_data$data, m = m) testing_w_params &lt;- pois.HMM.pn2pw(m = m, lambda = testing_params$lambda, gamma = testing_params$gamma, delta = testing_params$delta) # Test TMB suppressWarnings(mod_temp &lt;- TMB.estimate(TMB_data = TMB_benchmark_data, parameters = testing_w_params, std_error = std_error)) # If nlminb doesn&#39;t reach any result, discard the data if (is.null(mod_temp)) { failure[&quot;TMB_null&quot;] &lt;- failure[&quot;TMB_null&quot;] + 1 next } # If nlminb doesn&#39;t converge successfully, discard the data if (mod_temp$convergence != 0) { failure[&quot;TMB_converge&quot;] &lt;- failure[&quot;TMB_converge&quot;] + 1 next } # Test TMB_G suppressWarnings(mod_temp &lt;- TMB.estimate(TMB_data = TMB_benchmark_data, parameters = testing_w_params, gradient = TRUE, std_error = std_error)) # If nlminb doesn&#39;t reach any result, discard the data if (is.null(mod_temp)) { failure[&quot;TMB_G_null&quot;] &lt;- failure[&quot;TMB_G_null&quot;] + 1 next } # If nlminb doesn&#39;t converge successfully, discard the data if (mod_temp$convergence != 0) { failure[&quot;TMB_G_converge&quot;] &lt;- failure[&quot;TMB_G_converge&quot;] + 1 next } # Test TMB_H suppressWarnings(mod_temp &lt;- TMB.estimate(TMB_data = TMB_benchmark_data, parameters = testing_w_params, hessian = TRUE, std_error = std_error)) # If nlminb doesn&#39;t reach any result, discard the data if (is.null(mod_temp)) { failure[&quot;TMB_H_null&quot;] &lt;- failure[&quot;TMB_H_null&quot;] + 1 next } # If nlminb doesn&#39;t converge successfully, discard the data if (mod_temp$convergence != 0) { failure[&quot;TMB_H_converge&quot;] &lt;- failure[&quot;TMB_H_converge&quot;] + 1 next } # Test TMB_GH suppressWarnings(mod_temp &lt;- TMB.estimate(TMB_data = TMB_benchmark_data, parameters = testing_w_params, gradient = TRUE, hessian = TRUE, std_error = std_error)) # If nlminb doesn&#39;t reach any result, discard the data if (is.null(mod_temp)) { failure[&quot;TMB_GH_null&quot;] &lt;- failure[&quot;TMB_GH_null&quot;] + 1 next } # If nlminb doesn&#39;t converge successfully, discard the data if (mod_temp$convergence != 0) { failure[&quot;TMB_GH_converge&quot;] &lt;- failure[&quot;TMB_GH_converge&quot;] + 1 next } # Test marqLevAlg # marqLevAlg sometimes doesn&#39;t converge either, discard the data in these cases if (test_marqLevAlg == TRUE) { testing_w_params &lt;- unlist(testing_w_params) marq &lt;- tryCatch({ marqLevAlg(b = testing_w_params, fn = mod_temp$obj$fn, gr = mod_temp$obj$gr, hess = mod_temp$obj$he, maxiter = 10000) }, error = function(e) { return() }) # If nlminb doesn&#39;t reach any result, discard the data if (is.null(marq)) { failure[&quot;marqLevAlg_null&quot;] &lt;- failure[&quot;marqLevAlg_null&quot;] + 1 next } # If marqLevAlg doesn&#39;t converge successfully, discard the data if (marq$istop != 1) { failure[&quot;marqLevAlg_converge&quot;] &lt;- failure[&quot;marqLevAlg_converge&quot;] + 1 next } } if (label_switch == TRUE) { # Label switching # In practice, we don&#39;t use it for the tests because it doesn&#39;t sort mod$obj # Sorting them could lead to bugs and errors, and although it is better in principle, # there is little benefit in practice. The benefit would be to infer profile likelihood CIs # more easily when calculating coverage probabilities. natural_parameters &lt;- pois.HMM.label.order(m = m, lambda = mod_temp$lambda, gamma = mod_temp$gamma, delta = mod_temp$delta, lambda_std_error = mod_temp$lambda_std_error, gamma_std_error = mod_temp$gamma_std_error, delta_std_error = mod_temp$delta_std_error) } else { natural_parameters &lt;- list(m = m, lambda = mod_temp$lambda, gamma = mod_temp$gamma, delta = mod_temp$delta, lambda_std_error = mod_temp$lambda_std_error, gamma_std_error = mod_temp$gamma_std_error, delta_std_error = mod_temp$delta_std_error) } # If some parameters are NA for some reason, discard the data if (anyNA(natural_parameters[params_names], recursive = TRUE)) { failure[&quot;NA_value&quot;] &lt;- failure[&quot;NA_value&quot;] + 1 next } # If everything went well, end the &quot;repeat&quot; loop break } return(c(data = list(new_data$data), natural_parameters = list(natural_parameters), mod = list(mod_temp), failure = list(failure))) } 5.2.2 Bootstrap set.seed(123) library(TMB) TMB::compile(&quot;code/poi_hmm.cpp&quot;) dyn.load(dynlib(&quot;code/poi_hmm&quot;)) source(&quot;functions/utils.R&quot;) m &lt;- 2 load(&quot;data/tinnitus.RData&quot;) TMB_data &lt;- list(x = tinn_data, m = m) # Initial set of parameters lambda_init &lt;- c(1, 3) gamma_init &lt;- matrix(c(0.8, 0.2, 0.2, 0.8), byrow = TRUE, nrow = m) # Turn them into working parameters parameters &lt;- pois.HMM.pn2pw(m, lambda_init, gamma_init) # Build the TMB object obj_tmb &lt;- MakeADFun(TMB_data, parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE) # Optimize mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn, gradient = obj_tmb$gr, hessian = obj_tmb$he) # Bootstrap procedure bootstrap_estimates &lt;- data.frame() DATA_SIZE &lt;- length(tinn_data) # Set how many parametric bootstrap samples we create BOOTSTRAP_SAMPLES &lt;- 10 # MLE ML_working_estimates &lt;- obj_tmb$env$last.par.best ML_natural_estimates &lt;- obj_tmb$report(ML_working_estimates) lambda &lt;- ML_natural_estimates$lambda gamma &lt;- ML_natural_estimates$gamma delta &lt;- ML_natural_estimates$delta PARAMS_NAMES &lt;- c(&quot;lambda&quot;, &quot;gamma&quot;, &quot;delta&quot;) for (idx_sample in 1:BOOTSTRAP_SAMPLES) { # Generate a sample based on mod, and ensure a HMM can be estimated on it # with testing_params as initial parameters temp &lt;- pois.HMM.generate_estimable_sample(ns = DATA_SIZE, mod = list(m = m, lambda = lambda, gamma = gamma), testing_params = list(m = m, lambda = lambda_init, gamma = gamma_init))$natural_parameters # The values from gamma are taken columnwise natural_parameters &lt;- unlist(temp[PARAMS_NAMES]) len_par &lt;- length(natural_parameters) bootstrap_estimates[idx_sample, 1:len_par] &lt;- natural_parameters } # Lower and upper (2.5% and 97.5%) bounds q &lt;- apply(bootstrap_estimates, 2, function(par_estimate) { quantile(par_estimate, probs = c(0.025, 0.975)) }) PARAMS_NAMES &lt;- paste0(rep(&quot;lambda&quot;, m), 1:m) # Get row and column indexes for gamma instead of the default # columnwise index: the default indexes are 1:m for the 1st column, # then (m + 1):(2 * m) for the 2nd, etc... for (gamma_idx in 1:m ^ 2) { row &lt;- (gamma_idx - 1) %% m + 1 col &lt;- (gamma_idx - 1) %/% m + 1 row_col_idx &lt;- c(row, col) PARAMS_NAMES &lt;- c(PARAMS_NAMES, paste0(&quot;gamma&quot;, paste0(row_col_idx, collapse = &quot;&quot;))) } PARAMS_NAMES &lt;- c(PARAMS_NAMES, paste0(rep(&quot;delta&quot;, m), 1:m)) bootstrap_CI &lt;- data.frame(&quot;Parameter&quot; = PARAMS_NAMES, &quot;Estimate&quot; = c(lambda, gamma, delta), &quot;Lower bound&quot; = q[1, ], &quot;Upper bound&quot; = q[2, ]) print(bootstrap_CI, row.names = FALSE) It should be noted that some bootstrap estimates can be very large or very small. One possible reason is that the randomly generated bootstrap sample might contain long chains of the same values, thus causing some probabilities in the TPM to be near the boundary 0 or 1. However, a large number of bootstrap samples lowers that risk since we automatically discard 5% of the most extreme values during the data generating process. ## Profiling the negative log-likelihood Profile References "]]
