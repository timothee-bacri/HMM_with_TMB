[["confidence-intervals.html", "Chapter 5 Confidence intervals", " Chapter 5 Confidence intervals From the parameters ML estimates, we generate new data and re-estimate the parameters times. From that list of new estimates we can get the 2.5th and 97.5th percentiles and get 95% confidence intervals for the parameters.\\ We show below how we get confidence intervals using bootstrap, based on the 2 state Poisson HMM estimates from above. TIMO: we had a 2-state above, right? First, we need a function to generate random data from a HMM. # Generate a random sample from a HMM pois.HMM.generate_sample &lt;- function(ns, mod) { mvect &lt;- 1:mod$m state &lt;- numeric(ns) state[1] &lt;- sample(mvect, 1, prob = mod$delta) for (i in 2:ns) { state[i] &lt;- sample(mvect, 1, prob = mod$gamma[state[i - 1], ]) } x &lt;- rpois(ns, lambda = mod$lambda[state]) return(list(data = x, state = state)) } Then, when the model is estimated each time, we dont impose an order for the states. This can lead to the label switching problem, where states arent ordered the same way in each model. To address this, we re-ordered the states by ascending Poisson means.\\ Sorting the means is pretty straightforward. Re-ordering the TPM is a little trickier. To do so, we took the permutations of the states given by the sorted Poisson means, and permuted each row index and column index to its new value.\\ The function we used is # Relabel states by increasing Poisson means pois.HMM.label.order &lt;- function(m, lambda, gamma, delta = NULL, lambda_std_error = NULL, gamma_std_error = NULL, delta_std_error = NULL) { # Get the indexes of the sorted states # according to ascending lambda sorted_lambda &lt;- sort(lambda, index.return = TRUE)$ix ordered_lambda &lt;- lambda[sorted_lambda] # Re-order the TPM according to the switched states # in the sorted lambda ordered_gamma &lt;- matrix(0, nrow = m, ncol = m) for (col in 1:m) { new_col &lt;- which(sorted_lambda == col) for (row in 1:m) { new_row &lt;- which(sorted_lambda == row) ordered_gamma[row, col] &lt;- gamma[new_row, new_col] } } # Re-order the stationary distribution if it is provided # Generate it otherwise if (is.null(delta)) { ordered_delta &lt;- stat.dist(ordered_gamma) } else { ordered_delta &lt;- delta[sorted_lambda] } # Re-order the standard errors ordered_lambda_std_error &lt;- lambda_std_error[sorted_lambda] ordered_gamma_std_error &lt;- gamma_std_error[sorted_lambda] ordered_delta_std_error &lt;- delta_std_error[sorted_lambda] result &lt;- list(lambda = ordered_lambda, gamma = ordered_gamma, delta = ordered_delta, lambda_std_error = ordered_lambda_std_error, gamma_std_error = ordered_gamma_std_error, delta_std_error = ordered_delta_std_error) # Remove the NULL elements result[sapply(result, is.null)] &lt;- NULL return(result) } Lets show an example to understand the process. For readability, the TPM is filled with row and column indexes instead of probabilities. lambda &lt;- c(30, 10, 20) gamma &lt;- matrix(c(11, 12, 13, 21, 22, 23, 31, 32, 33), byrow = TRUE, ncol = 3) pois.HMM.label.order(m = 3, lambda, gamma) ## $lambda ## [1] 10 20 30 ## ## $gamma ## [,1] [,2] [,3] ## [1,] 33 31 32 ## [2,] 13 11 12 ## [3,] 23 21 22 ## ## $delta ## [1] -0.032786885 0.016393443 -0.008196721 State 1 has been relabeled state 3, state 2 became state 1, and state 3 became state 2. Bootstrap code bootstrap_estimates &lt;- data.frame() DATA_SIZE &lt;- length(lamb_data) # Set how many parametric bootstrap samples we create BOOTSTRAP_SAMPLES &lt;- 1000 # ML parameters ML_working_estimates &lt;- obj_tmb$env$last.par.best ML_natural_estimates &lt;- obj_tmb$report(ML_working_estimates) gamma &lt;- ML_natural_estimates$gamma lambda &lt;- ML_natural_estimates$lambda delta &lt;- ML_natural_estimates$delta # Parameters for TMB cols &lt;- names(ML_working_estimates) tgamma &lt;- ML_working_estimates[cols == &quot;tgamma&quot;] tlambda &lt;- ML_working_estimates[cols == &quot;tlambda&quot;] ML_TMB_parameters &lt;- list(tlambda = tlambda, tgamma = tgamma) params_names &lt;- c(&quot;lambda&quot;, &quot;gamma&quot;, &quot;delta&quot;) # counter1 &lt;- counter2 &lt;- counter3 &lt;- counter_threshold_gamma &lt;- counter_threshold_lambda &lt;- 0 # conv1 &lt;- c() # conv2 &lt;- data.frame(message = character(), # min_g = numeric(), # max_g = numeric(), # min_l = numeric(), # max_l = numeric()) # # nll &lt;- c() # idx_sample &lt;- 1 # myAIC &lt;- myBIC &lt;- data.frame() for (idx_sample in 1:BOOTSTRAP_SAMPLES) { # Loop as long as there is an issue with nlminb repeat { #simulate the data bootstrap_data &lt;- pois.HMM.generate_sample(DATA_SIZE, list(m = m, lambda = lambda, gamma = gamma, delta = delta)) # # TRUE STATE FILTERING # tab &lt;- as.numeric(table(bootstrap_data$state)) / DATA_SIZE # if (any(tab &gt;= 0.99)) { # counter = counter + 1 # next # } # Parameters for TMB TMB_data_bootstrap &lt;- list(x = bootstrap_data$data, m = m) # Estimate the parameters obj &lt;- MakeADFun(TMB_data_bootstrap, ML_TMB_parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE) # Using tryCatch to break the loop if an error happens # interrupts the outer (for) loop, not the inner one (repeat) mod_bootstrap &lt;- NULL try(mod_bootstrap &lt;- nlminb(start = obj$par, objective = obj$fn, gradient = obj$gr, hessian = obj$he), silent = TRUE) # If nlminb doesn&#39;t reach any result, retry if (is.null(mod_bootstrap)) { # conv1 &lt;- c(conv1, mod_bootstrap$convergence) # counter1 &lt;- counter1 + 1 next } # If nlminb doesn&#39;t converge successfully, retry if (mod_bootstrap$convergence != 0) { # working_parameters &lt;- obj$env$last.par.best # natural_parameters &lt;- obj$report(working_parameters) # # g &lt;- natural_parameters$gamma # conv2[counter2 + 1, ] &lt;- cbind(mod_bootstrap$message, # round(min(g), 4), # round(max(g), 4), # round(min(l), 4), # round(max(l), 4)) # if(min(l) &lt;= 1e-3) { # print(min(l), digits = 10) # } # counter2 &lt;- counter2 + 1 next } # # SMOOTHING FILTERING # smoot &lt;- HMM.decode(obj)$ldecode # tab &lt;- as.numeric(table(smoot)) / DATA_SIZE # if (any(tab &gt;= 0.99)) { # counter = counter + 1 # next # } working_parameters &lt;- obj$env$last.par.best natural_parameters &lt;- obj$report(working_parameters) l &lt;- natural_parameters$lambda g &lt;- natural_parameters$gamma d &lt;- natural_parameters$delta # # VITERBI FILTERING # vit &lt;- pois.HMM.viterbi(bootstrap_data$sample, m, lambda = l, gamma = g, delta = d) # tab &lt;- as.numeric(table(vit)) / DATA_SIZE # if (any(tab &gt;= 0.99)) { # counter = counter + 1 # next # } # Label switching natural_parameters &lt;- pois.HMM.label.order(m = m, lambda = l, gamma = g, delta = d) # If some parameters are NA for some reason, retry if (anyNA(natural_parameters[params_names], recursive = TRUE)) { # counter3 &lt;- counter3 + 1 next } # if(max(g) &gt;= 0.999999) { # counter_threshold_gamma &lt;- counter_threshold_gamma + 1 # } # if(min(l) &lt;= 1e-3) { # counter_threshold_lambda &lt;- counter_threshold_lambda + 1 # } # # ATTEMPT ONE STATE, AIC AND BIC # lambda1state &lt;- mean(bootstrap_data$data) # gamma1state &lt;- matrix(1, nrow = 1, ncol = 1) # # parvect &lt;- pois.HMM.pn2pw(m = 1, lambda = lambda1state, gamma = gamma1state) # # mllk &lt;- pois.HMM.mllk(parvect, x_alias = bootstrap_data$data, m_alias = 1) # mllk &lt;- -log(prod(dpois(bootstrap_data$data, lambda1state))) # # np &lt;- length(unlist(parvect)) # np &lt;- 1 # myAIC[idx_sample, &quot;1state&quot;] &lt;- 2 * (mllk + np) # n &lt;- sum(!is.na(bootstrap_data$data)) # myBIC[idx_sample, &quot;1state&quot;] &lt;- 2 * mllk + np * log(n) # # 2 STATE AIC AND BIC # mllk &lt;- mod_bootstrap$objective # np &lt;- length(mod_bootstrap$par) # myAIC[idx_sample, &quot;2state&quot;] &lt;- 2 * (mllk + np) # myBIC[idx_sample, &quot;2state&quot;] &lt;- 2 * mllk + np * log(n) # LOGLIKELIHOOD VALUES # nll &lt;- c(nll, mod_bootstrap$objective) # nll &lt;- mod_bootstrap$objective # if (nll &lt;= 142.1408 | nll &gt;= 215.7791) { # counter = counter + 1 # next # } # if (nll &lt;= 146.7423 | nll &gt;= 208.3097) { # counter = counter + 1 # next # } # If everything went well, end the &quot;repeat&quot; loop break } # The values from gamma are taken columnwise natural_parameters &lt;- unlist(natural_parameters[params_names]) len_par &lt;- length(natural_parameters) bootstrap_estimates[idx_sample, 1:len_par] &lt;- natural_parameters } # Lower and upper (2.5% and 97.5%) bounds q &lt;- apply(bootstrap_estimates, 2, function(par_estimate) { quantile(par_estimate, probs = c(0.025, 0.975)) }) params_names &lt;- paste0(rep(&quot;lambda&quot;, m), 1:m) # Get row and column indexes for gamma instead of the default # columnwise index: the default indexes are 1:m for the 1st column, # then (m + 1):(2 * m) for the 2nd, etc... for (gamma_idx in 1:m ^ 2) { row &lt;- (gamma_idx - 1) %% m + 1 col &lt;- (gamma_idx - 1) %/% m + 1 row_col_idx &lt;- c(row, col) params_names &lt;- c(params_names, paste0(&quot;gamma&quot;, paste0(row_col_idx, collapse = &quot;&quot;))) } params_names &lt;- c(params_names, paste0(rep(&quot;delta&quot;, m), 1:m)) bootstrap_CI &lt;- data.frame(&quot;Parameter&quot; = params_names, &quot;Estimate&quot; = c(lambda, gamma, delta), &quot;Lower bound&quot; = q[1, ], &quot;Upper bound&quot; = q[2, ]) print(bootstrap_CI, row.names = FALSE) # print(counter) # plot(x = 1:BOOTSTRAP_SAMPLES, y = nll) # hist(nll) # apply(bootstrap_estimates[, 1:2], 2, function(est) { # hist(est, breaks = &quot;FD&quot;) # }) # print(paste(&quot;Lack of convergence/missing results&quot;, counter2, counter1, &quot;times&quot;)) # myAIC[, &quot;pref&quot;] &lt;- ifelse(myAIC[, &quot;1state&quot;] &lt;= myAIC[, &quot;2state&quot;], 1, 2) # myBIC[, &quot;pref&quot;] &lt;- ifelse(myBIC[, &quot;1state&quot;] &lt;= myBIC[, &quot;2state&quot;], 1, 2) # print(paste(&quot;AIC prefers 1 state&quot;, sum(myAIC[, &quot;pref&quot;] == 1), &quot;times, and 2 states&quot;, sum(myAIC[, &quot;pref&quot;] == 2), &quot;times&quot;)) # print(paste(&quot;BIC prefers 1 state&quot;, sum(myBIC[, &quot;pref&quot;] == 1), &quot;times, and 2 states&quot;, sum(myBIC[, &quot;pref&quot;] == 2), &quot;times&quot;)) # # # print(paste(counter1, &quot;don&#39;t reach any result&quot;)) # # print(paste(&quot;conv codes:&quot;, conv1)) # print(paste(counter2, &quot;don&#39;t converge successfully&quot;)) # # print(paste(&quot;conv codes:&quot;, conv2)) # print(paste(counter3, &quot;parameters are NA&quot;)) # # print(paste(&quot;gamma over the threshold&quot;, counter_threshold_gamma, &quot;times&quot;)) # print(paste(&quot;lambda under the threshold&quot;, counter_threshold_lambda, &quot;times&quot;)) It should be noted that some estimates can be very large or very small. One possible reason is that the randomly generated bootstrap sample might contain long chains of the same values, thus causing a probability in the TPM to be almost 0 or almost 1. However, a large number of bootstrap samples lowers that risk since we leave out 5% of the most extreme values when computing the 95% CI. "]]
