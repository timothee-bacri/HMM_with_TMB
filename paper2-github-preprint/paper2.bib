@article{ailliot,
  title = {{Stochastic weather generators: an overview of weather type models}},
  shorttitle = {{Stochastic weather generators}},
  author = {Ailliot, Pierre and Allard, Denis and Monbet, Val{\'e}rie and Naveau, Philippe},
  year = {2015},
  journal = {Journal de la soci\'et\'e fran\c{c}aise de statistique},
  volume = {156},
  number = {1},
  pages = {101--113},
  langid = {french}
}

@article{altmana,
  title = {{Application of hidden Markov models to multiple sclerosis lesion count data}},
  author = {Altman, Rachel MacKay and Petkau, A. John},
  year = {2005},
  journal = {Statistics in Medicine},
  volume = {24},
  number = {15},
  pages = {2335--2344},
  issn = {1097-0258},
  doi = {10.1002/sim.2108},
  abstract = {This paper is motivated by the work of Albert et al. who consider lesion count data observed on multiple sclerosis patients, and develop models for each patient's data individually. From a medical perspective, adequate models for such data are important both for describing the behaviour of lesions over time, and for designing efficient clinical trials. In this paper, we discuss some issues surrounding the hidden Markov model proposed by these authors. We describe an efficient estimation method and propose some extensions to the original model. Our examples illustrate the need for models which describe all patients' data simultaneously, while allowing for inter-patient heterogeneity. Copyright \textcopyright{} 2005 John Wiley \& Sons, Ltd.},
  langid = {french},
  keywords = {count data,Hidden Markov model,multiple sclerosis,time series},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.2108}
}

@article{bacri,
  title = {A Gentle Tutorial on Accelerated Parameter and Confidence Interval Estimation for Hidden {{Markov}} Models Using {{Template Model Builder}}},
  author = {Bacri, Timoth{\'e}e and Berentsen, Geir D. and Bulla, Jan and H{\o}lleland, Sondre},
  year = {2022},
  month = may,
  journal = {Biometrical Journal},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {1521-4036},
  doi = {10.1002/bimj.202100256},
  abstract = {A very common way to estimate the parameters of a hidden Markov model (HMM) is the relatively straightforward computation of maximum likelihood (ML) estimates. For this task, most users rely on user-...},
  langid = {english}
}

@article{baum,
  ids = {baumc},
  title = {Statistical {{Inference}} for {{Probabilistic Functions}} of {{Finite State Markov Chains}}},
  author = {Baum, Leonard E. and Petrie, Ted},
  year = {1966},
  month = dec,
  journal = {The Annals of Mathematical Statistics},
  volume = {37},
  number = {6},
  pages = {1554--1563},
  issn = {0003-4851},
  doi = {10.1214/aoms/1177699147},
  fjournal = {Annals of Mathematical Statistics},
  langid = {english},
  mrclass = {62.85},
  mrnumber = {MR0202264 (34 \#2137)},
  mrreviewer = {K. Dietz}
}

@article{bauma,
  title = {A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of {{Markov}} Chains},
  author = {Baum, Leonard E. and Petrie, Ted and Soules, George and Weiss, Norman},
  year = {1970},
  journal = {The Annals of Mathematical Statistics},
  volume = {41},
  number = {1},
  pages = {164--171},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851},
  fjournal = {Annals of Mathematical Statistics},
  mrclass = {60.65},
  mrnumber = {MR0287613 (44 \#4816)},
  mrreviewer = {Z. \v{S}id\'ak}
}

@article{bhatia,
  title = {A {{Better Bound}} on the {{Variance}}},
  author = {Bhatia, Rajendra and Davis, Chandler},
  year = {2000},
  month = apr,
  journal = {The American Mathematical Monthly},
  volume = {107},
  number = {4},
  pages = {353--357},
  publisher = {{Taylor \& Francis}},
  issn = {0002-9890},
  doi = {10.1080/00029890.2000.12005203},
  keywords = {Bhatia-Davis,bound,inequality,upper,variance},
  annotation = {\_eprint: https://doi.org/10.1080/00029890.2000.12005203}
}

@book{bohning,
  title = {Computer-Assisted Analysis of Mixtures and Applications: Meta-Analysis, Disease Mapping and Others},
  author = {B{\"o}hning, Dankmar},
  year = {1999},
  volume = {81},
  publisher = {{CRC press}}
}

@article{broyden,
  title = {The {{Convergence}} of a {{Class}} of {{Double-rank Minimization Algorithms}} 1. {{General Considerations}}},
  author = {Broyden, C. G.},
  year = {1970},
  month = mar,
  journal = {IMA Journal of Applied Mathematics},
  volume = {6},
  number = {1},
  pages = {76--90},
  issn = {0272-4960},
  doi = {10.1093/imamat/6.1.76},
  abstract = {This paper presents a more detailed analysis of a class of minimization algorithms, which includes as a special case the DFP (Davidon-Fletcher-Powell) method, than has previously appeared. Only quadratic functions are considered but particular attention is paid to the magnitude of successive errors and their dependence upon the initial matrix. On the basis of this a possible explanation of some of the observed characteristics of the class is tentatively suggested.},
  keywords = {bfgs}
}

@article{bulla,
  ids = {bullaf},
  title = {Computational Issues in Parameter Estimation for Stationary Hidden {{Markov}} Models},
  author = {Bulla, Jan and Berzel, Andreas},
  year = {2008},
  month = jan,
  journal = {Computational Statistics},
  volume = {23},
  number = {1},
  pages = {1--18},
  issn = {1613-9658},
  doi = {10.1007/s00180-007-0063-y},
  abstract = {The parameters of a hidden Markov model (HMM) can be estimated by numerical maximization of the log-likelihood function or, more popularly, using the expectation\textendash maximization (EM) algorithm. In its standard implementation the latter is unsuitable for fitting stationary hidden Markov models (HMMs). We show how it can be modified to achieve this. We propose a hybrid algorithm that is designed to combine the advantageous features of the two algorithms and compare the performance of the three algorithms using simulated data from a designed experiment, and a real data set. The properties investigated are speed of convergence, stability, dependence on initial values, different parameterizations. We also describe the results of an experiment to assess the true coverage probability of bootstrap-based confidence intervals for the parameters.},
  fjournal = {Computational Statistics},
  keywords = {computation,hidden Markov model,issues}
}

@article{bullag,
  title = {Hidden {{Markov}} Models with t Components. {{Increased}} Persistence and Other Aspects},
  author = {Bulla, Jan},
  year = {2011},
  month = mar,
  journal = {Quantitative Finance},
  volume = {11},
  number = {3},
  pages = {459--475},
  publisher = {{Routledge}},
  issn = {1469-7688},
  doi = {10.1080/14697681003685563},
  abstract = {Hidden Markov models have been applied in many different fields, including econometrics and finance. However, the lion's share of the investigated models concerns Markovian mixtures of Gaussian distributions. We present an extension to conditional t-distributions, including models with unequal distribution types in different states. It is shown that the extended models, on the one hand, reproduce various stylized facts of daily returns better than the common Gaussian model. On the other hand, robustness to outliers and persistence of the visited states increases significantly.},
  fjournal = {Quantitave Finance},
  keywords = {Daily returns,Hidden Markov model,Markov-switching model,State persistence,t-distribution},
  annotation = {\_eprint: https://doi.org/10.1080/14697681003685563}
}

@article{byrd,
  title = {A {{Limited Memory Algorithm}} for {{Bound Constrained Optimization}}},
  author = {Byrd, Richard H. and Lu, Peihuang and Nocedal, Jorge and Zhu, Ciyou},
  year = {1995},
  month = sep,
  journal = {SIAM Journal on Scientific Computing},
  volume = {16},
  number = {5},
  pages = {1190--1208},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {1064-8275},
  doi = {10.1137/0916069},
  abstract = {An algorithm for solving large nonlinear optimization problems with simple bounds is described. It is based on the gradient projection method and uses a limited memory BFGS matrix to approximate the Hessian of the objective function. It is shown how to take advantage of the form of the limited memory approximation to implement the algorithm efficiently. The results of numerical tests on a set of large problems are reported.},
  keywords = {49,65,bound constrained optimization,L-BFGS-B,large-scale optimization,limited memory method,nonlinear optimization,quasi-Newton method}
}

@book{cappe,
  ids = {cappea},
  title = {Inference in {{Hidden Markov Models}}},
  author = {Capp{\'e}, Olivier and Moulines, Eric and Ryden, Tobias},
  year = {2006},
  month = apr,
  publisher = {{Springer Science \& Business Media}},
  abstract = {Hidden Markov models have become a widely used class of statistical models with applications in diverse areas such as communications engineering, bioinformatics, finance and many more. This book is a comprehensive treatment of inference for hidden Markov models, including both algorithms and statistical theory. Topics range from filtering and smoothing of the hidden Markov chain to parameter estimation, Bayesian methods and estimation of the number of states. In a unified way the book covers both models with finite state spaces, which allow for exact algorithms for filtering, estimation etc. and models with continuous state spaces (also called state-space models) requiring approximate simulation-based algorithms that are also described in detail. Simulation in hidden Markov models is addressed in five different chapters that cover both Markov chain Monte Carlo and sequential Monte Carlo approaches. Many examples illustrate the algorithms and theory. The book also carefully treats Gaussian linear state-space models and their extensions and it contains a chapter on general Markov chain theory and probabilistic aspects of hidden Markov models. This volume will suit anybody with an interest in inference for stochastic processes, and it will be useful for researchers and practitioners in areas such as statistics, signal processing, communications engineering, control theory, econometrics, finance and more. The algorithmic parts of the book do not require an advanced mathematical background, while the more theoretical parts require knowledge of probability theory at the measure-theoretical level. From the reviews: "By providing an overall survey of results obtained so far in a very readable manner, and also presenting some new ideas, this well-written book will appeal to academic researchers in the field of HMMs, with PhD students working on related topics included. It will also appeal to practitioners and researchers from other fields by guiding them through the computational steps needed for making inference HMMs and/or by providing them with the relevant underlying statistical theory. In the reviewer's opinion this book will shortly become a reference work in its field." MathSciNet "This monograph is a valuable resource. It provides a good literature review, an excellent account of the state of the art research on the necessary theory and algorithms, and ample illustrations of numerous applications of HMM. It goes much beyond the earlier resources on HMM...I anticipate this work to serve well many Technometrics readers in the coming years." Haikady N. Nagaraja for Technometrics, November 2006},
  googlebooks = {4d\_oEYn8Fl0C},
  isbn = {978-0-387-28982-3},
  langid = {english},
  keywords = {Business \& Economics / Statistics,Computers / Computer Simulation,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes,Technology \& Engineering / Electronics / General,Technology \& Engineering / Imaging Systems}
}

@article{dempster,
  ids = {dempstera},
  title = {Maximum {{Likelihood}} from {{Incomplete Data Via}} the {{EM Algorithm}}},
  author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
  year = {1977},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {39},
  number = {1},
  pages = {1--22},
  issn = {2517-6161},
  doi = {10.1111/j.2517-6161.1977.tb01600.x},
  abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
  copyright = {\textcopyright{} 1977 The Authors},
  fjournal = {Journal of the Royal Statistical Society. Series B. Methodological},
  langid = {english},
  mrclass = {62F10},
  mrnumber = {MR0501537 (58 \#18858)},
  mrreviewer = {Rolf Sundberg},
  keywords = {em algorithm,incomplete data,maximum likelihood,posterior mode},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1977.tb01600.x}
}

@article{dennis,
  ids = {dennisjr.},
  title = {Quasi-{{Newton Methods}}, {{Motivation}} and {{Theory}}},
  author = {Dennis, John E., Jr. and Mor{\'e}, Jorge J.},
  year = {1977},
  month = jan,
  journal = {SIAM Review},
  volume = {19},
  number = {1},
  pages = {46--89},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/1019005},
  abstract = {This paper is an attempt to motivate and justify quasi-Newton methods as useful modifications of Newton's method for general and gradient nonlinear systems of equations. References are given to ample numerical justification; here we give an overview of many of the important theoretical results and each is accompanied by sufficient discussion to make the results and hence the methods plausible.},
  fjournal = {SIAM Review},
  mrclass = {65HXX (49D10 65K05)},
  mrnumber = {MR0445812 (56 \#4146)},
  mrreviewer = {W. Niethammer},
  keywords = {2008,jan,nlminb}
}

@book{dennisa,
  title = {Numerical {{Methods}} for {{Unconstrained Optimization}} and {{Nonlinear Equations}}},
  author = {Dennis, J. E. and Schnabel, Robert B.},
  year = {1996},
  month = jan,
  series = {Classics in {{Applied Mathematics}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611971200},
  abstract = {We are delighted that SIAM is republishing our original 1983 book after what many in the optimization field have regarded as ``premature termination'' by the previous publisher. At 12 years of age, the book may be a little young to be a ``classic,'' but since its publication it has been well received in the numerical computation community. We are very glad that it will continue to be available for use in teaching, research, and applications. We set out to write this book in the late 1970s because we felt that the basic techniques for solving small to medium-sized nonlinear equations and unconstrained optimization problems had matured and converged to the point where they would remain relatively stable. Fortunately, the intervening years have confirmed this belief. The material that constitutes most of this book\textemdash the discussion of Newton-based methods, globally convergent line search and trust region methods, and secant (quasi-Newton) methods for nonlinear equations, unconstrained optimization, and nonlinear least squares\textemdash continues to represent the basis for algorithms and analysis in this field. On the teaching side, a course centered around Chapters 4 to 9 forms a basic, in-depth introduction to the solution of nonlinear equations and unconstrained optimization problems. For researchers or users of optimization software, these chapters give the foundations of methods and software for solving small to medium-sized problems of these types.},
  isbn = {978-0-89871-364-0},
  keywords = {finite-difference,linear search method,nlm,nonlinear least squares,trust region,unconstrained minimization}
}

@article{dunmur,
  ids = {dunmura},
  title = {The Influence of Initial Conditions on Maximum Likelihood Estimation of the Parameters of a Binary Hidden {{Markov}} Model},
  author = {Dunmur, A. P and Titterington, D. M},
  year = {1998},
  month = sep,
  journal = {Statistics \& Probability Letters},
  volume = {40},
  number = {1},
  pages = {67--73},
  issn = {0167-7152},
  doi = {10.1016/S0167-7152(98)00100-X},
  abstract = {The Baum-Welch (EM) algorithm is a familiar tool for calculation of the maximum likelihood estimate of the parameters in hidden Markov chain models. For the particular case of a binary Markov chain corrupted by binary channel noise a detailed study is carried out of the influence that the initial conditions impose on the results produced by the algorithm.},
  coden = {SPLTDC},
  fjournal = {Statistics \& Probability Letters},
  langid = {english},
  mrclass = {62M05},
  mrnumber = {MR1650524},
  keywords = {Baum-Welch algorithm,Binary Markov chain,EM algorithm,Initial conditions}
}

@book{durbin,
  ids = {durbina},
  title = {Biological {{Sequence Analysis}}: {{Probabilistic Models}} of {{Proteins}} and {{Nucleic Acids}}},
  shorttitle = {Biological {{Sequence Analysis}}},
  author = {Durbin, Richard},
  year = {1998},
  month = apr,
  publisher = {{Cambridge University Press}},
  abstract = {Probabilistic models are becoming increasingly important in analysing the huge amount of data being produced by large-scale DNA-sequencing efforts such as the Human Genome Project. For example, hidden Markov models are used for analysing biological sequences, linguistic-grammar-based probabilistic models for identifying RNA secondary structure, and probabilistic evolutionary models for inferring phylogenies of sequences from different organisms. This book gives a unified, up-to-date and self-contained account, with a Bayesian slant, of such methods, and more generally to probabilistic methods of sequence analysis. Written by an interdisciplinary team of authors, it aims to be accessible to molecular biologists, computer scientists, and mathematicians with no formal knowledge of the other fields, and at the same time present the state-of-the-art in this new and highly important field.},
  isbn = {978-0-521-62971-3},
  langid = {english}
}

@article{eddy,
  title = {Profile Hidden {{Markov}} Models.},
  author = {Eddy, S R},
  year = {1998},
  month = jan,
  journal = {Bioinformatics},
  volume = {14},
  number = {9},
  pages = {755--763},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/14.9.755},
  abstract = {The recent literature on profile hidden Markov model (profile HMM) methods and software is reviewed. Profile HMMs turn a multiple sequence alignment into a position-specific scoring system suitable for searching databases for remotely homologous sequences. Profile HMM analyses complement standard pairwise comparison methods for large-scale sequence analysis. Several software implementations and two large libraries of profile HMMs of common protein domains are available. HMM methods performed comparably to threading methods in the CASP2 structure prediction exercise.}
}

@book{efron,
  title = {An Introduction to the Bootstrap},
  author = {Efron, Bradley and Tibshirani, Robert J},
  year = {1993},
  publisher = {{Chapman \& Hall}},
  address = {{New York, N.Y.; London}},
  isbn = {978-0-412-04231-7},
  langid = {english},
  annotation = {OCLC: 797437299}
}

@book{feller,
  title = {An Introduction to Probability Theory and Its Applications},
  author = {Feller, William},
  year = {1968},
  publisher = {{Wiley}},
  abstract = {Major changes in this edition include the substitution of probabilistic arguments for combinatorial artifices, and the addition of new sections on branching processes, Markov chains, and the De Moivre-Laplace theorem.},
  googlebooks = {mfRQAAAAMAAJ},
  isbn = {978-0-471-25708-0},
  langid = {english},
  keywords = {aperiodic Markov chain,Business \& Economics / Statistics,Fiction / General,Mathematics / General,Mathematics / Probability \& Statistics / General}
}

@book{fletcher,
  title = {Practical {{Methods}} of {{Optimization}}},
  author = {Fletcher, R.},
  year = {2013},
  month = jun,
  publisher = {{John Wiley \& Sons}},
  abstract = {Fully describes optimization methods that are currently most valuable in solving real-life problems. Since optimization has applications in almost every branch of science and technology, the text emphasizes their practical aspects in conjunction with the heuristics useful in making them perform more reliably and efficiently. To this end, it presents comparative numerical studies to give readers a feel for possibile applications and to illustrate the problems in assessing evidence. Also provides theoretical background which provides insights into how methods are derived. This edition offers revised coverage of basic theory and standard techniques, with updated discussions of line search methods, Newton and quasi-Newton methods, and conjugate direction methods, as well as a comprehensive treatment of restricted step or trust region methods not commonly found in the literature. Also includes recent developments in hybrid methods for nonlinear least squares; an extended discussion of linear programming, with new methods for stable updating of LU factors; and a completely new section on network programming. Chapters include computer subroutines, worked examples, and study questions.},
  googlebooks = {\_WuAvIx0EE4C},
  isbn = {978-1-118-72318-0},
  langid = {english},
  keywords = {CG,Mathematics / Discrete Mathematics,Mathematics / General}
}

@article{fletchera,
  title = {A New Approach to Variable Metric Algorithms},
  author = {Fletcher, R.},
  year = {1970},
  month = jan,
  journal = {The Computer Journal},
  volume = {13},
  number = {3},
  pages = {317--322},
  issn = {0010-4620},
  doi = {10.1093/comjnl/13.3.317},
  abstract = {An approach to variable metric algorithms has been investigated in which the linear search sub-problem no longer becomes necessary. The property of quadratic termination has been replaced by one of monotonic convergence of the eigenvalues of the approximating matrix to the inverse hessian. A convex class of updating formulae which possess this property has been established, and a strategy has been indicated for choosing a member of the class so as to keep the approximation away from both singularity and unboundedness. A FORTRAN program has been tested extensively with encouraging results.},
  keywords = {bfgs}
}

@article{fredkin,
  title = {Bayesian {{Restoration}} of {{Single-Channel Patch Clamp Recordings}}},
  author = {Fredkin, Donald R. and Rice, John A.},
  year = {1992},
  journal = {Biometrics},
  volume = {48},
  number = {2},
  pages = {427--448},
  publisher = {{[Wiley, International Biometric Society]}},
  issn = {0006341X, 15410420},
  doi = {10.2307/2532301},
  abstract = {[The technique of patch clamp recording makes possible the measurement of current flowing through a single ion channel in a cell membrane. Examination of such recordings suggests that the current is quantal in nature, alternating in a seemingly random manner between "on" and "off," but the recordings are corrupted by noise from a variety of sources. In this paper we propose and illustrate methods for restoring the underlying quantal signal from such noisy measurements. The methods use a Markov chain prior distribution for the underlying quantal process and base the restoration on the resulting posterior distribution.]},
  keywords = {Speech recognition}
}

@article{gay,
  title = {Usage Summary for Selected Optimization Routines},
  author = {Gay, David M},
  year = {1990},
  journal = {Computing science technical report},
  volume = {153},
  pages = {1--21},
  keywords = {nlminb}
}

@article{goldfarb,
  title = {A Family of Variable-Metric Methods Derived by Variational Means},
  author = {Goldfarb, Donald},
  year = {1970},
  journal = {Mathematics of Computation},
  volume = {24},
  number = {109},
  pages = {23--26},
  issn = {0025-5718, 1088-6842},
  doi = {10.1090/S0025-5718-1970-0258249-6},
  abstract = {A new rank-two variable-metric method is derived using Greenstadt's variational approach [Math. Comp., this issue]. Like the Davidon-Fletcher-Powell (DFP) variable-metric method, the new method preserves the positive-definiteness of the approximating matrix. Together with Greenstadt's method, the new method gives rise to a one-parameter family of variable-metric methods that includes the DFP and rank-one methods as special cases. It is equivalent to Broyden's one-parameter family [Math. Comp., v. 21, 1967, pp. 368\textendash 381]. Choices for the inverse of the weighting matrix in the variational approach are given that lead to the derivation of the DFP and rank-one methods directly.},
  langid = {english},
  keywords = {bfgs,Davidon method,rank-one formulas,Unconstrained optimization,variable-metric,variational methods}
}

@book{grimmett,
  title = {Probability and {{Random Processes}}},
  author = {Grimmett, Geoffrey R. and Stirzaker, David R.},
  year = {2001},
  month = may,
  edition = {Third},
  publisher = {{Oxford University Press}},
  address = {{New York}},
  abstract = {The third edition of this successful text gives a rigorous introduction to probability theory and the discussion of the most important random processes in some depth. It includes various topics which are suitable for undergraduate courses, but are not routinely taught. It is suitable to the beginner, and provides a taste and encouragement for more advanced work. There are four main aims: 1) to provide a thorough but straightforward account of basic probability, giving the reader a natural feel for the subject unburdened by oppressive technicalities, 2) to discuss important random processes in depth with many examples. 3) to cover a range of important but less routine topics, 4) to impart to the beginner the flavour of more advanced work. The books begins with basic ideas common to many undergraduate courses in mathematics, statistics and the sciences; in concludes with topics usually found at graduate level. The ordering and numbering of material in this third edition has been mostly preserved from the second. Minor alterations and additions have been added for clearer exposition. Highlights include new sections on sampling and Markov chain Monte Carlo, geometric probability, coupling and Poisson approximation, large deviations, spatial Poisson processes, renewal-reward, queueing networks, stochastic calculus, It\^o's formula and option pricing in the Black-Scholes model for financial markets. In addition there are many (nearly 400) new exercises and problems that are entertaining and instructive; their solutions can be found in the companion volume 'One Thousand Exercises in Probability', (OUP 2001).},
  isbn = {978-0-19-857222-0},
  langid = {english},
  mrclass = {60-01},
  mrnumber = {MR2059709 (2004m:60002)},
  keywords = {irreducibile Markov chain,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes},
  annotation = {ISBN-10 ‏ : ‎ 0198572220 ISBN-13 ‏ : ‎ 978-0198572220}
}

@article{guidolina,
  title = {Economic {{Implications}} of {{Bull}} and {{Bear Regimes}} in {{UK Stock}} and {{Bond Returns}}},
  author = {Guidolin, Massimo and Timmermann, Allan},
  year = {2005},
  month = jan,
  journal = {The Economic Journal},
  volume = {115},
  number = {500},
  pages = {111--143},
  issn = {0013-0133},
  doi = {10.1111/j.1468-0297.2004.00962.x},
  abstract = {This paper presents evidence of persistent `bull' and `bear' regimes in UK stock and bond returns and considers their economic implications from the perspective of an investor's portfolio allocation. We find that the perceived state probability has a large effect on the optimal asset allocation, particularly at short investment horizons. If ignored, the presence of such regimes gives rise to substantial welfare costs. Parameter estimation uncertainty, while clearly important, does not overturn the conclusion that predictability in the return distribution linked to the presence of bull and bear states has a significant effect on investors' strategic asset allocation.}
}

@article{hamilton,
  ids = {hamiltona},
  title = {A {{New Approach}} to the {{Economic Analysis}} of {{Nonstationary Time Series}} and the {{Business Cycle}}},
  author = {Hamilton, James D.},
  year = {1989},
  journal = {Econometrica},
  volume = {57},
  number = {2},
  pages = {357--384},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {0012-9682},
  doi = {10.2307/1912559},
  abstract = {This paper proposes a very tractable approach to modeling changes in regime. The parameters of an autoregression are viewed as the outcome of a discrete-state Markov process. For example, the mean growth rate of a nonstationary series may be subject to occasional, discrete shifts. The econometrician is presumed not to observe these shifts directly, but instead must draw probabilistic inference about whether and when they may have occurred based on the observed behavior of the series. The paper presents an algorithm for drawing such estimation of population parameters by the method of maximum likelihood and provides the foundation for forecasting future values of the series. An empirical application of this technique to postwar U.S. real GNP suggests that the periodic shift from a positive growth rate to a negative growth rate is a recurrent feature of the U.S. business cycle, and indeed could be used as an objective criterion for defining and measuring economic recessions. The estimated parameter values suggest that a typical economic recession is associated with a 3\% permanent drop in the level of GNP.},
  coden = {ECMTA7},
  fjournal = {Econometrica. Journal of the Econometric Society},
  mrclass = {90A20 (62M10)},
  mrnumber = {MR996941}
}

@article{hardle,
  title = {Bootstrap {{Methods}} for {{Time Series}}},
  author = {H{\"a}rdle, Wolfgang and Horowitz, Joel and Kreiss, Jens-Peter},
  year = {2003},
  journal = {International Statistical Review},
  volume = {71},
  number = {2},
  pages = {435--459},
  issn = {1751-5823},
  doi = {10.1111/j.1751-5823.2003.tb00485.x},
  abstract = {The bootstrap is a method for estimating the distribution of an estimator or test statistic by resampling one's data or a model estimated from the data. The methods that are available for implementing the bootstrap and the accuracy of bootstrap estimates depend on whether the data are an independent random sample or a time series. This paper is concerned with the application of the bootstrap to time-series data when one does not have a finite-dimensional parametric model that reduces the data generation process to independent random sampling. We review the methods that have been proposed for implementing the bootstrap in this situation and discuss the accuracy of these methods relative to that of first-order asymptotic approximations. We argue that methods for implementing the bootstrap with time-series data are not as well understood as methods for data that are independent random samples. Although promising bootstrap methods for time series are available, there is a considerable need for further research in the application of the bootstrap to time series. We describe some of the important unsolved problems. R\'esum\'e Le bootstrap est une m\'ethode pour estimer la distribution d'un estimateur en r\'e\'echantillonnant ses donn\'ees ou un mod\'ele estim\'e\`a partir des donn\'ees. Les m\'ethodes disponibles pour mettre en oeuvre le bootstrap et la pr\'ecision des estimateurs de bootstrap d\'ependent de ce que les donn\'ees proviennent d'un \'echantillon al\'eatoire ind\'ependant ou d'une s\'erie temporelle. Cet article concerne l'application du bootstrap aux donn\'ees des s\'eries temporelles quand on ne dispose pas de mod\'ele param\'etrique de dimension finie qui r\'eduise le processus de g\'en\'eration des donn\'ees \`a l'\'echantillonnage al\'eatoire ind\'ependent. Nous examinons les m\'ethodes qui ont \'et\'e propos\'ees pour mettre en oeuvre le bootstrap dans cette situation et discutons la precision de ces m\'ethodes comparativement \`a celle des approximations asymptotiques de premier ordre. Nous montrons que les m\'ethodes pour mettre en oeuvre le bootstrap avec les donn\'ees des s\'eries temporelles ne sont pas aussi bien comprises que les m\'ethodes pour les donn\'ees des \'echantillons al\'eatoires ind\'ependants. Bien que des m\'ethodes de bootstrap prometteuses pour les s\'eries temporelles soient disponibles, il y a un besoin consid\'erable de recherche suppl\'emental re dans leur application. Nous d\'ecrivons quelques probl\'emes importants non r\'esolus.},
  langid = {english},
  keywords = {Asymptotic approximation confidence interval,Block bootstrap,Resampling,Time series}
}

@article{hassan,
  title = {A Fusion Model of {{HMM}}, {{ANN}} and {{GA}} for Stock Market Forecasting},
  author = {Hassan, Md. Rafiul and Nath, Baikunth and Kirley, Michael},
  year = {2007},
  month = jul,
  journal = {Expert Systems with Applications},
  volume = {33},
  number = {1},
  pages = {171--180},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2006.04.007},
  abstract = {In this paper we propose and implement a fusion model by combining the Hidden Markov Model (HMM), Artificial Neural Networks (ANN) and Genetic Algorithms (GA) to forecast financial market behaviour. The developed tool can be used for in depth analysis of the stock market. Using ANN, the daily stock prices are transformed to independent sets of values that become input to HMM. We draw on GA to optimize the initial parameters of HMM. The trained HMM is used to identify and locate similar patterns in the historical data. The price differences between the matched days and the respective next day are calculated. Finally, a weighted average of the price differences of similar patterns is obtained to prepare a forecast for the required next day. Forecasts are obtained for a number of securities in the IT sector and are compared with a conventional forecast method.},
  langid = {english},
  keywords = {Artificial Neural Network,Baum-Welch algorithm,Forecasting,Genetic Algorithm,Hidden Markov Model,Stock market}
}

@article{hathaway,
  ids = {hathawaya},
  title = {A Constrained Em Algorithm for Univariate Normal Mixtures},
  author = {Hathaway, Richard J.},
  year = {1986},
  month = jan,
  journal = {Journal of Statistical Computation and Simulation},
  volume = {23},
  number = {3},
  pages = {211--230},
  publisher = {{Taylor \& Francis}},
  issn = {0094-9655},
  doi = {10.1080/00949658608810872},
  abstract = {The EM algorithm is widely used to calculate maximum-likelihood estimates corresponding to a mixture of normal distributions. The algorithm is altered using simple constraints which increase robustness against poor initial guesses while maintaining a low work requirement per iteration. The modified algorithm is described and the results of some numerical tests are given.},
  fjournal = {Journal of Statistical Computation and Simulation},
  keywords = {EM algorithm,maximum likelihood,normal mixture distributions},
  annotation = {\_eprint: https://doi.org/10.1080/00949658608810872}
}

@article{hooke,
  title = {`` {{Direct Search}}'' {{Solution}} of {{Numerical}} and {{Statistical Problems}}},
  author = {Hooke, Robert and Jeeves, T. A.},
  year = {1961},
  month = apr,
  journal = {Journal of the ACM},
  volume = {8},
  number = {2},
  pages = {212--229},
  issn = {0004-5411},
  doi = {10.1145/321062.321069},
  keywords = {hjn}
}

@article{juang,
  title = {Hidden {{Markov Models}} for {{Speech Recognition}}},
  author = {Juang, B. H. and Rabiner, L. R.},
  year = {1991},
  month = aug,
  journal = {Technometrics},
  volume = {33},
  number = {3},
  pages = {251--272},
  publisher = {{Taylor \& Francis}},
  issn = {0040-1706},
  doi = {10.1080/00401706.1991.10484833},
  keywords = {Hidden Markov model,Speech recognition}
}

@article{kato,
  title = {An {{HMM-based}} Segmentation Method for Traffic Monitoring Movies},
  author = {Kato, J. and Watanabe, T. and Joga, S. and Rittscher, J. and Blake, A.},
  year = {2002},
  month = sep,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {24},
  number = {9},
  pages = {1291--1296},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2002.1033221},
  abstract = {Shadows of moving objects often obstruct robust visual tracking. We propose an HMM-based segmentation method which classifies in real time each pixel or region into three categories: shadows, foreground, and background objects. In the case of traffic monitoring movies, the effectiveness of the proposed method has been proven through experimental results.},
  keywords = {Hidden Markov models,Monitoring,Motion pictures}
}

@article{kristensen,
  title = {{{TMB}}: {{Automatic}} Differentiation and Laplace Approximation},
  author = {Kristensen, Kasper and Nielsen, Anders and Berg, Casper and Skaug, Hans and Bell, Bradley},
  year = {2016},
  journal = {Journal of Statistical Software, Articles},
  volume = {70},
  number = {5},
  pages = {1--21},
  issn = {1548-7660},
  doi = {10.18637/jss.v070.i05}
}

@article{lange,
  ids = {langeb},
  title = {Efficient Computation of Lod Scores: Genotype Elimination, Genotype Redefinition, and Hybrid Maximum Likelihood Algorithms},
  shorttitle = {Efficient Computation of Lod Scores},
  author = {Lange, K. and Weeks, D. E.},
  year = {1989},
  journal = {Annals of Human Genetics},
  volume = {53},
  number = {1},
  pages = {67--83},
  issn = {1469-1809},
  doi = {10.1111/j.1469-1809.1989.tb01122.x},
  abstract = {Calculation of multilocus lod scores presents challenging problems in numerical analysis, combinatories, programming, and genetics. It is possible to accelerate these computations by exploiting the simple pedigree structure of a CEPH-type pedigree consisting of a nuclear family plus all four grandparents. Lathrop et al. (1986) have done' this by introducing likelihood factorization and transformation rules and Lander \& Green (1987) by the method of `hidden Markov chains'. The present paper explores an alternative approach based on genotype redefinition in the grandparents and systematic phase elimination in all pedigree members. All three approaches accelerate the computation of a single likelihood. Equally relevant to multilocus mapping are search strategies for finding the maximum likelihood estimates of recombination fractions. Hybrid algorithms that start with the EM algorithm and switch midway to quasi-Newton algorithms show promise. These issues are investigated in the context of a simulated 10 locus example. This same example allows us to illustrate a simple strategy for determining locus order.},
  coden = {ANHGAA},
  fjournal = {Annals of Human Genetics},
  langid = {english},
  mrclass = {92A10},
  mrnumber = {MR1002607 (90h:92014)},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-1809.1989.tb01122.x}
}

@article{leroux,
  title = {Maximum-{{Penalized-Likelihood Estimation}} for {{Independent}} and {{Markov- Dependent Mixture Models}}},
  author = {Leroux, Brian G. and Puterman, Martin L.},
  year = {1992},
  journal = {Biometrics},
  volume = {48},
  number = {2},
  pages = {545--558},
  issn = {0006-341X},
  doi = {10.2307/2532308},
  abstract = {This paper concerns the use and implementation of maximum-penalized-likelihood procedures for choosing the number of mixing components and estimating the parameters in independent and Markov-dependent mixture models. Computation of the estimates is achieved via algorithms for the automatic generation of starting values for the EM algorithm. Computation of the information matrix is also discussed. Poisson mixture models are applied to a sequence of counts of movements by a fetal lamb in utero obtained by ultrasound. The resulting estimates are seen to provide plausible mechanisms for the physiological process.},
  keywords = {Hidden Markov model,lamb}
}

@article{levenberg,
  title = {A Method for the Solution of Certain Non-Linear Problems in Least Squares},
  author = {Levenberg, Kenneth},
  year = {1944},
  journal = {Quarterly of Applied Mathematics},
  volume = {2},
  number = {2},
  pages = {164--168},
  issn = {0033-569X, 1552-4485},
  doi = {10.1090/qam/10666},
  abstract = {Advancing research. Creating connections.},
  langid = {english},
  keywords = {marqLevAlg}
}

@article{liporace,
  ids = {liporacea},
  title = {Maximum Likelihood Estimation for Multivariate Observations of {{Markov}} Sources},
  author = {Liporace, L.},
  year = {1982},
  month = sep,
  journal = {IEEE Transactions on Information Theory},
  volume = {28},
  number = {5},
  pages = {729--734},
  issn = {1557-9654},
  doi = {10.1109/TIT.1982.1056544},
  abstract = {Parameter estimation for multivariate functions of Markov chains, a class of versatile statistical models for vector random processes, is discussed. The model regards an ordered sequence of vectors as noisy multivariate observations of a Markov chain. Mixture distributions are a special case. The foundations of the theory presented here were established by Baum, Petrie, Soules, and Weiss. A powerful representation theorem by Fan is employed to generalize the analysis of Baum, \textbackslash em et al. to a larger class of distributions.},
  coden = {IETTAW},
  fjournal = {Institute of Electrical and Electronics Engineers. Transactions on Information Theory},
  mrclass = {62M09 (62M05)},
  mrnumber = {MR680137 (84c:62116)},
  mrreviewer = {Steffen Lauritzen},
  keywords = {Baum-Welch algorithm,hidden Markov model}
}

@article{lystig,
  title = {Exact {{Computation}} of the {{Observed Information Matrix}} for {{Hidden Markov Models}}},
  author = {Lystig, Theodore C and Hughes, James P},
  year = {2002},
  month = sep,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {11},
  number = {3},
  pages = {678--689},
  publisher = {{Taylor \& Francis}},
  issn = {1061-8600},
  doi = {10.1198/106186002402},
  abstract = {This article describes a new algorithm for exact computation of the observed information matrix in hidden Markov models that may be performed in a single pass through the data. The score vector and log-likelihood are computed in the same pass. The new algorithm is derived from the forward\textendash backward algorithm traditionally used to evaluate the likelihood in hidden Markov models. Our result is discussed in the context of previous approaches that have been used to obtain approximate standard errors of parameter estimates in these models. Implications for parameter estimation are also discussed. An application of the proposed methods to rainfall occurrence data is provided.}
}

@book{macdonald,
  title = {Hidden {{Markov}} and Other Models for Discrete-Valued Time Series},
  author = {MacDonald, Iain L. and Zucchini, Walter},
  year = {1997},
  series = {Monographs on Statistics and Applied Probability},
  volume = {70},
  publisher = {{Chapman \& Hall}},
  address = {{London}},
  abstract = {Discrete-valued time series are common in practice, but methods for their analysis are not well-known. In recent years, methods have been developed which are specifically designed for the analysis of discrete-valued time series. Hidden Markov and Other Models for Discrete-Valued Time Series introduces a new, versatile, and computationally tractable class of models, the "hidden Markov" models. It presents a detailed account of these models, then applies them to data from a wide range of diverse subject areas, including medicine, climatology, and geophysics. This book will be invaluable to researchers and postgraduate and senior undergraduate students in statistics. Researchers and applied statisticians who analyze time series data in medicine, animal behavior, hydrology, and sociology will also find this information useful.},
  isbn = {0-412-55850-5},
  mrclass = {62-02 (62M10)},
  mrnumber = {MR1692202 (2000g:62009)},
  mrreviewer = {Martin Jacobsen}
}

@article{maheua,
  title = {Identifying {{Bull}} and {{Bear Markets}} in {{Stock Returns}}},
  author = {{maheu}, John M. and McCurdy, Thomas H.},
  year = {2000},
  month = jan,
  journal = {Journal of Business \& Economic Statistics},
  volume = {18},
  number = {1},
  pages = {100--112},
  publisher = {{Taylor \& Francis}},
  issn = {0735-0015},
  doi = {10.1080/07350015.2000.10524851},
  abstract = {This article uses a Markov-switching model that incorporates duration dependence to capture nonlinear structure in both the conditional mean and the conditional variance of stock returns. The model sorts returns into a high-return stable state and a low-return volatile state. We label these as bull and bear markets, respectively. The filter identifies all major stock-market downturns in over 160 years of monthly data. Bull markets have a declining hazard functions although the best market gains come at the start of a bull market. Volatility increases with duration in bear markets. Allowing volatility to vary with duration captures volatility clustering.},
  keywords = {Duration dependence,Filter,Markov chain,Regime switching},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/07350015.2000.10524851}
}

@article{marquardt,
  title = {An {{Algorithm}} for {{Least-Squares Estimation}} of {{Nonlinear Parameters}}},
  author = {Marquardt, Donald W.},
  year = {1963},
  month = jun,
  journal = {Journal of the Society for Industrial and Applied Mathematics},
  volume = {11},
  number = {2},
  pages = {431--441},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0368-4245},
  doi = {10.1137/0111030},
  keywords = {marqLevAlg}
}

@article{mcclintock,
  title = {Uncovering Ecological State Dynamics with Hidden {{Markov}} Models},
  author = {McClintock, Brett T. and Langrock, Roland and Gimenez, Olivier and Cam, Emmanuelle and Borchers, David L. and Glennie, Richard and Patterson, Toby A.},
  year = {2020},
  journal = {Ecology Letters},
  volume = {23},
  number = {12},
  pages = {1878--1903},
  issn = {1461-0248},
  doi = {10.1111/ele.13610},
  abstract = {Ecological systems can often be characterised by changes among a finite set of underlying states pertaining to individuals, populations, communities or entire ecosystems through time. Owing to the inherent difficulty of empirical field studies, ecological state dynamics operating at any level of this hierarchy can often be unobservable or `hidden'. Ecologists must therefore often contend with incomplete or indirect observations that are somehow related to these underlying processes. By formally disentangling state and observation processes based on simple yet powerful mathematical properties that can be used to describe many ecological phenomena, hidden Markov models (HMMs) can facilitate inferences about complex system state dynamics that might otherwise be intractable. However, HMMs have only recently begun to gain traction within the broader ecological community. We provide a gentle introduction to HMMs, establish some common terminology, review the immense scope of HMMs for applied ecological research and provide a tutorial on implementation and interpretation. By illustrating how practitioners can use a simple conceptual template to customise HMMs for their specific systems of interest, revealing methodological links between existing applications, and highlighting some practical considerations and limitations of these approaches, our goal is to help establish HMMs as a fundamental inferential tool for ecologists.},
  copyright = {Published 2020. This article is a U.S. Government work and is in the public domain in the USA. Ecology Letters published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {Behavioural ecology,community ecology,ecosystem ecology,hidden Markov model,hierarchical model,movement ecology,observation error,population ecology,state-space model,time series},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ele.13610}
}

@article{mor,
  title = {A {{Systematic Review}} of {{Hidden Markov Models}} and {{Their Applications}}},
  author = {Mor, Bhavya and Garhwal, Sunita and Kumar, Ajay},
  year = {2021},
  month = may,
  journal = {Archives of Computational Methods in Engineering},
  volume = {28},
  number = {3},
  pages = {1429--1448},
  issn = {1886-1784},
  doi = {10.1007/s11831-020-09422-4},
  abstract = {The hidden Markov models are statistical models used in many real-world applications and communities. The use of hidden Markov models has become predominant in the last decades, as evidenced by a large number of published papers. In this survey, 146 papers (101 from Journals and 45 from Conferences/Workshops) from 93 Journals and 44 Conferences/Workshops are considered. The authors evaluate the literature based on hidden Markov model variants that have been applied to various application fields. The paper represents a short but comprehensive description of research on hidden Markov model and its variants for various applications. The paper shows the significant trends in the research on hidden Markov model variants and their applications.},
  langid = {english}
}

@article{nagy,
  title = {{\"Uber algebraische Gleichungen mit lauter reellen Wurzeln.}},
  author = {Nagy, Julius},
  year = {1918},
  journal = {Jahresbericht der Deutschen Mathematiker-Vereinigung},
  volume = {27},
  pages = {37--43},
  issn = {0012-0456; 1869-7135},
  langid = {und},
  keywords = {bound,inequality,lower,variance,von Szokefalvi Nagy}
}

@article{nelder,
  ids = {neldera},
  title = {A {{Simplex Method}} for {{Function Minimization}}},
  author = {Nelder, J. A. and Mead, R.},
  year = {1965},
  month = jan,
  journal = {The Computer Journal},
  volume = {7},
  number = {4},
  pages = {308--313},
  issn = {0010-4620},
  doi = {10.1093/comjnl/7.4.308},
  abstract = {A method is described for the minimization of a function of n variables, which depends on the comparison of function values at the (n + 1) vertices of a general simplex, followed by the replacement of the vertex with the highest value by another point. The simplex adapts itself to the local landscape, and contracts on to the final minimum. The method is shown to be effective and computationally compact. A procedure is given for the estimation of the Hessian matrix in the neighbourhood of the minimum, needed in statistical estimation problems.},
  fjournal = {Computer Journal}
}

@book{nielsen,
  title = {{{UCMINF}} - an Algorithm for Unconstrained, Nonlinear Optimization},
  author = {Nielsen, Hans Bruun},
  year = {2000},
  publisher = {{Informatics and Mathematical Modelling, Technical University of Denmark, DTU}},
  langid = {english},
  keywords = {optimizer,ucminf}
}

@inproceedings{oudelha,
  title = {{{HMM}} Parameters Estimation Using Hybrid {{Baum-Welch}} Genetic Algorithm},
  booktitle = {2010 {{International Symposium}} on {{Information Technology}}},
  author = {Oudelha, Mourad and Ainon, Raja N.},
  year = {2010},
  month = jun,
  volume = {2},
  pages = {542--545},
  issn = {2155-899X},
  doi = {10.1109/ITSIM.2010.5561388},
  abstract = {Automatic speech recognition (ASR) has been for a long time an active research area with a large variety of applications in human-computer intelligent interaction. The most used technique in this field is the Hidden Markov Models (HMMs) which is a statistical and extremely powerful method. The HMM model parameters are crucial information in HMM ASR process, and directly influence the recognition precision since they can make an excellent description of the speech signal. Therefore optimizing HMM parameters is still an important and challenging work in automatic speech recognition research area. Usually the Baum-Welch (B-W) Algorithm is used to calculate the HMM model parameters. However, the B-W algorithm uses an initial random guess of the parameters, therefore after convergence the output tends to be close to this initial value of the algorithm, which is not necessarily the global optimum of the model parameters. In this paper, a Genetic Algorithm (GA) combined with Baum-Welch (GA-BW) is proposed; the idea is to use GA exploration ability to obtain the optimal parameters within the solution space.},
  keywords = {Analytical models,Automatic speech recognition,Barium,Baum-Welch algorithm,Genetic Algorithm,Genetics,Hidden Markov Model,Hidden Markov models,HMM training,Speech recognition,Training}
}

@incollection{powell,
  title = {The {{NEWUOA}} Software for Unconstrained Optimization without Derivatives},
  booktitle = {Large-{{Scale Nonlinear Optimization}}},
  author = {Powell, M. J. D.},
  editor = {Di Pillo, G. and Roma, M.},
  year = {2006},
  series = {Nonconvex {{Optimization}} and {{Its Applications}}},
  pages = {255--297},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/0-387-30065-1_16},
  abstract = {The NEWUOA software seeks the least value of a function F(x), x{$\in$}Rn, when F(x) can be calculated for any vector of variables x. The algorithm is iterative, a quadratic model Q{$\approx$}F being required at the beginning of each iteration, which is used in a trust region procedure for adjusting the variables. When Q is revised, the new Q interpolates F at m points, the value m = 2n + 1 being recommended. The remaining freedom in the new Q is taken up by minimizing the Frobenius norm of the change to {$\nabla$}2Q. Only one interpolation point is altered on each iteration. Thus, except for occasional origin shifts, the amount of work per iteration is only of order (m+n)2, which allows n to be quite large. Many questions were addressed during the development of NEWUOA, for the achievement of good accuracy and robustness. They include the choice of the initial quadratic model, the need to maintain enough linear independence in the interpolation conditions in the presence of computer rounding errors, and the stability of the updating of certain matrices that allow the fast revision of Q. Details are given of the techniques that answer all the questions that occurred. The software was tried on several test problems. Numerical results for nine of them are reported and discussed, in order to demonstrate the performance of the software for up to 160 variables.},
  isbn = {978-0-387-30065-8},
  langid = {english},
  keywords = {direct search,optimizer,quadratic models,trust regions,unconstrained minimization,updating}
}

@article{probst,
  title = {Emotion Dynamics and Tinnitus: {{Daily}} Life Data from the ``{{TrackYourTinnitus}}'' Application},
  shorttitle = {Emotion Dynamics and Tinnitus},
  author = {Probst, Thomas and Pryss, R{\"u}diger and Langguth, Berthold and Schlee, Winfried},
  year = {2016},
  month = aug,
  journal = {Scientific Reports},
  volume = {6},
  number = {1},
  pages = {31166},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/srep31166},
  abstract = {It is well established that emotions influence tinnitus, but the role of emotion dynamics remains unclear. The present study investigated emotion dynamics in N\,=\,306 users of the ``TrackYourTinnitus'' application who completed the Mini-Tinnitus Questionnaire (Mini-TQ) at one assessment point and provided complete data on at least five assessment points for the following state variables: tinnitus loudness, tinnitus distress, arousal, valence. The repeated arousal and valence ratings were used for two operationalizations of emotion dynamics: intra-individual variability of affect intensity (pulse) as well as intra-individual variability of affect quality (spin). Pearson correlation coefficients showed that the Mini-TQ was positively correlated with pulse (r\,=\,0.19; p\,{$<$}\,0.05) as well as with spin (r\,=\,0.12; p\,{$<$}\,0.05). Multilevel models revealed the following results: increases in tinnitus loudness were more strongly associated with increases in tinnitus distress at higher levels of pulse as well as at higher levels of spin (both p\,{$<$}\,0.05), whereby increases in tinnitus loudness correlated even stronger with increases in tinnitus distress when both pulse as well as spin were high (p\,{$<$}\,0.05). Moreover, increases in spin were associated with a less favorable time course of tinnitus loudness (p\,{$<$}\,0.05). To conclude, equilibrating emotion dynamics might be a potential target in the prevention and treatment of tinnitus.},
  copyright = {2016 The Author(s)},
  langid = {english}
}

@article{probsta,
  title = {Does {{Tinnitus Depend}} on {{Time-of-Day}}? {{An Ecological Momentary Assessment Study}} with the ``{{TrackYourTinnitus}}'' {{Application}}},
  shorttitle = {Does {{Tinnitus Depend}} on {{Time-of-Day}}?},
  author = {Probst, Thomas and Pryss, R{\"u}diger C. and Langguth, Berthold and Rauschecker, Josef P. and Schobel, Johannes and Reichert, Manfred and Spiliopoulou, Myra and Schlee, Winfried and Zimmermann, Johannes},
  year = {2017},
  month = aug,
  journal = {Frontiers in Aging Neuroscience},
  volume = {9},
  pages = {253},
  issn = {1663-4365},
  doi = {10.3389/fnagi.2017.00253},
  abstract = {Only few previous studies used ecological momentary assessments to explore the time-of-day-dependence of tinnitus. The present study used data from the mobile application ``TrackYourTinnitus'' to explore whether tinnitus loudness and tinnitus distress fluctuate within a 24-h interval. Multilevel models were performed to account for the nested structure of assessments (level 1: 17,209 daily life assessments) nested within days (level 2: 3,570 days with at least three completed assessments), and days nested within participants (level 3: 350 participants). Results revealed a time-of-day-dependence of tinnitus. In particular, tinnitus was perceived as louder and more distressing during the night and early morning hours (from 12 a.m. to 8 a.m.) than during the upcoming day. Since previous studies suggested that stress (and stress-associated hormones) show a circadian rhythm and this might influence the time-of-day-dependence of tinnitus, we evaluated whether the described results change when statistically controlling for subjectively reported stress-levels. Correcting for subjective stress-levels, however, did not change the result that tinnitus (loudness and distress) was most severe at night and early morning. These results show that time-of-day contributes to the level of both tinnitus loudness and tinnitus distress. Possible implications of our results for the clinical management of tinnitus are that tailoring the timing of therapeutic interventions to the circadian rhythm of individual patients (chronotherapy) might be promising.},
  pmcid = {PMC5539131},
  pmid = {28824415}
}

@inproceedings{pryss,
  title = {Mobile {{Crowd Sensing Services}} for {{Tinnitus Assessment}}, {{Therapy}}, and {{Research}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Mobile Services}}},
  author = {Pryss, R{\"u}diger and Reichert, Manfred and Langguth, Berthold and Schlee, Winfried},
  year = {2015},
  month = jun,
  pages = {352--359},
  publisher = {{IEEE}},
  issn = {2329-6453},
  doi = {10.1109/MobServ.2015.55},
  abstract = {Tinnitus, the phantom sensation of sound, is a highly prevalent disorder that is difficult to treat, i.e., Available treatments are only effective for patient subgroups. Sufficiently large and qualitative longitudinal data sets, which aggregate the individuals' demographic and clinical characteristics, together with their response to specific therapeutic interventions, would therefore facilitate evidence-based treatment suggestions for individual patients. Currently, clinical trials are the standard instrument for realizing evidence-based medicine. However, the related information gathering is limited. For example, clinical trials try to reduce the complexity of the individual case by generating homogeneous groups to obtain significant results. From the latter, individual treatment decisions are inferred. A complementary approach would be to assess the effect of specific interventions in large samples considering the individual peculiarity of each subject. This allows providing individualized treatment decisions. Recently, mobile crowd sensing emerged as an approach for collecting large and ecological valid datasets at rather low costs. By providing mobile crowd sensing services to large numbers of patients, large datasets can be gathered cheaply on a daily basis. In the TrackYourTinnitus project, we implemented a mobile crowd sensing platform to reveal new medical aspects on tinnitus and its treatment. Additionally, we work on mobile services exploring approaches for understanding tinnitus and for improving its diagnostic and therapeutic management. We present the TrackYourTinnitus platform as well as its goals, architecture and preliminary results. Overall, the platform and its mobile services offer promising perspectives for tinnitus research and treatment.},
  keywords = {Androids,clinical trial,Clinical trials,evidence-based medicine,health care,Humanoid robots,Mobile communication,mobile computing,mobile crowd sensing,mobile crowdsensing services,Mobile handsets,mobile healthcare application,Operating systems,patient treatment,Sensors,sound phantom sensation,tinnitus,tinnitus assessment,tinnitus therapy,tinnitus variablity,TrackYourTinnitus platform}
}

@inproceedings{pryssa,
  title = {Mobile {{Crowd Sensing}} in {{Clinical}} and {{Psychological Trials}} \textendash{} {{A Case Study}}},
  booktitle = {2015 {{IEEE}} 28th {{International Symposium}} on {{Computer-Based Medical Systems}}},
  author = {Pryss, R. and Reichert, M. and Herrmann, J. and Langguth, B. and Schlee, W.},
  year = {2015},
  month = jun,
  pages = {23--24},
  publisher = {{IEEE}},
  issn = {2372-9198},
  doi = {10.1109/CBMS.2015.26},
  abstract = {Many highly prevalent diseases (e.g., tinnitus, migraine, chronic pain) are difficult to treat and universally effective treatments are missing. Available treatments are only effective in patient subgroups, i.e., medical doctors and patients have to figure out which therapy might be helpful in the patient's situation. Sufficiently large and qualitative longitudinal data sets, however, would be desirable to facilitate evidence-based treatment decisions for individual patients. On one hand, traditional sensing techniques (i.e., clinical trials) have many merits enabling evidence-based medicine. On the other, they have inherent limitations. First, clinical trials are very cost- and labour-intensive. Second, the traditional approach aims at reducing ecological heterogeneity to enable the investigation of homogeneous subsamples. Recently, a new paradigm emerged that offers promising perspectives for collecting large amounts of longitudinal patient data \textendash{} Mobile Crowd Sensing. By utilizing smart mobile devices of a large number of patients, health information can be gathered from large patient collections as well as at many different time points and in various real life environmental situations. In the Track Your Tinnitus project, we implemented such a mobile crowd sensing platform to reveal new medical aspects about tinnitus with a particular focus on the variability of tinnitus over time depending on the environmental situation. In this paper, the current project status as well as first lessons learned from running the mobile application for twelve months are presented. In turn, the lessons learned are discussed in the context of the new perspectives offered by mobile crowd sensing in the medical field.},
  keywords = {biomedical communication,biomedical measurement,clinical trial,clinical trials,Clinical trials,diseases,ecological heterogeneity,evidence-based medicine,evidence-based treatment decisions,health care,health information,homogeneous subsamples,large longitudinal data sets,medical computing,medical field,mobile application,Mobile communication,mobile computing,mobile crowd sensing,mobile crowd sensing platform,Mobile handsets,mobile healthcare application,patient subgroups,patient treatment,psychological trial,psychological trials,psychology,Psychology,qualitative longitudinal data sets,real life environmental situations,Sensors,smart mobile devices,smart phones,time 12 month,time points,tinnitus,tinnitus variablity,Track Your Tinnitus project,traditional sensing techniques,universally effective treatments}
}

@article{rabiner,
  ids = {rabinerb},
  title = {A Tutorial on Hidden {{Markov}} Models and Selected Applications in Speech Recognition},
  author = {Rabiner, L.R.},
  year = {1989},
  month = feb,
  journal = {Proceedings of the IEEE},
  volume = {77},
  number = {2},
  pages = {257--286},
  issn = {1558-2256},
  doi = {10.1109/5.18626},
  abstract = {This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described.{$<>$}},
  fjournal = {Institute of Electrical and Electronics Engineers. Transactions on Information Theory},
  keywords = {Baum-Welch algorithm,Hidden Markov models,Speech recognition,Tutorial}
}

@article{rabinera,
  title = {An Introduction to Hidden {{Markov}} Models},
  author = {Rabiner, L. and Juang, B.},
  year = {1986},
  month = jan,
  journal = {IEEE ASSP Magazine},
  volume = {3},
  number = {1},
  pages = {4--16},
  issn = {1558-1284},
  doi = {10.1109/MASSP.1986.1165342},
  abstract = {The basic theory of Markov chains has been known to mathematicians and engineers for close to 80 years, but it is only in the past decade that it has been applied explicitly to problems in speech processing. One of the major reasons why speech models, based on Markov chains, have not been developed until recently was the lack of a method for optimizing the parameters of the Markov model to match observed signal patterns. Such a method was proposed in the late 1960's and was immediately applied to speech processing in several research institutions. Continued refinements in the theory and implementation of Markov modelling techniques have greatly enhanced the method, leading to a wide range of applications of these models. It is the purpose of this tutorial paper to give an introduction to the theory of Markov models, and to illustrate how they have been applied to problems in speech recognition.},
  keywords = {Fluctuations,Hidden Markov models,Linear systems,Mathematical model,Optimization methods,Pattern matching,Speech processing,Speech recognition,Time varying systems}
}

@manual{rcoreteam,
  type = {Manual},
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2021},
  address = {{Vienna, Austria}},
  organization = {{R Foundation for Statistical Computing}}
}

@article{redner,
  ids = {rednera},
  title = {Mixture {{Densities}}, {{Maximum Likelihood}} and the {{EM Algorithm}}},
  author = {Redner, Richard A. and Walker, Homer F.},
  year = {1984},
  month = apr,
  journal = {SIAM Review},
  volume = {26},
  number = {2},
  pages = {195--239},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/1026034},
  abstract = {The problem of estimating the parameters which determine a mixture density has been the subject of a large, diverse body of literature spanning nearly ninety years. During the last two decades, the method of maximum likelihood has become the most widely followed approach to this problem, thanks primarily to the advent of high speed electronic computers. Here, we first offer a brief survey of the literature directed toward this problem and review maximum-likelihood estimation for it. We then turn to the subject of ultimate interest, which is a particular iterative procedure for numerically approximating maximum-likelihood estimates for mixture density problems. This procedure, known as the EM algorithm, is a specialization to the mixture density context of a general algorithm of the same name used to approximate maximum-likelihood estimates for incomplete data problems. We discuss the formulation and theoretical and practical properties of the EM algorithm for mixture densities, focussing in particular on mixtures of densities from exponential families.},
  coden = {SIREAD},
  fjournal = {SIAM Review. A Publication of the Society for Industrial and Applied Mathematics},
  mrclass = {62F10 (62-04)},
  mrnumber = {MR738930 (85j:62027)},
  mrreviewer = {Barry C. Arnold},
  keywords = {EM algorithm,exponential families,incomplete data,maximum likelihood,mixture densities}
}

@article{rydenb,
  ids = {rydend},
  title = {Stylized {{Facts}} of {{Daily Return Series}} and the {{Hidden Markov Model}}},
  author = {Ryd{\'e}n, Tobias and Ter{\"a}svirta, Timo and {\AA}sbrink, Stefan},
  year = {1998},
  journal = {Journal of Applied Econometrics},
  volume = {13},
  number = {3},
  pages = {217--244},
  publisher = {{John Wiley \& Sons Ltd}},
  address = {{W Sussex}},
  issn = {0883-7252},
  doi = {10.1002/(SICI)1099-1255(199805/06)13:3<217::AID-JAE476>3.0.CO;2-V},
  abstract = {In two recent papers, Granger and Ding (1995a, b) considered long return series that are first differences of logarithmed price series or price indices. They established a set of temporal and distributional properties for such series and suggested that the returns are well characterized by the double exponential distribution. The present paper shows that a mixture of normal variables with zero mean can generate series with most of the properties Granger and Ding singled out. In that case, the temporal higher-order dependence observed in return series may be described by a hidden Markov model. Such a model is estimated for ten subseries of the well-known S\&P 500 return series of about 17,000 daily observations. It reproduces the stylized facts of Granger and Ding quite well, but the parameter estimates of the model sometimes vary considerably from one subseries to the next. The implications of these results are discussed.},
  fjournal = {Journal of Applied Econometrics},
  langid = {english},
  keywords = {volatility},
  annotation = {WOS:000074333000001}
}

@article{schadt,
  title = {Computational {{Advances}} in {{Maximum Likelihood Methods}} for {{Molecular Phylogeny}}},
  author = {Schadt, Eric E. and Sinsheimer, Janet S. and Lange, Kenneth},
  year = {1998},
  month = jan,
  journal = {Genome Research},
  volume = {8},
  number = {3},
  pages = {222--233},
  publisher = {{Cold Spring Harbor Lab}},
  issn = {1088-9051, 1549-5469},
  doi = {10.1101/gr.8.3.222},
  abstract = {We have developed a generalization of Kimura's Markov chain model for base substitution at a single nucleotide site. This generalized model incorporates more flexible transition rates and consequently allows irreversible as well as reversible chains. Because the model embodies just the right amount of symmetry, it permits explicit calculation of finite-time transition probabilities and equilibrium distributions. The model also meshes well with maximum likelihood methods for phylogenetic analysis. Quick calculation of likelihoods and their derivatives can be carried out by adapting Baum's forward and backward algorithms from the theory of hidden Markov chains. Analysis of HIV sequence data illustrates the speed of the algorithms on trees with many contemporary taxa. Analysis of some of Lake's data on the origin of the eukaryotic nucleus contrasts the reversible and irreversible versions of the model.},
  langid = {english},
  pmid = {9521926}
}

@article{schnabel,
  ids = {schnabela},
  title = {A Modular System of Algorithms for Unconstrained Minimization},
  author = {Schnabel, Robert B. and Koonatz, John E. and Weiss, Barry E.},
  year = {1985},
  month = dec,
  journal = {ACM Transactions on Mathematical Software},
  volume = {11},
  number = {4},
  pages = {419--440},
  issn = {0098-3500},
  doi = {10.1145/6187.6192},
  abstract = {We describe a new package, UNCMIN, for finding a local minimizer of a real valued function of more than one variable. The novel feature of UNCMIN is that it is a modular system of algorithms, containing three different step selection strategies (line search, dogleg, and optimal step) that may be combined with either analytic or finite difference gradient evaluation and with either analytic, finite difference, or BFGS Hessian approximation. We present the results of a comparison of the three step selection strategies on the problems in More, Garbow, and Hillstrom in two separate cases: using finite difference gradients and Hessians, and using finite difference gradients with BFGS Hessian approximations. We also describe a second package, REVMIN, that uses optimization algorithms identical to UNCMIN but obtains values of user-supplied functions by reverse communication.},
  coden = {ACMSCU},
  fjournal = {Association for Computing Machinery. Transactions on Mathematical Software},
  mrclass = {65K10 (65-04 90-04 90C30)},
  mrnumber = {MR828567},
  keywords = {2008,jan,nlminb}
}

@misc{schwendinger,
  title = {{{CRAN Task View}}: {{Optimization}} and {{Mathematical Programming}}},
  shorttitle = {{{CRAN Task View}}},
  author = {Schwendinger, Florian and Borchers, Hans W.},
  year = {2022},
  month = dec,
  publisher = {{Comprehensive R Archive Network (CRAN)}},
  abstract = {This CRAN Task View contains a list of packages which offer facilities for solving optimization problems. Although every regression model in statistics solves an optimization problem, they are not part of this view. If you are looking for regression methods, the following views will also contain useful starting points: MachineLearning, Econometrics, Robust The focus of this task view is on Optimization Infrastructure Packages, General Purpose Continuous Solvers, Mathematical Programming Solvers, Specific Applications in Optimization, or Multi Objective Optimization.},
  howpublished = {https://CRAN.R-project.org/view=Optimization},
  keywords = {CRAN,Mathematical,Optimization,optimx,Programming}
}

@article{schwerta,
  title = {Why Does Stock Market Volatility Change over Time},
  author = {Schwert, G. William},
  year = {1989},
  journal = {Journal of Finance},
  volume = {44},
  number = {5},
  pages = {1115--1153},
  doi = {10.1111/j.1540-6261.1989.tb02647.x},
  fjournal = {Journal of Finance}
}

@article{shanno,
  title = {Conditioning of Quasi-{{Newton}} Methods for Function Minimization},
  author = {Shanno, D. F.},
  year = {1970},
  journal = {Mathematics of Computation},
  volume = {24},
  number = {111},
  pages = {647--656},
  issn = {0025-5718, 1088-6842},
  doi = {10.1090/S0025-5718-1970-0274029-X},
  abstract = {Quasi-Newton methods accelerate the steepest-descent technique for function minimization by using computational history to generate a sequence of approximations to the inverse of the Hessian matrix. This paper presents a class of approximating matrices as a function of a scalar parameter. The problem of optimal conditioning of these matrices under an appropriate norm as a function of the scalar parameter is investigated. A set of computational results verifies the superiority of the new methods arising from conditioning considerations to known methods.},
  langid = {english},
  keywords = {bfgs,conditioning of search methods,Function minimization,gradient search,Hessian matrix,inverse approximations,quasi-Newton methods,stability of search methods,steepest-descent methods,variable metric methods}
}

@article{stolcke,
  title = {Hidden {{Markov}} Model Induction by {{Bayesian}} Model Merging},
  author = {Stolcke, Andreas and Omohundro, Stephen},
  year = {1993},
  journal = {Advances in neural information processing systems},
  pages = {11--11},
  publisher = {{MORGAN KAUFMANN PUBLISHERS}}
}

@article{turner,
  title = {Direct Maximization of the Likelihood of a Hidden {{Markov}} Model},
  author = {Turner, Rolf},
  year = {2008},
  month = may,
  journal = {Computational Statistics \& Data Analysis},
  volume = {52},
  number = {9},
  pages = {4147--4160},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2008.01.029},
  abstract = {Ever since the introduction of hidden Markov models by Baum and his co-workers, the method of choice for fitting such models has been maximum likelihood via the EM algorithm. In recent years it has been noticed that the gradient and Hessian of the log likelihood of hidden Markov and related models may be calculated in parallel with a filtering process by which the likelihood may be calculated. Various authors have used, or suggested the use of, this idea in order to maximize the likelihood directly, without using the EM algorithm. In this paper we discuss an implementation of such an approach. We have found that a straightforward implementation of Newton's method sometimes works but is unreliable. A form of the Levenberg\textendash Marquardt algorithm appears to provide excellent reliability. Two rather complex examples are given for applying this algorithm to the fitting of hidden Markov models. In the first a better than 6-fold increase in speed over the EM algorithm was achieved. The second example turned out to be problematic (somewhat interestingly) in that the maximum likelihood estimator appears to be inconsistent. Whatever its merit, this estimator is calculated much faster by Levenberg\textendash Marquardt than by EM. We also compared the Levenberg\textendash Marquardt algorithm, applied to the first example, with a generic numerical maximization procedure. The Levenberg\textendash Marquardt algorithm appeared to perform almost three times better than the generic procedure, even when analytic derivatives were provided, and 19 times better when they were not provided.}
}

@article{varadhan,
  title = {{{BB}}: {{An R Package}} for {{Solving}} a {{Large System}} of {{Nonlinear Equations}} and for {{Optimizing}} a {{High-Dimensional Nonlinear Objective Function}}},
  shorttitle = {{{BB}}},
  author = {Varadhan, Ravi and Gilbert, Paul},
  year = {2010},
  journal = {Journal of Statistical Software},
  volume = {32},
  pages = {1--26},
  issn = {1548-7660},
  doi = {10.18637/jss.v032.i04},
  abstract = {We discuss R package BB, in particular, its capabilities for solving a nonlinear system of equations.  The function BBsolve in BB can be used for this purpose. We demonstrate the utility of these functions for solving: (a) large systems of nonlinear equations, (b) smooth, nonlinear estimating equations in statistical modeling, and  (c) non-smooth estimating equations arising in rank-based regression modeling of censored failure time data. The function BBoptim can be used to solve smooth, box-constrained optimization problems. A main strength of BB is that, due to its low memory and storage requirements, it is ideally suited for solving high-dimensional problems with thousands of variables.},
  copyright = {Copyright (c) 2008 Ravi Varadhan, Paul Gilbert},
  langid = {english},
  keywords = {optimizer,R}
}

@article{visser,
  title = {Confidence Intervals for Hidden {{Markov}} Model Parameters},
  author = {Visser, Ingmar and Raijmakers, Maartje E. J. and Molenaar, Peter C. M.},
  year = {2000},
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {53},
  number = {2},
  pages = {317--327},
  publisher = {{Blackwell Publishing Ltd}},
  issn = {2044-8317},
  doi = {10.1348/000711000159240},
  fjournal = {British Journal of Mathematical \& Statistical Psychology},
  keywords = {bootstrap,confidence intervals,finite differences approximation,invariance,profile likelihood}
}

@article{wu,
  ids = {wua},
  title = {On the {{Convergence Properties}} of the {{EM Algorithm}}},
  author = {Wu, C. F. Jeff},
  year = {1983},
  journal = {The Annals of Statistics},
  volume = {11},
  number = {1},
  pages = {95--103},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364},
  abstract = {Two convergence aspects of the EM algorithm are studied: (i) does the EM algorithm find a local maximum or a stationary value of the (incomplete-data) likelihood function? (ii) does the sequence of parameter estimates generated by EM converge? Several convergence results are obtained under conditions that are applicable to many practical situations. Two useful special cases are: (a) if the unobserved complete-data specification can be described by a curved exponential family with compact parameter space, all the limit points of any EM sequence are stationary points of the likelihood function; (b) if the likelihood function is unimodal and a certain differentiability condition is satisfied, then any EM sequence converges to the unique maximum likelihood estimate. A list of key properties of the algorithm is included.},
  coden = {ASTSC7},
  fjournal = {The Annals of Statistics},
  mrclass = {62F10},
  mrnumber = {MR684867 (85e:62049)},
  mrreviewer = {Huynh Huynh},
  keywords = {Baum-Welch algorithm,hidden Markov model}
}

@book{zucchini,
  title = {Hidden Markov Models for Time Series: An Introduction Using r, Second Edition},
  author = {Zucchini, W. and MacDonald, I.L. and Langrock, R.},
  year = {2016},
  series = {Chapman \& {{Hall}}/{{CRC}} Monographs on Statistics \& Applied Probability},
  publisher = {{CRC Press}},
  isbn = {978-1-4822-5384-9}
}
