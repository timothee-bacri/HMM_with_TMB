
@article{ailliot,
  title = {{Stochastic weather generators: an overview of weather type models}},
  shorttitle = {{Stochastic weather generators}},
  author = {Ailliot, Pierre and Allard, Denis and Monbet, Val{\'e}rie and Naveau, Philippe},
  year = {2015},
  journal = {Journal de la soci\'et\'e fran\c{c}aise de statistique},
  volume = {156},
  number = {1},
  pages = {101--113},
  langid = {french}
}

@article{altman,
  title = {Mixed {{Hidden Markov Models}}},
  author = {Altman, Rachel MacKay},
  year = {2007},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {102},
  number = {477},
  pages = {201--210},
  issn = {0162-1459},
  doi = {10.1198/016214506000001086},
  abstract = {Hidden Markov models (HMMs) are a useful tool for capturing the behavior of overdispersed, autocorrelated data. These models have been applied to many different problems, including speech recognition, precipitation modeling, and gene finding and profiling. Typically, HMMs are applied to individual stochastic processes; HMMs for simultaneously modeling multiple processes\textemdash as in the longitudinal data setting\textemdash have not been widely studied. In this article I present a new class of models, mixed HMMs (MHMMs), where I use both covariates and random effects to capture differences among processes. I define the models using the framework of generalized linear mixed models and discuss their interpretation. I then provide algorithms for parameter estimation and illustrate the properties of the estimators via a simulation study. Finally, to demonstrate the practical uses of MHMMs, I provide an application to data on lesion counts in multiple sclerosis patients. I show that my model, while parsimonious, can describe the heterogeneity among such patients.},
  keywords = {Hidden Markov model,Latent process,Longitudinal model,Mixed model,Random effect}
}

@book{bartolucci,
  title = {Latent Markov Models for Longitudinal Data},
  author = {Bartolucci, F. and Farcomeni, A. and Pennoni, F.},
  year = {2012},
  series = {Chapman \& {{Hall}}/{{CRC}} Statistics in the Social and Behavioral Sciences},
  publisher = {{Taylor \& Francis}},
  isbn = {978-1-4398-1708-7},
  lccn = {2012021567}
}

@article{bauer,
  title = {Testing {{Equivalence Simultaneously}} for {{Location}} and {{Dispersion}} of Two {{Normally Distributed Populations}}},
  author = {Bauer, Peter and Bauer, Michael M.},
  year = {1994},
  journal = {Biometrical Journal},
  volume = {36},
  number = {6},
  pages = {643--660},
  issn = {1521-4036},
  doi = {10.1002/bimj.4710360602},
  abstract = {In clinical trials with an active control usually therapeutical equivalence of a new treatment is investigated by looking at a location parameter of the distributions of the primary efficacy variable. But even if the location parameters are close to each other existing differences in variability may be connected with different risks for under or over treatment in an individual patient. Assuming normally distributed responses a multiple test procedure applying two shifted one-sided t-tests for the mean and accordingly two one-sided F-tests for the variances is proposed. Equivalence in location and variability is established if all four tests lead to a rejection at the (one-sided) level {$\alpha$}. A conservative procedure ``correcting'' the t-tests for heteroscedasticity is derived. The choice of a design in terms of the global level {$\alpha$}, the global power, the relevant deviations in the population means and variances, as well as the sample size is outlined. Numerical calculations of the actual level and power for the proposed designs show, that for balanced sample sizes the classical uncorrected one-sided t-tests can be used safely without exaggerating the global type I error probability. Finally an example is given.},
  langid = {english},
  keywords = {Location and variance,Normal distributions,Simultaneous equivalence test},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/bimj.4710360602}
}

@article{baum,
  title = {Statistical {{Inference}} for {{Probabilistic Functions}} of {{Finite State Markov Chains}}},
  author = {Baum, Leonard E. and Petrie, Ted},
  year = {1966},
  month = dec,
  journal = {The Annals of Mathematical Statistics},
  volume = {37},
  number = {6},
  pages = {1554--1563},
  issn = {0003-4851},
  doi = {10.1214/aoms/1177699147},
  langid = {english}
}

@article{bauma,
  title = {A {{Maximization Technique Occurring}} in the {{Statistical Analysis}} of {{Probabilistic Functions}} of {{Markov Chains}}},
  author = {Baum, Leonard E. and Petrie, Ted and Soules, George and Weiss, Norman},
  year = {1970},
  journal = {The Annals of Mathematical Statistics},
  volume = {41},
  number = {1},
  pages = {164--171},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {00034851}
}

@article{bulla,
  title = {Computational Issues in Parameter Estimation for Stationary Hidden {{Markov}} Models},
  author = {Bulla, Jan and Berzel, Andreas},
  year = {2008},
  month = jan,
  journal = {Computational Statistics},
  volume = {23},
  number = {1},
  pages = {1--18},
  issn = {1613-9658},
  doi = {10.1007/s00180-007-0063-y},
  abstract = {The parameters of a hidden Markov model (HMM) can be estimated by numerical maximization of the log-likelihood function or, more popularly, using the expectation\textendash maximization (EM) algorithm. In its standard implementation the latter is unsuitable for fitting stationary hidden Markov models (HMMs). We show how it can be modified to achieve this. We propose a hybrid algorithm that is designed to combine the advantageous features of the two algorithms and compare the performance of the three algorithms using simulated data from a designed experiment, and a real data set. The properties investigated are speed of convergence, stability, dependence on initial values, different parameterizations. We also describe the results of an experiment to assess the true coverage probability of bootstrap-based confidence intervals for the parameters.},
  keywords = {computation,hidden Markov model,issues}
}

@article{bullaa,
  title = {Markov-Switching Asset Allocation: {{Do}} Profitable Strategies Exist?},
  shorttitle = {Markov-Switching Asset Allocation},
  author = {Bulla, Jan and Mergner, Sascha and Bulla, Ingo and Sesbo{\"u}{\'e}, Andr{\'e} and Chesneau, Christophe},
  year = {2011},
  month = nov,
  journal = {Journal of Asset Management},
  volume = {12},
  number = {5},
  pages = {310--321},
  issn = {1479-179X},
  doi = {10.1057/jam.2010.27},
  abstract = {This article proposes a straightforward Markov-switching asset allocation model, which reduces the market exposure to periods of high volatility. The main purpose of the study is to examine the performance of a regime-based asset allocation strategy under realistic assumptions, compared to a buy-and-hold strategy. An empirical study, utilizing daily return series of major equity indices in the United States, Japan and Germany over the past 40 years, investigates the performance of the model. In an out-of-sample context, the strategy proves profitable after taking transaction costs into account. For the regional markets under consideration, the volatility reduces on average by 41 per cent. In addition, annualized excess returns attain 18.5 to 201.6 basis points.},
  langid = {english},
  keywords = {classical result,high,low,market,mean,variance,volatility}
}

@book{cappe,
  title = {Inference in {{Hidden Markov Models}}},
  author = {Capp{\'e}, Olivier and Moulines, Eric and Ryden, Tobias},
  year = {2006},
  month = apr,
  publisher = {{Springer Science \& Business Media}},
  abstract = {Hidden Markov models have become a widely used class of statistical models with applications in diverse areas such as communications engineering, bioinformatics, finance and many more. This book is a comprehensive treatment of inference for hidden Markov models, including both algorithms and statistical theory. Topics range from filtering and smoothing of the hidden Markov chain to parameter estimation, Bayesian methods and estimation of the number of states. In a unified way the book covers both models with finite state spaces, which allow for exact algorithms for filtering, estimation etc. and models with continuous state spaces (also called state-space models) requiring approximate simulation-based algorithms that are also described in detail. Simulation in hidden Markov models is addressed in five different chapters that cover both Markov chain Monte Carlo and sequential Monte Carlo approaches. Many examples illustrate the algorithms and theory. The book also carefully treats Gaussian linear state-space models and their extensions and it contains a chapter on general Markov chain theory and probabilistic aspects of hidden Markov models. This volume will suit anybody with an interest in inference for stochastic processes, and it will be useful for researchers and practitioners in areas such as statistics, signal processing, communications engineering, control theory, econometrics, finance and more. The algorithmic parts of the book do not require an advanced mathematical background, while the more theoretical parts require knowledge of probability theory at the measure-theoretical level. From the reviews: "By providing an overall survey of results obtained so far in a very readable manner, and also presenting some new ideas, this well-written book will appeal to academic researchers in the field of HMMs, with PhD students working on related topics included. It will also appeal to practitioners and researchers from other fields by guiding them through the computational steps needed for making inference HMMs and/or by providing them with the relevant underlying statistical theory. In the reviewer's opinion this book will shortly become a reference work in its field." MathSciNet "This monograph is a valuable resource. It provides a good literature review, an excellent account of the state of the art research on the necessary theory and algorithms, and ample illustrations of numerous applications of HMM. It goes much beyond the earlier resources on HMM...I anticipate this work to serve well many Technometrics readers in the coming years." Haikady N. Nagaraja for Technometrics, November 2006},
  googlebooks = {4d\_oEYn8Fl0C},
  isbn = {978-0-387-28982-3},
  langid = {english},
  keywords = {Business \& Economics / Statistics,Computers / Computer Simulation,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes,Technology \& Engineering / Electronics / General,Technology \& Engineering / Imaging Systems}
}

@book{casella,
  title = {Statistical {{Inference}}},
  author = {Casella, George and Berger, Roger L.},
  year = {2021},
  month = jan,
  publisher = {{Cengage Learning}},
  abstract = {This book builds theoretical statistics from the first principles of probability theory. Starting from the basics of probability, the authors develop the theory of statistical inference using techniques, definitions, and concepts that are statistical and are natural extensions and consequences of previous concepts. Intended for first-year graduate students, this book can be used for students majoring in statistics who have a solid mathematics background. It can also be used in a way that stresses the more practical uses of statistical theory, being more concerned with understanding basic statistical concepts and deriving reasonable statistical procedures for a variety of situations, and less concerned with formal optimality investigations.Important Notice: Media content referenced within the product description or the product text may not be available in the ebook version.},
  googlebooks = {FAUVEAAAQBAJ},
  isbn = {978-0-357-75313-2},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General}
}

@article{celeux,
  title = {Selecting Hidden {{Markov}} Model State Number with Cross-Validated Likelihood},
  author = {Celeux, Gilles and Durand, Jean-Baptiste},
  year = {2008},
  month = oct,
  journal = {Computational Statistics},
  volume = {23},
  number = {4},
  pages = {541--564},
  issn = {1613-9658},
  doi = {10.1007/s00180-007-0097-1},
  abstract = {The problem of estimating the number of hidden states in a hidden Markov model is considered. Emphasis is placed on cross-validated likelihood criteria. Using cross-validation to assess the number of hidden states allows to circumvent the well-documented technical difficulties of the order identification problem in mixture models. Moreover, in a predictive perspective, it does not require that the sampling distribution belongs to one of the models in competition. However, computing cross-validated likelihood for hidden Markov models for which only one training sample is available, involves difficulties since the data are not independent. Two approaches are proposed to compute cross-validated likelihood for a hidden Markov model. The first one consists of using a deterministic half-sampling procedure, and the second one consists of an adaptation of the EM algorithm for hidden Markov models, to take into account randomly missing values induced by cross-validation. Numerical experiments on both simulated and real data sets compare different versions of cross-validated likelihood criterion and penalised likelihood criteria, including BIC and a penalised marginal likelihood criterion. Those numerical experiments highlight a promising behaviour of the deterministic half-sampling criterion.},
  langid = {english}
}

@article{dempster,
  title = {Maximum {{Likelihood}} from {{Incomplete Data Via}} the {{EM Algorithm}}},
  author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
  year = {1977},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {39},
  number = {1},
  pages = {1--22},
  issn = {2517-6161},
  doi = {10.1111/j.2517-6161.1977.tb01600.x},
  abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
  copyright = {\textcopyright{} 1977 The Authors},
  langid = {english},
  keywords = {em algorithm,incomplete data,maximum likelihood,posterior mode},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1977.tb01600.x}
}

@article{drton,
  title = {A {{Bayesian}} Information Criterion for Singular Models},
  author = {Drton, Mathias and Plummer, Martyn},
  year = {2017},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {79},
  number = {2},
  pages = {323--380},
  issn = {1467-9868},
  doi = {10.1111/rssb.12187},
  abstract = {We consider approximate Bayesian model choice for model selection problems that involve models whose Fisher information matrices may fail to be invertible along other competing submodels. Such singular models do not obey the regularity conditions underlying the derivation of Schwarz's Bayesian information criterion BIC and the penalty structure in BIC generally does not reflect the frequentist large sample behaviour of the marginal likelihood. Although large sample theory for the marginal likelihood of singular models has been developed recently, the resulting approximations depend on the true parameter value and lead to a paradox of circular reasoning. Guided by examples such as determining the number of components in mixture models, the number of factors in latent factor models or the rank in reduced rank regression, we propose a resolution to this paradox and give a practical extension of BIC for singular model selection problems.},
  copyright = {\textcopyright{} 2017 Royal Statistical Society},
  langid = {english},
  keywords = {Bayesian information criterion,Factor analysis,Mixture model,Model selection,Reduced rank regression,Schwarz information criterion,Singular learning theory},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12187}
}

@book{durbin,
  title = {Biological {{Sequence Analysis}}: {{Probabilistic Models}} of {{Proteins}} and {{Nucleic Acids}}},
  shorttitle = {Biological {{Sequence Analysis}}},
  author = {Durbin, Richard},
  year = {1998},
  month = apr,
  abstract = {Probabilistic models are becoming increasingly important in analysing the huge amount of data being produced by large-scale DNA-sequencing efforts such as the Human Genome Project. For example, hidden Markov models are used for analysing biological sequences, linguistic-grammar-based probabilistic models for identifying RNA secondary structure, and probabilistic evolutionary models for inferring phylogenies of sequences from different organisms. This book gives a unified, up-to-date and self-contained account, with a Bayesian slant, of such methods, and more generally to probabilistic methods of sequence analysis. Written by an interdisciplinary team of authors, it aims to be accessible to molecular biologists, computer scientists, and mathematicians with no formal knowledge of the other fields, and at the same time present the state-of-the-art in this new and highly important field.}
}

@article{eddy,
  title = {Profile Hidden {{Markov}} Models.},
  author = {Eddy, S R},
  year = {1998},
  month = jan,
  journal = {Bioinformatics},
  volume = {14},
  number = {9},
  pages = {755--763},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/14.9.755},
  abstract = {The recent literature on profile hidden Markov model (profile HMM) methods and software is reviewed. Profile HMMs turn a multiple sequence alignment into a position-specific scoring system suitable for searching databases for remotely homologous sequences. Profile HMM analyses complement standard pairwise comparison methods for large-scale sequence analysis. Several software implementations and two large libraries of profile HMMs of common protein domains are available. HMM methods performed comparably to threading methods in the CASP2 structure prediction exercise.}
}

@book{efron,
  title = {An Introduction to the Bootstrap},
  author = {Efron, Bradley and Tibshirani, Robert J},
  year = {1993},
  publisher = {{Chapman \& Hall}},
  address = {{New York, N.Y.; London}},
  isbn = {978-0-412-04231-7},
  langid = {english},
  annotation = {OCLC: 797437299}
}

@book{feller,
  title = {An Introduction to Probability Theory and Its Applications},
  author = {Feller, William},
  year = {1968},
  publisher = {{Wiley}},
  abstract = {Major changes in this edition include the substitution of probabilistic arguments for combinatorial artifices, and the addition of new sections on branching processes, Markov chains, and the De Moivre-Laplace theorem.},
  googlebooks = {mfRQAAAAMAAJ},
  isbn = {978-0-471-25708-0},
  langid = {english},
  keywords = {aperiodic Markov chain,Business \& Economics / Statistics,Fiction / General,Mathematics / General,Mathematics / Probability \& Statistics / General}
}

@article{fischer,
  title = {A Robust and Efficient Algorithm to Find Profile Likelihood Confidence Intervals},
  author = {Fischer, Samuel M. and Lewis, Mark A.},
  year = {2021},
  month = may,
  journal = {Statistics and Computing},
  volume = {31},
  number = {4},
  pages = {38},
  issn = {1573-1375},
  doi = {10.1007/s11222-021-10012-y},
  abstract = {Profile likelihood confidence intervals are a robust alternative to Wald's method if the asymptotic properties of the maximum likelihood estimator are not met. However, the constrained optimization problem defining profile likelihood confidence intervals can be difficult to solve in these situations, because the likelihood function may exhibit unfavorable properties. As a result, existing methods may be inefficient and yield misleading results. In this paper, we address this problem by computing profile likelihood confidence intervals via a trust-region approach, where steps computed based on local approximations are constrained to regions where these approximations are sufficiently precise. As our algorithm also accounts for numerical issues arising if the likelihood function is strongly non-linear or parameters are not estimable, the method is applicable in many scenarios where earlier approaches are shown to be unreliable. To demonstrate its potential in applications, we apply our algorithm to benchmark problems and compare it with 6 existing approaches to compute profile likelihood confidence intervals. Our algorithm consistently achieved higher success rates than any competitor while also being among the quickest methods. As our algorithm can be applied to compute both confidence intervals of parameters and model predictions, it is useful in a wide range of scenarios.},
  langid = {english},
  keywords = {confidence intervals,constrained,optimization,profile likelihood}
}

@article{fredkin,
  title = {Bayesian {{Restoration}} of {{Single-Channel Patch Clamp Recordings}}},
  author = {Fredkin, Donald R. and Rice, John A.},
  year = {1992},
  journal = {Biometrics},
  volume = {48},
  number = {2},
  pages = {427--448},
  publisher = {{[Wiley, International Biometric Society]}},
  issn = {0006341X, 15410420},
  doi = {10.2307/2532301},
  abstract = {[The technique of patch clamp recording makes possible the measurement of current flowing through a single ion channel in a cell membrane. Examination of such recordings suggests that the current is quantal in nature, alternating in a seemingly random manner between "on" and "off," but the recordings are corrupted by noise from a variety of sources. In this paper we propose and illustrate methods for restoring the underlying quantal signal from such noisy measurements. The methods use a Markov chain prior distribution for the underlying quantal process and base the restoration on the resulting posterior distribution.]},
  keywords = {Speech recognition}
}

@book{fruhwirth-schnatter,
  title = {Finite {{Mixture}} and {{Markov Switching Models}}},
  author = {{Fr{\"u}hwirth-Schnatter}, Sylvia},
  year = {2006},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/978-0-387-35768-3},
  abstract = {The prominence of finite mixture modelling is greater than ever. Many important statistical topics like clustering data, outlier treatment, or dealing with unobserved heterogeneity involve finite mixture models in some way or other. The area of potential applications goes beyond simple data analysis and extends to regression analysis and to non-linear time series analysis using Markov switching models. For more than the hundred years since Karl Pearson showed in 1894 how to estimate the five parameters of a mixture of two normal distributions using the method of moments, statistical inference for finite mixture models has been a challenge to everybody who deals with them. In the past ten years, very powerful computational tools emerged for dealing with these models which combine a Bayesian approach with recent Monte simulation techniques based on Markov chains. This book reviews these techniques and covers the most recent advances in the field, among them bridge sampling techniques and reversible jump Markov chain Monte Carlo methods. It is the first time that the Bayesian perspective of finite mixture modelling is systematically presented in book form. It is argued that the Bayesian approach provides much insight in this context and is easily implemented in practice. Although the main focus is on Bayesian inference, the author reviews several frequentist techniques, especially selecting the number of components of a finite mixture model, and discusses some of their shortcomings compared to the Bayesian approach. The aim of this book is to impart the finite mixture and Markov switching approach to statistical modelling to a wide-ranging community. This includes not only statisticians, but also biologists, economists, engineers, financial agents, market researcher, medical researchers or any other frequent user of statistical models. This book should help newcomers to the field to understand how finite mixture and Markov switching models are formulated, what structures they imply on the data, what they could be used for, and how they are estimated. Researchers familiar with the subject also will profit from reading this book. The presentation is rather informal without abandoning mathematical correctness. Previous notions of Bayesian inference and Monte Carlo simulation are useful but not needed. Sylvia Fr\"uhwirth-Schnatter is Professor of Applied Statistics and Econometrics at the Department of Applied Statistics of the Johannes Kepler University in Linz, Austria. She received her Ph.D. in mathematics from the University of Technology in Vienna in 1988. She has published in many leading journals in applied statistics and econometrics on topics such as Bayesian inference, finite mixture models, Markov switching models, state space models, and their application in marketing, economics and finance.},
  isbn = {978-0-387-32909-3},
  langid = {english}
}

@article{gales,
  title = {The {{Application}} of {{Hidden Markov Models}} in {{Speech Recognition}}},
  author = {Gales, Mark and Young, Steve},
  year = {2008},
  month = feb,
  journal = {Foundations and Trends\textregistered{} in Signal Processing},
  volume = {1},
  number = {3},
  pages = {195--304},
  publisher = {{Now Publishers, Inc.}},
  issn = {1932-8346, 1932-8354},
  doi = {10.1561/2000000004},
  abstract = {The Application of Hidden Markov Models in Speech Recognition},
  langid = {english}
}

@article{gay,
  title = {Usage Summary for Selected Optimization Routines},
  author = {Gay, David M},
  year = {1990},
  journal = {Computing science technical report},
  volume = {153},
  pages = {1--21}
}

@book{grimmett,
  title = {Probability and {{Random Processes}}},
  author = {Grimmett, Geoffrey and Grimmett, Geoffrey R. and Grimmett, Professor of Mathematical Statistics Geoffrey and Stirzaker, David and Stirzaker, Mathematical Institute David R.},
  year = {2001},
  month = may,
  publisher = {{OUP Oxford}},
  abstract = {The third edition of this successful text gives a rigorous introduction to probability theory and the discussion of the most important random processes in some depth. It includes various topics which are suitable for undergraduate courses, but are not routinely taught. It is suitable to the beginner, and provides a taste and encouragement for more advanced work. There are four main aims: 1) to provide a thorough but straightforward account of basic probability, giving the reader a natural feel for the subject unburdened by oppressive technicalities, 2) to discuss important random processes in depth with many examples. 3) to cover a range of important but less routine topics, 4) to impart to the beginner the flavour of more advanced work. The books begins with basic ideas common to many undergraduate courses in mathematics, statistics and the sciences; in concludes with topics usually found at graduate level. The ordering and numbering of material in this third edition has been mostly preserved from the second. Minor alterations and additions have been added for clearer exposition. Highlights include new sections on sampling and Markov chain Monte Carlo, geometric probability, coupling and Poisson approximation, large deviations, spatial Poisson processes, renewal-reward, queueing networks, stochastic calculus, It\^o's formula and option pricing in the Black-Scholes model for financial markets. In addition there are many (nearly 400) new exercises and problems that are entertaining and instructive; their solutions can be found in the companion volume 'One Thousand Exercises in Probability', (OUP 2001).},
  isbn = {978-0-19-857222-0},
  langid = {english},
  keywords = {irreducibile Markov chain,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes}
}

@article{hamilton,
  title = {A {{New Approach}} to the {{Economic Analysis}} of {{Nonstationary Time Series}} and the {{Business Cycle}}},
  author = {Hamilton, James D.},
  year = {1989},
  journal = {Econometrica},
  volume = {57},
  number = {2},
  pages = {357--384},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {0012-9682},
  doi = {10.2307/1912559},
  abstract = {This paper proposes a very tractable approach to modeling changes in regime. The parameters of an autoregression are viewed as the outcome of a discrete-state Markov process. For example, the mean growth rate of a nonstationary series may be subject to occasional, discrete shifts. The econometrician is presumed not to observe these shifts directly, but instead must draw probabilistic inference about whether and when they may have occurred based on the observed behavior of the series. The paper presents an algorithm for drawing such estimation of population parameters by the method of maximum likelihood and provides the foundation for forecasting future values of the series. An empirical application of this technique to postwar U.S. real GNP suggests that the periodic shift from a positive growth rate to a negative growth rate is a recurrent feature of the U.S. business cycle, and indeed could be used as an objective criterion for defining and measuring economic recessions. The estimated parameter values suggest that a typical economic recession is associated with a 3\% permanent drop in the level of GNP.}
}

@article{hardle,
  title = {Bootstrap {{Methods}} for {{Time Series}}},
  author = {H{\"a}rdle, Wolfgang and Horowitz, Joel and Kreiss, Jens-Peter},
  year = {2003},
  journal = {International Statistical Review},
  volume = {71},
  number = {2},
  pages = {435--459},
  issn = {1751-5823},
  doi = {10.1111/j.1751-5823.2003.tb00485.x},
  abstract = {The bootstrap is a method for estimating the distribution of an estimator or test statistic by resampling one's data or a model estimated from the data. The methods that are available for implementing the bootstrap and the accuracy of bootstrap estimates depend on whether the data are an independent random sample or a time series. This paper is concerned with the application of the bootstrap to time-series data when one does not have a finite-dimensional parametric model that reduces the data generation process to independent random sampling. We review the methods that have been proposed for implementing the bootstrap in this situation and discuss the accuracy of these methods relative to that of first-order asymptotic approximations. We argue that methods for implementing the bootstrap with time-series data are not as well understood as methods for data that are independent random samples. Although promising bootstrap methods for time series are available, there is a considerable need for further research in the application of the bootstrap to time series. We describe some of the important unsolved problems. R\'esum\'e Le bootstrap est une m\'ethode pour estimer la distribution d'un estimateur en r\'e\'echantillonnant ses donn\'ees ou un mod\'ele estim\'e\`a partir des donn\'ees. Les m\'ethodes disponibles pour mettre en oeuvre le bootstrap et la pr\'ecision des estimateurs de bootstrap d\'ependent de ce que les donn\'ees proviennent d'un \'echantillon al\'eatoire ind\'ependant ou d'une s\'erie temporelle. Cet article concerne l'application du bootstrap aux donn\'ees des s\'eries temporelles quand on ne dispose pas de mod\'ele param\'etrique de dimension finie qui r\'eduise le processus de g\'en\'eration des donn\'ees \`a l'\'echantillonnage al\'eatoire ind\'ependent. Nous examinons les m\'ethodes qui ont \'et\'e propos\'ees pour mettre en oeuvre le bootstrap dans cette situation et discutons la precision de ces m\'ethodes comparativement \`a celle des approximations asymptotiques de premier ordre. Nous montrons que les m\'ethodes pour mettre en oeuvre le bootstrap avec les donn\'ees des s\'eries temporelles ne sont pas aussi bien comprises que les m\'ethodes pour les donn\'ees des \'echantillons al\'eatoires ind\'ependants. Bien que des m\'ethodes de bootstrap prometteuses pour les s\'eries temporelles soient disponibles, il y a un besoin consid\'erable de recherche suppl\'emental re dans leur application. Nous d\'ecrivons quelques probl\'emes importants non r\'esolus.},
  langid = {english},
  keywords = {Asymptotic approximation confidence interval,Block bootstrap,Resampling,Time series}
}

@article{kass,
  title = {Approximate {{Bayesian Inference}} in {{Conditionally Independent Hierarchical Models}} ({{Parametric Empirical Bayes Models}})},
  author = {Kass, Robert E. and Steffey, Duane},
  year = {1989},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {84},
  number = {407},
  pages = {717--726},
  issn = {0162-1459},
  doi = {10.1080/01621459.1989.10478825},
  abstract = {We consider two-stage models of the kind used in parametric empirical Bayes (PEB) methodology, calling them conditionally independent hierarchical models. We suppose that there are k ``units,'' which may be experimental subjects, cities, study centers, etcetera. At the first stage, the observation vectors Yi for units i = 1, \ldots, k are independently distributed with densities p(yi | \texttheta i ), or more generally, p(yi | \texttheta i, {$\lambda$}). At the second stage, the unit-specific parameter vectors \texttheta i are iid with densities p(\texttheta i | {$\lambda$}). The PEB approach proceeds by regarding the second-stage distribution as a prior and noting that, if {$\lambda$} were known, inference about \texttheta{} could be based on its posterior. Since {$\lambda$} is not known, the simplest PEB methods estimate the parameter {$\lambda$} by maximum likelihood or some variant, and then treat {$\lambda$} as if it were known to be equal to this estimate. Although this procedure is sometimes satisfactory, a well-known defect is that it neglects the uncertainty due to the estimation of {$\lambda$}. In this article we suggest that approximate Bayesian inference can provide simple and manageable solutions to this problem. In Bayesian inferences, a prior density {$\pi$}({$\cdot$}) on {$\lambda$} is introduced, the posterior p({$\lambda$} | y) is calculated, and the posterior density of \texttheta i is then equal to the expectation, with respect to p({$\lambda$} | y), of the conditional posterior p(\texttheta i | yi, {$\lambda$}). From the Bayesian point of view, the PEB estimate is of interest because it is a first-order approximation to the posterior mean [having an error of order O(k -1)]. Letting E{$\lambda$} and V{$\lambda$} denote the expectation and variance with respect to p({$\lambda$} | y), we may write the posterior variance of \texttheta i as V(\texttheta i | y) = E{$\lambda$} V(\texttheta i | yi, {$\lambda$}) + V{$\lambda$} E(\texttheta i | yi, {$\lambda$}). The conditional posterior variance , where is the maximum likelihood estimator, approximates only the first term. When we include an approximation to the second term we obtain a first-order approximation to the posterior variance itself. In many examples, this elementary method, incorporating approximations to both terms, will substantially account for the estimation of {$\lambda$}. We briefly consider second-order approximations, noting that the work of Deely and Lindley (1981) may be extended using expansions derived by Lindley (1980), Mosteller and Wallace (1964), Tierney and Kadane (1986), and Tierney, Kass, and Kadane (1989). We suggest that second-order approximations provide rough and, often, easily computed assessments of accuracy of first-order approximations. Although we confine our data-analytical examples to simple models, we believe the methods will be useful in general settings. An important area of application is longitudinal data analysis.},
  keywords = {Asymptotic posterior,Asymptotic variance,Bayes empirical Bayes,Delta method,Hyperparameters,Laplace's method,Longitudinal analysis,Mixed models,Random-effects models}
}

@article{kristensen,
  title = {{{TMB}}: {{Automatic}} Differentiation and {{Laplace}} Approximation},
  author = {Kristensen, Kasper and Nielsen, Anders and Berg, Casper W. and Skaug, Hans and Bell, Bradley M.},
  year = {2016},
  journal = {Journal of Statistical Software},
  volume = {70},
  number = {5},
  pages = {1--21},
  doi = {10.18637/jss.v070.i05}
}

@article{lagona,
  title = {Latent Time-Varying Factors in Longitudinal Analysis: A Linear Mixed Hidden {{Markov}} Model for Heart Rates},
  author = {Lagona, Francesco and Jdanov, Dmitri and Shkolnikova, Maria},
  year = {2014},
  journal = {Statistics in Medicine},
  volume = {33},
  number = {23},
  pages = {4116--4134},
  issn = {1097-0258},
  doi = {10.1002/sim.6220}
}

@article{langeheine,
  title = {Bootstrapping {{Goodness-of-Fit Measures}} in {{Categorical Data Analysis}}},
  author = {Langeheine, Rolf and Pannekoek, Jeroen and {van de Pol}, Frank},
  year = {1996},
  month = may,
  journal = {Sociological Methods \& Research},
  volume = {24},
  pages = {492--516},
  doi = {10.1177/0049124196024004004},
  abstract = {When sparse data have to be fitted to a log-linear or latent class model, one cannot use the theoretical chi-square distribution to evaluate model fit, because with sparse data the observed cross-table has too many cells in relation to the number of observations to use a distribution that only holds asymptotically. The choice of a theoretical distribution is also difficult when model-expected frequencies are 0 or when model probabilities are estimated 0 or 1. The authors propose to solve these problems by estimating the distribution of a fit measure, using bootstrap methods. An algorithm is presented for estimating this distribution by drawing bootstrap samples from the model-expected proportions, the so-called nonnaive bootstrap method. For the first time the method is applied to empirical data of varying sparseness, from five different data sets. Results show that the asymptotic chi-square distribution is not at all valid for sparse data.},
  keywords = {Confidence intervals}
}

@article{leroux,
  title = {Maximum-{{Penalized-Likelihood Estimation}} for {{Independent}} and {{Markov- Dependent Mixture Models}}},
  author = {Leroux, Brian G. and Puterman, Martin L.},
  year = {1992},
  journal = {Biometrics},
  volume = {48},
  number = {2},
  pages = {545--558},
  issn = {0006-341X},
  doi = {10.2307/2532308},
  abstract = {This paper concerns the use and implementation of maximum-penalized-likelihood procedures for choosing the number of mixing components and estimating the parameters in independent and Markov-dependent mixture models. Computation of the estimates is achieved via algorithms for the automatic generation of starting values for the EM algorithm. Computation of the information matrix is also discussed. Poisson mixture models are applied to a sequence of counts of movements by a fetal lamb in utero obtained by ultrasound. The resulting estimates are seen to provide plausible mechanisms for the physiological process.},
  keywords = {Hidden Markov model,lamb}
}

@article{lystig,
  title = {Exact {{Computation}} of the {{Observed Information Matrix}} for {{Hidden Markov Models}}},
  author = {Lystig, Theodore C and Hughes, James P},
  year = {2002},
  month = sep,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {11},
  number = {3},
  pages = {678--689},
  publisher = {{Taylor \& Francis}},
  issn = {1061-8600},
  doi = {10.1198/106186002402},
  abstract = {This article describes a new algorithm for exact computation of the observed information matrix in hidden Markov models that may be performed in a single pass through the data. The score vector and log-likelihood are computed in the same pass. The new algorithm is derived from the forward\textendash backward algorithm traditionally used to evaluate the likelihood in hidden Markov models. Our result is discussed in the context of previous approaches that have been used to obtain approximate standard errors of parameter estimates in these models. Implications for parameter estimation are also discussed. An application of the proposed methods to rainfall occurrence data is provided.}
}

@article{maruotti,
  title = {Mixed Hidden Markov Models for Longitudinal Data: An Overview},
  author = {Maruotti, Antonello},
  year = {2011},
  journal = {International Statistical Review},
  volume = {79},
  number = {3},
  pages = {427--454},
  publisher = {{Blackwell Publishing Ltd}},
  issn = {1751-5823},
  doi = {10.1111/j.1751-5823.2011.00160.x},
  keywords = {Longitudinal data,mixed hidden Markov model,random effects model,unobserved heterogeneity}
}

@article{mcclintock,
  title = {Uncovering Ecological State Dynamics with Hidden {{Markov}} Models},
  author = {McClintock, Brett T. and Langrock, Roland and Gimenez, Olivier and Cam, Emmanuelle and Borchers, David L. and Glennie, Richard and Patterson, Toby A.},
  year = {2020},
  journal = {Ecology Letters},
  volume = {23},
  number = {12},
  pages = {1878--1903},
  issn = {1461-0248},
  doi = {10.1111/ele.13610},
  abstract = {Ecological systems can often be characterised by changes among a finite set of underlying states pertaining to individuals, populations, communities or entire ecosystems through time. Owing to the inherent difficulty of empirical field studies, ecological state dynamics operating at any level of this hierarchy can often be unobservable or `hidden'. Ecologists must therefore often contend with incomplete or indirect observations that are somehow related to these underlying processes. By formally disentangling state and observation processes based on simple yet powerful mathematical properties that can be used to describe many ecological phenomena, hidden Markov models (HMMs) can facilitate inferences about complex system state dynamics that might otherwise be intractable. However, HMMs have only recently begun to gain traction within the broader ecological community. We provide a gentle introduction to HMMs, establish some common terminology, review the immense scope of HMMs for applied ecological research and provide a tutorial on implementation and interpretation. By illustrating how practitioners can use a simple conceptual template to customise HMMs for their specific systems of interest, revealing methodological links between existing applications, and highlighting some practical considerations and limitations of these approaches, our goal is to help establish HMMs as a fundamental inferential tool for ecologists.},
  copyright = {Published 2020. This article is a U.S. Government work and is in the public domain in the USA. Ecology Letters published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {Behavioural ecology,community ecology,ecosystem ecology,hidden Markov model,hierarchical model,movement ecology,observation error,population ecology,state-space model,time series},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ele.13610}
}

@book{mclachlan,
  title = {Finite {{Mixture Models}}},
  author = {McLachlan, Geoffrey J. and Peel, David},
  year = {2004},
  month = mar,
  publisher = {{John Wiley \& Sons}},
  abstract = {An up-to-date, comprehensive account of major issues in finite mixture modeling This volume provides an up-to-date account of the theory and applications of modeling via finite mixture distributions. With an emphasis on the applications of mixture models in both mainstream analysis and other areas such as unsupervised pattern recognition, speech recognition, and medical imaging, the book describes the formulations of the finite mixture approach, details its methodology, discusses aspects of its implementation, and illustrates its application in many common statistical contexts. Major issues discussed in this book include identifiability problems, actual fitting of finite mixtures through use of the EM algorithm, properties of the maximum likelihood estimators so obtained, assessment of the number of components to be used in the mixture, and the applicability of asymptotic theory in providing a basis for the solutions to some of these problems. The author also considers how the EM algorithm can be scaled to handle the fitting of mixture models to very large databases, as in data mining applications. This comprehensive, practical guide: * Provides more than 800 references-40\% published since 1995 * Includes an appendix listing available mixture software * Links statistical literature with machine learning and pattern recognition literature * Contains more than 100 helpful graphs, charts, and tables Finite Mixture Models is an important resource for both applied and theoretical statisticians as well as for researchers in the many areas in which finite mixture models can be used to analyze data.},
  googlebooks = {c2\_fAox0DQoC},
  isbn = {978-0-471-65406-3},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes}
}

@article{meeker,
  title = {Teaching about {{Approximate Confidence Regions Based}} on {{Maximum Likelihood Estimation}}},
  author = {Meeker, William Q. and Escobar, Luis A.},
  year = {1995},
  month = feb,
  journal = {The American Statistician},
  volume = {49},
  number = {1},
  pages = {48--53},
  issn = {0003-1305},
  doi = {10.1080/00031305.1995.10476112},
  abstract = {Maximum likelihood (ML) provides a powerful and extremely general method for making inferences over a wide range of data/model combinations. The likelihood function and likelihood ratios have clear intuitive meanings that make it easy for students to grasp the important concepts. Modern computing technology has made it possible to use these methods over a wide range of practical applications. However, many mathematical statistics textbooks, particularly those at the Senior/Masters level, do not give this important topic coverage commensurate with its place in the world of modern applications. Similarly, in nonlinear estimation problems, standard practice (as reflected by procedures available in the popular commercial statistical packages) has been slow to recognize the advantages of likelihood-based confidence regions/intervals over the commonly use ``normal-theory'' regions/intervals based on the asymptotic distribution of the ``Wald statistic.'' In this note we outline our approach for presenting, to students, confidence regions/intervals based on ML estimation.},
  keywords = {Asymptotic approximation,Confidence interval,invariance,Large sample approximation,parameter transformation,Profile likelihood,reparametrization}
}

@article{mor,
  title = {A {{Systematic Review}} of {{Hidden Markov Models}} and {{Their Applications}}},
  author = {Mor, Bhavya and Garhwal, Sunita and Kumar, Ajay},
  year = {2021},
  month = may,
  journal = {Archives of Computational Methods in Engineering},
  volume = {28},
  number = {3},
  pages = {1429--1448},
  issn = {1886-1784},
  doi = {10.1007/s11831-020-09422-4},
  abstract = {The hidden Markov models are statistical models used in many real-world applications and communities. The use of hidden Markov models has become predominant in the last decades, as evidenced by a large number of published papers. In this survey, 146 papers (101 from Journals and 45 from Conferences/Workshops) from 93 Journals and 44 Conferences/Workshops are considered. The authors evaluate the literature based on hidden Markov model variants that have been applied to various application fields. The paper represents a short but comprehensive description of research on hidden Markov model and its variants for various applications. The paper shows the significant trends in the research on hidden Markov model variants and their applications.},
  langid = {english}
}

@article{pohle,
  title = {Selecting the {{Number}} of {{States}} in {{Hidden Markov Models}} - {{Pitfalls}}, {{Practical Challenges}} and {{Pragmatic Solutions}}},
  author = {Pohle, Jennifer and Langrock, Roland and {van Beest}, Floris and Schmidt, Niels Martin},
  year = {2017},
  month = apr,
  journal = {arXiv:1701.08673 [q-bio, stat]},
  eprint = {1701.08673},
  eprinttype = {arxiv},
  primaryclass = {q-bio, stat},
  abstract = {We discuss the notorious problem of order selection in hidden Markov models, i.e. of selecting an adequate number of states, highlighting typical pitfalls and practical challenges arising when analyzing real data. Extensive simulations are used to demonstrate the reasons that render order selection particularly challenging in practice despite the conceptual simplicity of the task. In particular, we demonstrate why well-established formal procedures for model selection, such as those based on standard information criteria, tend to favor models with numbers of states that are undesirably large in situations where states shall be meaningful entities. We also offer a pragmatic step-by-step approach together with comprehensive advice for how practitioners can implement order selection. Our proposed strategy is illustrated with a real-data case study on muskox movement.},
  archiveprefix = {arXiv},
  keywords = {Hidden Markov model,number,Quantitative Biology - Quantitative Methods,select,states,Statistics - Methodology}
}

@article{pohlea,
  title = {Selecting the {{Number}} of {{States}} in {{Hidden Markov Models}}: {{Pragmatic Solutions Illustrated Using Animal Movement}}},
  shorttitle = {Selecting the {{Number}} of {{States}} in {{Hidden Markov Models}}},
  author = {Pohle, Jennifer and Langrock, Roland and {van Beest}, Floris M. and Schmidt, Niels Martin},
  year = {2017},
  month = sep,
  journal = {Journal of Agricultural, Biological and Environmental Statistics},
  volume = {22},
  number = {3},
  pages = {270--293},
  issn = {1537-2693},
  doi = {10.1007/s13253-017-0283-8},
  abstract = {We discuss the notorious problem of order selection in hidden Markov models, that is of selecting an adequate number of states, highlighting typical pitfalls and practical challenges arising when analyzing real data. Extensive simulations are used to demonstrate the reasons that render order selection particularly challenging in practice despite the conceptual simplicity of the task. In particular, we demonstrate why well-established formal procedures for model selection, such as those based on standard information criteria, tend to favor models with numbers of states that are undesirably large in situations where states shall be meaningful entities. We also offer a pragmatic step-by-step approach together with comprehensive advice for how practitioners can implement order selection. Our proposed strategy is illustrated with a real-data case study on muskox movement.},
  langid = {english},
  keywords = {Hidden Markov model,number,select,states}
}

@article{probst,
  title = {Emotion Dynamics and Tinnitus: {{Daily}} Life Data from the ``{{TrackYourTinnitus}}'' Application},
  shorttitle = {Emotion Dynamics and Tinnitus},
  author = {Probst, Thomas and Pryss, R{\"u}diger and Langguth, Berthold and Schlee, Winfried},
  year = {2016},
  month = aug,
  journal = {Scientific Reports},
  volume = {6},
  number = {1},
  pages = {31166},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/srep31166},
  abstract = {It is well established that emotions influence tinnitus, but the role of emotion dynamics remains unclear. The present study investigated emotion dynamics in N\,=\,306 users of the ``TrackYourTinnitus'' application who completed the Mini-Tinnitus Questionnaire (Mini-TQ) at one assessment point and provided complete data on at least five assessment points for the following state variables: tinnitus loudness, tinnitus distress, arousal, valence. The repeated arousal and valence ratings were used for two operationalizations of emotion dynamics: intra-individual variability of affect intensity (pulse) as well as intra-individual variability of affect quality (spin). Pearson correlation coefficients showed that the Mini-TQ was positively correlated with pulse (r\,=\,0.19; p\,{$<$}\,0.05) as well as with spin (r\,=\,0.12; p\,{$<$}\,0.05). Multilevel models revealed the following results: increases in tinnitus loudness were more strongly associated with increases in tinnitus distress at higher levels of pulse as well as at higher levels of spin (both p\,{$<$}\,0.05), whereby increases in tinnitus loudness correlated even stronger with increases in tinnitus distress when both pulse as well as spin were high (p\,{$<$}\,0.05). Moreover, increases in spin were associated with a less favorable time course of tinnitus loudness (p\,{$<$}\,0.05). To conclude, equilibrating emotion dynamics might be a potential target in the prevention and treatment of tinnitus.},
  copyright = {2016 The Author(s)},
  langid = {english}
}

@article{probsta,
  title = {Does {{Tinnitus Depend}} on {{Time-of-Day}}? {{An Ecological Momentary Assessment Study}} with the ``{{TrackYourTinnitus}}'' {{Application}}},
  shorttitle = {Does {{Tinnitus Depend}} on {{Time-of-Day}}?},
  author = {Probst, Thomas and Pryss, R{\"u}diger C. and Langguth, Berthold and Rauschecker, Josef P. and Schobel, Johannes and Reichert, Manfred and Spiliopoulou, Myra and Schlee, Winfried and Zimmermann, Johannes},
  year = {2017},
  month = aug,
  journal = {Frontiers in Aging Neuroscience},
  volume = {9},
  pages = {253},
  issn = {1663-4365},
  doi = {10.3389/fnagi.2017.00253},
  abstract = {Only few previous studies used ecological momentary assessments to explore the time-of-day-dependence of tinnitus. The present study used data from the mobile application ``TrackYourTinnitus'' to explore whether tinnitus loudness and tinnitus distress fluctuate within a 24-h interval. Multilevel models were performed to account for the nested structure of assessments (level 1: 17,209 daily life assessments) nested within days (level 2: 3,570 days with at least three completed assessments), and days nested within participants (level 3: 350 participants). Results revealed a time-of-day-dependence of tinnitus. In particular, tinnitus was perceived as louder and more distressing during the night and early morning hours (from 12 a.m. to 8 a.m.) than during the upcoming day. Since previous studies suggested that stress (and stress-associated hormones) show a circadian rhythm and this might influence the time-of-day-dependence of tinnitus, we evaluated whether the described results change when statistically controlling for subjectively reported stress-levels. Correcting for subjective stress-levels, however, did not change the result that tinnitus (loudness and distress) was most severe at night and early morning. These results show that time-of-day contributes to the level of both tinnitus loudness and tinnitus distress. Possible implications of our results for the clinical management of tinnitus are that tailoring the timing of therapeutic interventions to the circadian rhythm of individual patients (chronotherapy) might be promising.},
  pmcid = {PMC5539131},
  pmid = {28824415}
}

@inproceedings{pryss,
  title = {Mobile {{Crowd Sensing Services}} for {{Tinnitus Assessment}}, {{Therapy}}, and {{Research}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Mobile Services}}},
  author = {Pryss, R. and Reichert, M. and Langguth, B. and Schlee, W.},
  year = {2015},
  month = jun,
  pages = {352--359},
  issn = {2329-6453},
  doi = {10.1109/MobServ.2015.55},
  abstract = {Tinnitus, the phantom sensation of sound, is a highly prevalent disorder that is difficult to treat, i.e., Available treatments are only effective for patient subgroups. Sufficiently large and qualitative longitudinal data sets, which aggregate the individuals' demographic and clinical characteristics, together with their response to specific therapeutic interventions, would therefore facilitate evidence-based treatment suggestions for individual patients. Currently, clinical trials are the standard instrument for realizing evidence-based medicine. However, the related information gathering is limited. For example, clinical trials try to reduce the complexity of the individual case by generating homogeneous groups to obtain significant results. From the latter, individual treatment decisions are inferred. A complementary approach would be to assess the effect of specific interventions in large samples considering the individual peculiarity of each subject. This allows providing individualized treatment decisions. Recently, mobile crowd sensing emerged as an approach for collecting large and ecological valid datasets at rather low costs. By providing mobile crowd sensing services to large numbers of patients, large datasets can be gathered cheaply on a daily basis. In the TrackYourTinnitus project, we implemented a mobile crowd sensing platform to reveal new medical aspects on tinnitus and its treatment. Additionally, we work on mobile services exploring approaches for understanding tinnitus and for improving its diagnostic and therapeutic management. We present the TrackYourTinnitus platform as well as its goals, architecture and preliminary results. Overall, the platform and its mobile services offer promising perspectives for tinnitus research and treatment.},
  keywords = {Androids,clinical trial,Clinical trials,evidence-based medicine,health care,Humanoid robots,Mobile communication,mobile computing,mobile crowd sensing,mobile crowdsensing services,Mobile handsets,mobile healthcare application,Operating systems,patient treatment,Sensors,sound phantom sensation,tinnitus,tinnitus assessment,tinnitus therapy,tinnitus variablity,TrackYourTinnitus platform}
}

@inproceedings{pryssa,
  title = {Mobile {{Crowd Sensing}} in {{Clinical}} and {{Psychological Trials}} \textendash{} {{A Case Study}}},
  booktitle = {2015 {{IEEE}} 28th {{International Symposium}} on {{Computer-Based Medical Systems}}},
  author = {Pryss, R. and Reichert, M. and Herrmann, J. and Langguth, B. and Schlee, W.},
  year = {2015},
  month = jun,
  pages = {23--24},
  issn = {2372-9198},
  doi = {10.1109/CBMS.2015.26},
  abstract = {Many highly prevalent diseases (e.g., tinnitus, migraine, chronic pain) are difficult to treat and universally effective treatments are missing. Available treatments are only effective in patient subgroups, i.e., medical doctors and patients have to figure out which therapy might be helpful in the patient's situation. Sufficiently large and qualitative longitudinal data sets, however, would be desirable to facilitate evidence-based treatment decisions for individual patients. On one hand, traditional sensing techniques (i.e., clinical trials) have many merits enabling evidence-based medicine. On the other, they have inherent limitations. First, clinical trials are very cost- and labour-intensive. Second, the traditional approach aims at reducing ecological heterogeneity to enable the investigation of homogeneous subsamples. Recently, a new paradigm emerged that offers promising perspectives for collecting large amounts of longitudinal patient data \textendash{} Mobile Crowd Sensing. By utilizing smart mobile devices of a large number of patients, health information can be gathered from large patient collections as well as at many different time points and in various real life environmental situations. In the Track Your Tinnitus project, we implemented such a mobile crowd sensing platform to reveal new medical aspects about tinnitus with a particular focus on the variability of tinnitus over time depending on the environmental situation. In this paper, the current project status as well as first lessons learned from running the mobile application for twelve months are presented. In turn, the lessons learned are discussed in the context of the new perspectives offered by mobile crowd sensing in the medical field.},
  keywords = {biomedical communication,biomedical measurement,clinical trial,clinical trials,Clinical trials,diseases,ecological heterogeneity,evidence-based medicine,evidence-based treatment decisions,health care,health information,homogeneous subsamples,large longitudinal data sets,medical computing,medical field,mobile application,Mobile communication,mobile computing,mobile crowd sensing,mobile crowd sensing platform,Mobile handsets,mobile healthcare application,patient subgroups,patient treatment,psychological trial,psychological trials,psychology,Psychology,qualitative longitudinal data sets,real life environmental situations,Sensors,smart mobile devices,smart phones,time 12 month,time points,tinnitus,tinnitus variablity,Track Your Tinnitus project,traditional sensing techniques,universally effective treatments}
}

@manual{rcoreteam,
  type = {Manual},
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2021},
  address = {{Vienna, Austria}},
  organization = {{R Foundation for Statistical Computing}}
}

@article{schadt,
  title = {Computational {{Advances}} in {{Maximum Likelihood Methods}} for {{Molecular Phylogeny}}},
  author = {Schadt, Eric E. and Sinsheimer, Janet S. and Lange, Kenneth},
  year = {1998},
  month = mar,
  journal = {Genome Research},
  volume = {8},
  number = {3},
  pages = {222--233},
  publisher = {{Cold Spring Harbor Lab}},
  issn = {1088-9051, 1549-5469},
  doi = {10.1101/gr.8.3.222},
  abstract = {We have developed a generalization of Kimura's Markov chain model for base substitution at a single nucleotide site. This generalized model incorporates more flexible transition rates and consequently allows irreversible as well as reversible chains. Because the model embodies just the right amount of symmetry, it permits explicit calculation of finite-time transition probabilities and equilibrium distributions. The model also meshes well with maximum likelihood methods for phylogenetic analysis. Quick calculation of likelihoods and their derivatives can be carried out by adapting Baum's forward and backward algorithms from the theory of hidden Markov chains. Analysis of HIV sequence data illustrates the speed of the algorithms on trees with many contemporary taxa. Analysis of some of Lake's data on the origin of the eukaryotic nucleus contrasts the reversible and irreversible versions of the model.},
  langid = {english},
  pmid = {9521926}
}

@article{schliehe,
  title = {On the Application of Mixed Hidden {{Markov}} Models to Multiple Behavioural Time Series},
  author = {{Schliehe-Diecks}, S. and Kappeler, P. M. and Langrock, R.},
  year = {2012},
  journal = {Interface Focus},
  volume = {2},
  number = {2},
  eprint = {http://rsfs.royalsocietypublishing.org/content/2/2/180.full.pdf},
  pages = {180--189},
  publisher = {{Royal Society}},
  issn = {2042-8898},
  doi = {10.1098/rsfs.2011.0077}
}

@article{turner,
  title = {Direct Maximization of the Likelihood of a Hidden {{Markov}} Model},
  author = {Turner, Rolf},
  year = {2008},
  month = may,
  journal = {Computational Statistics \& Data Analysis},
  volume = {52},
  number = {9},
  pages = {4147--4160},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2008.01.029},
  abstract = {Ever since the introduction of hidden Markov models by Baum and his co-workers, the method of choice for fitting such models has been maximum likelihood via the EM algorithm. In recent years it has been noticed that the gradient and Hessian of the log likelihood of hidden Markov and related models may be calculated in parallel with a filtering process by which the likelihood may be calculated. Various authors have used, or suggested the use of, this idea in order to maximize the likelihood directly, without using the EM algorithm. In this paper we discuss an implementation of such an approach. We have found that a straightforward implementation of Newton's method sometimes works but is unreliable. A form of the Levenberg\textendash Marquardt algorithm appears to provide excellent reliability. Two rather complex examples are given for applying this algorithm to the fitting of hidden Markov models. In the first a better than 6-fold increase in speed over the EM algorithm was achieved. The second example turned out to be problematic (somewhat interestingly) in that the maximum likelihood estimator appears to be inconsistent. Whatever its merit, this estimator is calculated much faster by Levenberg\textendash Marquardt than by EM. We also compared the Levenberg\textendash Marquardt algorithm, applied to the first example, with a generic numerical maximization procedure. The Levenberg\textendash Marquardt algorithm appeared to perform almost three times better than the generic procedure, even when analytic derivatives were provided, and 19 times better when they were not provided.}
}

@article{venzon,
  title = {A {{Method}} for {{Computing Profile-Likelihood-Based Confidence Intervals}}},
  author = {Venzon, D. J. and Moolgavkar, S. H.},
  year = {1988},
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  volume = {37},
  number = {1},
  pages = {87--94},
  issn = {1467-9876},
  doi = {10.2307/2347496},
  abstract = {The method of constructing confidence regions based on the generalised likelihood ratio statistic is well known for parameter vectors. A similar construction of a confidence interval for a single entry of a vector can be implemented by repeatedly maximising over the other parameters. We present an algorithm for finding these confidence interval endpoints that requires less computation. It employs a modified Newton-Raphson iteration to solve a system of equations that defines the endpoints.},
  copyright = {\textcopyright{} 1988 Royal Statistical Society},
  langid = {english},
  keywords = {Confidence intervals,Profile likelihood}
}

@article{visser,
  title = {Confidence Intervals for Hidden {{Markov}} Model Parameters},
  author = {Visser, Ingmar and Raijmakers, Maartje E. J. and Molenaar, Peter C. M.},
  year = {2000},
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {53},
  number = {2},
  pages = {317--327},
  publisher = {{Blackwell Publishing Ltd}},
  issn = {2044-8317},
  doi = {10.1348/000711000159240},
  keywords = {bootstrap,confidence intervals,finite differences approximation,invariance,profile likelihood}
}

@article{wald,
  title = {Tests of {{Statistical Hypotheses Concerning Several Parameters When}} the {{Number}} of {{Observations}} Is {{Large}}},
  author = {Wald, Abraham},
  year = {1943},
  journal = {Transactions of the American Mathematical Society},
  volume = {54},
  number = {3},
  pages = {426--482},
  publisher = {{American Mathematical Society}},
  issn = {0002-9947},
  doi = {10.2307/1990256}
}

@book{zucchini,
  title = {Hidden Markov Models for Time Series: An Introduction Using r, Second Edition},
  author = {Zucchini, W. and MacDonald, I.L. and Langrock, R.},
  year = {2016},
  series = {Chapman \& {{Hall}}/{{CRC}} Monographs on Statistics \& Applied Probability},
  publisher = {{CRC Press}},
  isbn = {978-1-4822-5384-9}
}


