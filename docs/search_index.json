[["index.html", "A gentle tutorial of accelerated parameter and confidence interval estimation for Hidden Markov Models using Template Model Builder Summary", " A gentle tutorial of accelerated parameter and confidence interval estimation for Hidden Markov Models using Template Model Builder Timothée Bacri timothee.bacri@uib.no Jan Bulla jan.bulla@uib.no Geir Berentsen geir.berentsen@nhh.no Sondre Hølleland sondre.hoelleland@hi.no 2021-08-11 Summary Welcome ! This website aims to accompany the reader of [Link to official article] The files used in this repository are available in https://github.com/timothee-bacri/HMM_with_TMB. For a description of the files and the directory structure, please read Directory Structure. Note that only the folders code, functions, and data contain files used in the article. The other folders and files relate to this Gitbook and are not described in Directory Structure. "],["license.html", "License", " License This project is licensed under the xxxx License - see LICENSE.md for details. "],["state-inf.html", "Chapter 1 State inference 1.1 Prepare the model 1.2 Setup 1.3 Log-forward probabilities 1.4 Log-backward probabilities 1.5 Local decoding 1.6 Forecast 1.7 Global decoding", " Chapter 1 State inference 1.1 Prepare the model library(TMB) source(&quot;functions/utils.R&quot;) load(&quot;data/fetal-lamb.RData&quot;) TMB::compile(&quot;code/poi_hmm.cpp&quot;) dyn.load(dynlib(&quot;code/poi_hmm&quot;)) lamb_data &lt;- lamb m &lt;- 2 TMB_data &lt;- list(x = lamb_data, m = m) lambda &lt;- c(1, 3) gamma &lt;- matrix(c(0.8, 0.2, 0.2, 0.8), byrow = TRUE, nrow = m) parameters &lt;- pois.HMM.pn2pw(m, lambda, gamma) obj_tmb &lt;- MakeADFun(TMB_data, parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE) mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn, gradient = obj_tmb$gr, hessian = obj_tmb$he) 1.2 Setup 1.2.1 Define variables Given an optimized MakeADFun object obj, we need to setup some variables to compute the probabilities detailed below. # Retrieve the objects at ML value adrep &lt;- obj_tmb$report(obj_tmb$env$last.par.best) delta &lt;- adrep$delta gamma &lt;- adrep$gamma emission_probs &lt;- adrep$emission_probs n &lt;- adrep$n m &lt;- length(delta) mllk &lt;- adrep$mllk Note that \\({\\boldsymbol\\lambda}\\) is not needed as we already have access to the emission probabilities. 1.2.2 Emission/output probabilities Emission probabilities (also called output probabilities) are the conditional probabilities of the data given a state. Using the notation in the article Section 3.1, emission probabilities are the conditional distributions \\(p_i(x) = \\text{P}(X_t = x \\vert C_t = i) = \\frac{e^{-\\lambda_i} \\lambda_i^x}{x!}\\). We store all these in a data frame where each row and column represent a data and a state respectively. We compute these probabilities in C++ rather than in R because it is faster and not more complicated to write // Evaluate conditional distribution: Put conditional // probabilities of observed x in n times m matrix // (one column for each state, one row for each datapoint): matrix&lt;Type&gt; emission_probs(n, m); matrix&lt;Type&gt; row1vec(1, m); row1vec.setOnes(); for (int i = 0; i &lt; n; i++) { if (x[i] != x[i]) { // f != f returns true if and only if f is NaN. // Replace missing values (NA in R, NaN in C++) with 1 emission_probs.row(i) = row1vec; } else { emission_probs.row(i) = dpois(x[i], lambda, false); } } Nevertheless, we also need to compute them in R for [#forecast] # Calculate emission probabilities get.emission.probs &lt;- function(data, lambda) { n &lt;- length(data) m &lt;- length(lambda) emission_probs &lt;- matrix(0, nrow = n, ncol = m) for (i in 1:n) { if (is.na(data[i])) { emission_probs[i, ] &lt;- rep(1, m) } else { emission_probs[i, ] &lt;- dpois(data[i], lambda) } } return(emission_probs) } 1.3 Log-forward probabilities The forward probabilities have been detailed in TO FIX LINK_TO_hmm_likelihood!!!!!!!!!!!!!!!!!. We show here a way to compute the log of the forward probabilities, using a scaling scheme defined by (see Zucchini, MacDonald, and Langrock 2016, 66 and p.334). # Compute log-forward probabilities (scaling used) lalpha &lt;- matrix(NA, m, n) foo &lt;- delta * emission_probs[1, ] sumfoo &lt;- sum(foo) lscale &lt;- log(sumfoo) foo &lt;- foo / sumfoo lalpha[, 1] &lt;- log(foo) + lscale for (i in 2:n) { foo &lt;- foo %*% gamma * emission_probs[i, ] sumfoo &lt;- sum(foo) lscale &lt;- lscale + log(sumfoo) foo &lt;- foo / sumfoo lalpha[, i] &lt;- log(foo) + lscale } # lalpha contains n=240 columns, so we only display 5 for readability lalpha[, 1:5] ## [,1] [,2] [,3] [,4] [,5] ## [1,] -0.2920639 -0.5591179 -0.8265948 -1.094088 -1.361583 ## [2,] -6.4651986 -7.7716769 -8.1146145 -8.385232 -8.652850 1.4 Log-backward probabilities The backward probabilities have been defined in the same section Zucchini, MacDonald, and Langrock (2016, p~.67 and p~.334). # Compute log-backwards probabilities (scaling used) lbeta &lt;- matrix(NA, m, n) lbeta[, n] &lt;- rep(0, m) foo &lt;- rep (1 / m, m) lscale &lt;- log(m) for (i in (n - 1):1) { foo &lt;- gamma %*% (emission_probs[i + 1, ] * foo) lbeta[, i] &lt;- log(foo) + lscale sumfoo &lt;- sum(foo) foo &lt;- foo / sumfoo lscale &lt;- lscale + log(sumfoo) } # lbeta contains n=240 columns, so we only display 4 for readability lbeta[, 1:4] ## [,1] [,2] [,3] [,4] ## [1,] -177.2275 -176.9600 -176.6925 -176.4250 ## [2,] -178.3456 -178.0781 -177.8099 -177.5253 1.5 Local decoding The smoothing probabilities are defined in (Zucchini, MacDonald, and Langrock 2016, 87 and p.336) as \\[ \\text{P}(C_t = i \\vert X^{(n)}) = x^{(n)}) = \\frac{\\alpha_t(i) \\beta_t(i)}{L_n} \\] # Compute conditional state probabilities, smoothing probabilities stateprobs &lt;- matrix(NA, ncol = n, nrow = m) llk &lt;- - mllk for(i in 1:n) { stateprobs[, i] &lt;- exp(lalpha[, i] + lbeta[, i] - llk) } Local decoding is a straightforward maximum of the smoothing probabilities. # Most probable states (local decoding) ldecode &lt;- rep(NA, n) for (i in 1:n) { ldecode[i] &lt;- which.max(stateprobs[, i]) } ldecode ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [47] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 1 ## [93] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [139] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [185] 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [231] 1 1 1 1 1 1 1 1 1 1 1.6 Forecast The forecast distribution or h-step-ahead-probabilities as well as its implementation in R is detailed in (Zucchini, MacDonald, and Langrock 2016, 85 and p.337) Then, \\[ \\text{P}(X_{n+h} = x \\vert X^{(n)} = x^{(n)}) = \\frac{{\\boldsymbol\\alpha}_T {\\boldsymbol\\gamma}^h {\\boldsymbol P}(x) {\\boldsymbol 1}&#39;}{{\\boldsymbol\\alpha}_T {\\boldsymbol 1}&#39;} = {\\boldsymbol\\Phi}_T {\\boldsymbol\\gamma}^h {\\boldsymbol P}(x) {\\boldsymbol 1}&#39; \\] An implementation of this, using a scaling scheme is # Number of steps h &lt;- 1 # Values for which we want the forecast probabilities xf &lt;- 0:50 nxf &lt;- length(xf) dxf &lt;- matrix(0, nrow = h, ncol = nxf) foo &lt;- delta * emission_probs[1, ] sumfoo &lt;- sum(foo) lscale &lt;- log(sumfoo) foo &lt;- foo / sumfoo for (i in 2:n) { foo &lt;- foo %*% gamma * emission_probs[i, ] sumfoo &lt;- sum( foo) lscale &lt;- lscale + log(sumfoo) foo &lt;- foo / sumfoo } emission_probs_xf &lt;- get.emission.probs(xf, lambda) for (i in 1:h) { foo &lt;- foo %*% gamma for (j in 1:m) { dxf[i, ] &lt;- dxf[i, ] + foo[j] * emission_probs_xf[, j] } } # dxf contains n=240 columns, so we only display 4 for readability dxf[, 1:4] ## [1] 0.36414482 0.36531388 0.18441055 0.06322379 1.7 Global decoding The Viterbi algorithm is detailed in (Zucchini, MacDonald, and Langrock 2016, 88 and p.334). It calculates the sequence of states \\((i_1^*, \\ldots, i_T^*)\\) which maximizes the conditional probability of all states simultaneously, i.e. \\[ (i_1^*, \\ldots, i_n^*) = \\mathop{\\mathrm{argmax}}_{i_1, \\ldots, i_n \\in \\{1, \\ldots, m \\}} \\text{P}(C_1 = i_1, \\ldots, C_n = i_n \\vert X^{(n)} = x^{(n)}). \\] An implementation of it is xi &lt;- matrix(0, n, m) foo &lt;- delta * emission_probs[1, ] xi[1, ] &lt;- foo / sum(foo) for (i in 2:n) { foo &lt;- apply(xi[i - 1, ] * gamma, 2, max) * emission_probs[i, ] xi[i, ] &lt;- foo / sum(foo) } iv &lt;- numeric(n) iv[n] &lt;- which.max(xi[n, ]) for (i in (n - 1):1){ iv[i] &lt;- which.max(gamma[, iv[i + 1]] * xi[i, ]) } iv ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [47] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 1 ## [93] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [139] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [185] 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [231] 1 1 1 1 1 1 1 1 1 1 References "],["principles-of-using-tmb-for-mle.html", "Chapter 2 Principles of using TMB for MLE 2.1 Setting up TMB 2.2 Getting started 2.3 Extending the C++ nll", " Chapter 2 Principles of using TMB for MLE 2.1 Setting up TMB In order to run the scripts and example code, you first need to set up TMB by going through the following steps: Install Rtools Install the R-package TMB (Kristensen 2021) by executing the following code in R: install.packages(&quot;TMB&quot;) (Optional) Setup error debugging in RStudio by running the command TMB:::setupRstudio() Advanced use is detailed in https://kaskr.github.io/adcomp/_book/Tutorial.html 2.2 Getting started Let \\({\\boldsymbol x}\\) and \\({\\boldsymbol y}\\) denote the predictor and response vector, respectively, both of length \\(n\\). For a simple linear regression model with intercept \\(a\\) and slope \\(b\\), the negative log-likelihood equals \\[\\begin{equation*} - \\log L(a, b, \\sigma^2) = - \\sum_{i=1}^n \\log(\\phi(y_i; a + bx_i, \\sigma^2)), \\end{equation*}\\] where \\(\\phi(\\cdot; \\mu, \\sigma^2)\\) corresponds to the density function of the univariate normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). The use of TMB requires the (negative) log-likelihood function to be coded in C++ under a specific template, which is then loaded into R. The minimization of this function and other post-processing procedures are all carried out in R. Therefore, we require two files. The first file, named code/linreg.cpp, is written in C++ and defines the objective function, i.e. the negative log-likelihood (nll) function of the linear model. #include &lt;TMB.hpp&gt; //import the TMB template template&lt;class Type&gt; Type objective_function&lt;Type&gt;::operator() () { DATA_VECTOR(y); // Data vector y passed from R DATA_VECTOR(x); // Data vector x passed from R PARAMETER(a); // Parameter a passed from R PARAMETER(b); // Parameter b passed from R PARAMETER(tsigma); // Parameter sigma (transformed, on log-scale) // passed from R // Transform tsigma back to natural scale Type sigma = exp(tsigma); // Declare negative log-likelihood Type nll = - sum(dnorm(y, a + b * x, sigma, true)); // Necessary for inference on sigma, not only tsigma ADREPORT(sigma); return nll; } The second file needed is written in R and serves for compiling the nll function defined above and carrying out the estimation procedure by numerical optimization of the nll function. The .R file (shown below) carries out the compilation of the C++ file and minimization of the nll function: # Loading TMB package library(TMB) # Compilation. The compiler returns 0 if the compilation of # the cpp file was successful TMB::compile(&quot;code/linreg.cpp&quot;) ## [1] 0 # Dynamic loading of the compiled cpp file dyn.load(dynlib(&quot;code/linreg&quot;)) # Generate the data for our test sample set.seed(123) data &lt;- list(y = rnorm(20) + 1:20, x = 1:20) parameters &lt;- list(a = 0, b = 0, tsigma = 0) # Instruct TMB to create the likelihood function obj_linreg &lt;- MakeADFun(data, parameters, DLL = &quot;linreg&quot;, silent = TRUE) # Optimization of the objective function with nlminb mod_linreg &lt;- nlminb(obj_linreg$par, obj_linreg$fn, obj_linreg$gr, obj_linreg$he) mod_linreg$par ## a b tsigma ## 0.31009251 0.98395536 -0.05814649 Now that the optimization is taken care of, we can display the estimates with standard errors using the sdreport function. sdreport(obj_linreg) ## sdreport(.) result ## Estimate Std. Error ## a 0.31009251 0.43829087 ## b 0.98395536 0.03658782 ## tsigma -0.05814649 0.15811383 ## Maximum gradient component: 1.300261e-10 If we use summary on this object, we also get the variables we have passed to ADREPORT in the code/linreg.cpp file. In this example, this is only the residual standard deviation; sigma. summary(sdreport(obj_linreg)) ## Estimate Std. Error ## a 0.31009251 0.43829087 ## b 0.98395536 0.03658782 ## tsigma -0.05814649 0.15811383 ## sigma 0.94351172 0.14918225 The select argument restricts the output to variables passed by ADREPORT(variable_name); in the cpp file. As we will see in Wald-type, this lets us derive confidence intervals for these natural parameters easily. summary(sdreport(obj_linreg), select = &quot;report&quot;) ## Estimate Std. Error ## sigma 0.9435117 0.1491823 Certainly, you would not build a TMB model to fit a linear regression. We can use standard R functions for that. Therefore, we use stats::lm on the same data and compare the estimates to those obtained by TMB. rbind( &quot;lm&quot; = lm(y ~ x, data = data)$coef, # linear regression using R &quot;TMB&quot; = mod_linreg$par[1:2] # intercept and slope from TMB fit ) ## (Intercept) x ## lm 0.3100925 0.9839554 ## TMB 0.3100925 0.9839554 As we can see, the parameter estimates are exactly the same. 2.3 Extending the C++ nll Sometimes it is useful to write subroutines as a separate function that can be used within your TMB likelihood function. This can increase readability of your code and reduce the number of lines of code in your main cpp file. Writing extra files to define functions compatible with TMB requires some care, as these must follow TMBs template. To illustrate how this works, we add a separate function to the linreg.cpp example. The following function does not do anything meaningful, but is just meant to show you how you can add write an additional function and load it into your C++. We start by writing the subroutine function as a separate file called functions/utils_linreg.cpp. template&lt;class Type&gt; matrix&lt;Type&gt; function_example(matrix&lt;Type&gt; mat_example) { // This function doesn&#39;t do anything meaningful matrix&lt;Type&gt; mat(2, 3); mat.setOnes(); mat.row(1) &lt;&lt; 5, 5, 5; mat(0, 2) = mat.row(1).sum(); return mat; } template&lt;class Type&gt; Type logistic(Type tsigma) { return 1 / (1 + exp (-tsigma)); } We then import it into code/linreg_extended.cpp. #include &lt;TMB.hpp&gt; //import the TMB template #include &quot;../functions/utils_linreg_extended.cpp&quot; template&lt;class Type&gt; Type objective_function&lt;Type&gt;::operator() () { DATA_VECTOR(y); // Data vector y passed from R DATA_VECTOR(x); // Data vector x passed from R PARAMETER(a); // Parameter a passed from R PARAMETER(b); // Parameter b passed from R PARAMETER(tsigma); // Parameter sigma (transformed by logistic function) // constrained on [0, 1] // passed from R // Transform tsigma back to natural scale with our external function Type sigma = logistic(tsigma); // Declare negative log-likelihood Type nll = - sum(dnorm(y, a + b * x, sigma, true)); // Necessary for inference on sigma, not only tsigma ADREPORT(sigma); /* This is a useless example to show how to manipulate matrices * in C++ * This creates a matrix of 2 rows and 3 columns. * The Eigen library is used to manipulate vectors, arrays, matrices... */ matrix&lt;Type&gt; mat_example(2, 3); mat_example &lt;&lt; 1, 2, 3, 4, 5, 6; mat_example.setOnes(); mat_example(1, 2) = sigma; matrix&lt;Type&gt; mat_example2 = function_example(mat_example); // This lets us retrieve any variables in a nice format REPORT(mat_example); REPORT(mat_example2); return nll; } And eventually we can use it in R, as shown in this minimal example. # Loading TMB package library(TMB) # Compilation. The compiler returns 0 if the compilation of # the cpp file was successful TMB::compile(&quot;code/linreg_extended.cpp&quot;) ## [1] 0 # Dynamic loading of the compiled cpp file dyn.load(dynlib(&quot;code/linreg_extended&quot;)) # Generate the data for our test sample set.seed(123) sigma &lt;- 0.6 data &lt;- list(y = rnorm(20, sd = sigma) + 1:20, x = 1:20) tsigma &lt;- log(sigma / (1 - sigma)) # Logit transform parameters &lt;- list(a = 0, b = 0, tsigma = 0.1) # Instruct TMB to create the likelihood function obj_linreg &lt;- MakeADFun(data, parameters, DLL = &quot;linreg_extended&quot;, silent = TRUE) # Optimization of the objective function with nlminb mod_linreg &lt;- nlminb(obj_linreg$par, obj_linreg$fn, obj_linreg$gr, obj_linreg$he) # Objects returned by ADREPORT() in C++ summary(sdreport(obj_linreg), select = &quot;report&quot;) ## Estimate Std. Error ## sigma 0.566107 0.08950934 # Object obj_linreg$report() ## $mat_example ## [,1] [,2] [,3] ## [1,] 1 1 1.0000000 ## [2,] 1 1 0.5658614 ## ## $mat_example2 ## [,1] [,2] [,3] ## [1,] 1 1 15 ## [2,] 5 5 5 Note that when you are writing the C++ nll file, compiling the file again may lead to an error. This is because the dynamic library (the .dll file) is already loaded, and cannot be overwritten. To prevent this, it is useful to either to manually unload the file manually via the code dyn.unload(dynlib(&quot;code/linreg_extended&quot;)) or to restart the R session via the menu Session-&gt;Restart R (shortcut Ctrl+Shift+F10 on Windows). Note that restarting the session unload all packages from the session, and will require you to load the TMB package again. References "],["parameter-estimation.html", "Chapter 3 Parameter estimation 3.1 Utility functions 3.2 Negative log-likelihood in TMB", " Chapter 3 Parameter estimation In this section we recall basic concepts underlying parameter estimation for HMMs via direct numerical optimization of the likelihood. In terms of notation, we stay as close as possible to Zucchini, MacDonald, and Langrock (2016), where a more detailed presentation is available. 3.1 Utility functions 3.1.1 In TMB Defining the negative log-likelihood function requires transforming the working parameters into their natural form. We define the function gamma.w2n to perform this transformation. The function stat.dist handles computing the stationary distribution, while delta.w2n can be used if a non-stationary distribution is provided. They are defined in functions/utils.cpp // Function transforming working parameters in initial distribution // to natural parameters template&lt;class Type&gt; vector&lt;Type&gt; delta_w2n(int m, vector&lt;Type&gt; tdelta) { vector&lt;Type&gt; delta(m); vector&lt;Type&gt; foo(m); if (m == 1) return Type(1); // set first element to one. // Fill in the last m - 1 elements with working parameters // and take exponential foo &lt;&lt; Type(1), tdelta.exp(); // normalize delta = foo / foo.sum(); return delta; } // Function transforming the working parameters in TPM to // natural parameters (w2n) template&lt;class Type&gt; matrix&lt;Type&gt; gamma_w2n(int m, vector&lt;Type&gt; tgamma) { // Construct m x m identity matrix matrix&lt;Type&gt; gamma(m, m); gamma.setIdentity(); if (m == 1) return gamma; // Fill offdiagonal elements with working parameters column-wise: int idx = 0; for (int i = 0; i &lt; m; i++){ for (int j = 0; j &lt; m; j++){ if (j != i){ // Fill gamma according to mapping and take exponential gamma(j, i) = tgamma.exp()(idx); idx++; } } } // Normalize each row: vector&lt;Type&gt; cs = gamma.rowwise().sum(); for (int i = 0; i &lt; m; i++) gamma.row(i) /= cs[i]; return gamma; } // Function computing the stationary distribution of a Markov chain template&lt;class Type&gt; vector&lt;Type&gt; stat_dist(int m, matrix&lt;Type&gt; gamma) { // Construct stationary distribution matrix&lt;Type&gt; I(m, m); matrix&lt;Type&gt; U(m, m); matrix&lt;Type&gt; row1vec(1, m); U = U.setOnes(); I = I.setIdentity(); row1vec.setOnes(); matrix&lt;Type&gt; A = I - gamma + U; matrix&lt;Type&gt; Ainv = A.inverse(); matrix&lt;Type&gt; deltamat = row1vec * Ainv; vector&lt;Type&gt; delta = deltamat.row(0); return delta; } Transforming the Poisson means into their natural form can be done simply with the exp function and doesnt require a dedicated function. 3.1.2 In R 3.1.2.1 Parameter transformation While TMB requires functions to transform working parameters to their natural form, pre-processing in R requires the inverse transformation. Functions to achieve this are available in (Zucchini, MacDonald, and Langrock 2016, 51) and are displayed here with explaining comments for convenience. # Transform Poisson natural parameters to working parameters pois.HMM.pn2pw &lt;- function(m, lambda, gamma, delta = NULL, stationary = TRUE) { tlambda &lt;- log(lambda) foo &lt;- log(gamma / diag(gamma)) tgamma &lt;- as.vector(foo[!diag(m)]) if (stationary) { # If tdelta is set to NULL and returned in the list, # it will cause issues when optimizing with TMB return(list(tlambda = tlambda, tgamma = tgamma)) } else { tdelta &lt;- log(delta[- 1] / delta[1]) # TMB requires a list return(list(tlambda = tlambda, tgamma = tgamma, tdelta = tdelta)) } } This can be broken down into sub-functions if necessary. For the \\({\\boldsymbol\\Gamma}\\) part, we introduce gamma.n2w below. gamma.n2w &lt;- function(m, gamma){ foo &lt;- log(gamma / diag(gamma)) tgamma &lt;- as.vector(foo[!diag(m)]) return(tgamma) } In the case where a non-stationary distribution is specified, transforming \\({\\boldsymbol\\delta}\\) is necessary. For this we use the delta.n2w function. # Function to transform natural parameters to working delta.n2w &lt;- function(m, delta){ tdelta &lt;- log(delta[- 1] / delta[1]) return(tdelta) } When assuming a stationary distribution, computing it can be done via the following stat.dist function. # Compute the stationary distribution of a Markov chain # with transition probability gamma stat.dist &lt;- function(gamma) { m &lt;- nrow(gamma) return(solve(t(diag(m) - gamma + 1), rep(1, m))) } (Zucchini, MacDonald, and Langrock 2016, 51) shows that calculating the stationary distribution can be achieved by solving the equation below for \\({\\boldsymbol\\delta}\\), where \\({\\boldsymbol I}_m\\) is the \\(m*m\\) identity matrix, \\({\\boldsymbol U}\\) is a \\(m*m\\) matrix of ones, and \\({\\boldsymbol 1}\\) is a row vector of ones. \\[ {\\boldsymbol\\delta}({\\boldsymbol I}_m - {\\boldsymbol\\Gamma} + {\\boldsymbol U}) = {\\boldsymbol 1} \\] 3.1.2.2 Label switching As the model gets estimated each time, we do not impose by default an order for the states. This can lead to the label switching problem, where states arent ordered the same way in each model and are grouped incorrectly. The issue can be relevant when comparing results of different optimizers, initial parameters, or classes of models. To address the problem, we reorder the states by ascending Poisson means. Sorting the means is direct, however re-ordering the TPM is not as straightforward. To do so, we take the permutations of the states given by the sorted Poisson means, and permute each row index and column index to its new value. A function to achieve this is named pois.HMM.label.order and presented below. # Relabel states by increasing Poisson means pois.HMM.label.order &lt;- function(m, lambda, gamma, delta = NULL, lambda_std_error = NULL, gamma_std_error = NULL, delta_std_error = NULL) { # gamma_vector_indices is used to calculate the indices of the reordered TPM gamma as a vector # for reordering the rows of the complete CI data.frame used for the article. gamma_vector_indices &lt;- 1:(m ^ 2) gamma_vector_matrix &lt;- matrix(gamma_vector_indices, nrow = m, ncol = m) ordered_gamma_vector_matrix &lt;- matrix(0, nrow = m, ncol = m) # Get the indexes of the sorted states # according to ascending lambda # sorted_lambda contains the permutations needed ordered_lambda_indices &lt;- sort(lambda, index.return = TRUE)$ix ordered_lambda &lt;- lambda[ordered_lambda_indices] # Reorder the TPM according to the switched states # in the sorted lambda ordered_gamma &lt;- matrix(0, nrow = m, ncol = m) for (col in 1:m) { new_col &lt;- which(ordered_lambda_indices == col) for (row in 1:m) { new_row &lt;- which(ordered_lambda_indices == row) ordered_gamma[row, col] &lt;- gamma[new_row, new_col] # Reorder the vector TPM ordered_gamma_vector_matrix[row, col] &lt;- gamma_vector_matrix[new_row, new_col] } } # Same for the TPM&#39;s standard errors ordered_gamma_std_error &lt;- NULL if (!is.null(gamma_std_error)) { ordered_gamma_std_error &lt;- matrix(0, nrow = m, ncol = m) for (col in 1:m) { new_col &lt;- which(ordered_lambda_indices == col) for (row in 1:m) { new_row &lt;- which(ordered_lambda_indices == row) ordered_gamma_std_error[row, col] &lt;- gamma_std_error[new_row, new_col] } } } # Reorder the stationary distribution if it is provided # Generate it otherwise if (is.null(delta)) { ordered_delta &lt;- stat.dist(ordered_gamma) } else { ordered_delta &lt;- delta[ordered_lambda_indices] } # Reorder the standard errors ordered_lambda_std_error &lt;- lambda_std_error[ordered_lambda_indices] ordered_delta_std_error &lt;- delta_std_error[ordered_lambda_indices] # The vector is assumed filled column-wise instead of row-wise, because column-wise is the default way R handles matrix to vector conversion. # Change to row-wise if needed by replacing ordered_gamma_vector_matrix with t(ordered_gamma_vector_matrix), or add byrow=TRUE to &quot;ordered_gamma_vector_matrix &lt;- matrix(0, nrow = m, ncol = m)&quot; # We don&#39;t use it in case there is a bug, but it makes logical sense that it should work ordered_gamma_vector_matrix &lt;- as.numeric(ordered_gamma_vector_matrix) result &lt;- list(lambda = ordered_lambda, gamma = ordered_gamma, delta = ordered_delta, lambda_std_error = ordered_lambda_std_error, gamma_std_error = ordered_gamma_std_error, delta_std_error = ordered_delta_std_error, ordered_lambda_indices = ordered_lambda_indices, ordered_gamma_vector_indices = ordered_gamma_vector_matrix, # delta and lambda are the same size, so they are ordered the same way ordered_delta_indices = ordered_lambda_indices) # Remove the NULL elements result[sapply(result, is.null)] &lt;- NULL return(result) } We will go through an example to better understand the process. For readability, the TPM is filled with standard row and column indexes instead of probabilities. lambda &lt;- c(3, 1, 2) gamma &lt;- matrix(c(11, 12, 13, 21, 22, 23, 31, 32, 33), byrow = TRUE, ncol = 3) pois.HMM.label.order(m = 3, lambda, gamma) ## $lambda ## [1] 1 2 3 ## ## $gamma ## [,1] [,2] [,3] ## [1,] 33 31 32 ## [2,] 13 11 12 ## [3,] 23 21 22 ## ## $delta ## [1] -0.032786885 0.016393443 -0.008196721 ## ## $ordered_lambda_indices ## [1] 2 3 1 ## ## $ordered_gamma_vector_indices ## [1] 9 7 8 3 1 2 6 4 5 ## ## $ordered_delta_indices ## [1] 2 3 1 State 1 has been relabeled state 3, state 3 became state 2, and state 2 became state 1. Another way to address this issue is by parameterizing in terms of non-negative increments \\(\\lambda_j - \\lambda_{j-1}\\) with \\(\\lambda_0 \\equiv 0\\), as explained by Zucchini, MacDonald, and Langrock (2016Section 7.1.1 p. 112). However, Bulla and Berzel (2008Section 3.2 p. 7) shows this can impose optimization issues: over all series, the simplest parameterization, i.e., the use of log-transformed state-dependent parameters, leads to the best results as regards the number of failures and the convergence to the global maximum. These utility functions or subroutines are not complicated, but as you can see, they would cloud up your main code. Therefore, we put them in functions we can call from our main program. 3.2 Negative log-likelihood in TMB The Poisson HMM negative log-likelihood function in R can be written the following way. #include &lt;TMB.hpp&gt; #include &quot;../functions/utils.cpp&quot; // Likelihood for a poisson hidden markov model. template&lt;class Type&gt; Type objective_function&lt;Type&gt;::operator() () { // Data DATA_VECTOR(x); // timeseries vector DATA_INTEGER(m); // Number of states m // Parameters PARAMETER_VECTOR(tlambda); // conditional log_lambdas&#39;s PARAMETER_VECTOR(tgamma); // m(m-1) working parameters of TPM // Uncomment only when using a non-stationary distribution //PARAMETER_VECTOR(tdelta); // transformed stationary distribution, // Transform working parameters to natural parameters: vector&lt;Type&gt; lambda = tlambda.exp(); matrix&lt;Type&gt; gamma = gamma_w2n(m, tgamma); // Construct stationary distribution vector&lt;Type&gt; delta = stat_dist(m, gamma); // If using a non-stationary distribution, use this instead //vector&lt;Type&gt; delta = delta_w2n(m, tdelta); // Get number of timesteps (n) int n = x.size(); // Evaluate conditional distribution: Put conditional // probabilities of observed x in n times m matrix // (one column for each state, one row for each datapoint): matrix&lt;Type&gt; emission_probs(n, m); matrix&lt;Type&gt; row1vec(1, m); row1vec.setOnes(); for (int i = 0; i &lt; n; i++) { if (x[i] != x[i]) { // f != f returns true if and only if f is NaN. // Replace missing values (NA in R, NaN in C++) with 1 emission_probs.row(i) = row1vec; } else { emission_probs.row(i) = dpois(x[i], lambda, false); } } // Corresponds to Zucchini&#39;s book page 333 matrix&lt;Type&gt; foo, P; Type mllk, sumfoo, lscale; if (m == 1) { mllk = - emission_probs.col(0).array().log().sum(); // Use adreport on variables we are interested in: ADREPORT(lambda); ADREPORT(gamma); ADREPORT(delta); // Things we need for local decoding REPORT(lambda); REPORT(gamma); REPORT(delta); return mllk; } foo = (delta * vector&lt;Type&gt;(emission_probs.row(0))).matrix(); sumfoo = foo.sum(); lscale = log(sumfoo); foo.transposeInPlace(); foo /= sumfoo; for (int i = 2; i &lt;= n; i++) { P = emission_probs.row(i - 1); foo = ((foo * gamma).array() * P.array()).matrix(); sumfoo = foo.sum(); lscale += log(sumfoo); foo /= sumfoo; } mllk = -lscale; // Use adreport on variables for which we want standard errors ADREPORT(lambda); ADREPORT(gamma); ADREPORT(delta); // Variables we need for local decoding and in a convenient format REPORT(lambda); REPORT(gamma); REPORT(delta); REPORT(n); REPORT(emission_probs); REPORT(mllk); return mllk; } The case where \\(m = 1\\) doesnt involve a hidden state, and thus is a Poisson regression instead of a Poisson HMM. Nonetheless, TMB also accelerates its estimation and may be useful to the reader. References "],["using-tmb.html", "Chapter 4 Using TMB 4.1 Likelihood function 4.2 Preparing data and functions 4.3 Modeling 4.4 Nested models", " Chapter 4 Using TMB 4.1 Likelihood function The function is defined in code/poi_hmm.cpp #include &lt;TMB.hpp&gt; #include &quot;../functions/utils.cpp&quot; // Likelihood for a poisson hidden markov model. template&lt;class Type&gt; Type objective_function&lt;Type&gt;::operator() () { // Data DATA_VECTOR(x); // timeseries vector DATA_INTEGER(m); // Number of states m // Parameters PARAMETER_VECTOR(tlambda); // conditional log_lambdas&#39;s PARAMETER_VECTOR(tgamma); // m(m-1) working parameters of TPM // Uncomment only when using a non-stationary distribution //PARAMETER_VECTOR(tdelta); // transformed stationary distribution, // Transform working parameters to natural parameters: vector&lt;Type&gt; lambda = tlambda.exp(); matrix&lt;Type&gt; gamma = gamma_w2n(m, tgamma); // Construct stationary distribution vector&lt;Type&gt; delta = stat_dist(m, gamma); // If using a non-stationary distribution, use this instead //vector&lt;Type&gt; delta = delta_w2n(m, tdelta); // Get number of timesteps (n) int n = x.size(); // Evaluate conditional distribution: Put conditional // probabilities of observed x in n times m matrix // (one column for each state, one row for each datapoint): matrix&lt;Type&gt; emission_probs(n, m); matrix&lt;Type&gt; row1vec(1, m); row1vec.setOnes(); for (int i = 0; i &lt; n; i++) { if (x[i] != x[i]) { // f != f returns true if and only if f is NaN. // Replace missing values (NA in R, NaN in C++) with 1 emission_probs.row(i) = row1vec; } else { emission_probs.row(i) = dpois(x[i], lambda, false); } } // Corresponds to Zucchini&#39;s book page 333 matrix&lt;Type&gt; foo, P; Type mllk, sumfoo, lscale; if (m == 1) { mllk = - emission_probs.col(0).array().log().sum(); // Use adreport on variables we are interested in: ADREPORT(lambda); ADREPORT(gamma); ADREPORT(delta); // Things we need for local decoding REPORT(lambda); REPORT(gamma); REPORT(delta); return mllk; } foo = (delta * vector&lt;Type&gt;(emission_probs.row(0))).matrix(); sumfoo = foo.sum(); lscale = log(sumfoo); foo.transposeInPlace(); foo /= sumfoo; for (int i = 2; i &lt;= n; i++) { P = emission_probs.row(i - 1); foo = ((foo * gamma).array() * P.array()).matrix(); sumfoo = foo.sum(); lscale += log(sumfoo); foo /= sumfoo; } mllk = -lscale; // Use adreport on variables for which we want standard errors ADREPORT(lambda); ADREPORT(gamma); ADREPORT(delta); // Variables we need for local decoding and in a convenient format REPORT(lambda); REPORT(gamma); REPORT(delta); REPORT(n); REPORT(emission_probs); REPORT(mllk); return mllk; } 4.2 Preparing data and functions Before we can fit the model, we need to load some necessary packages and data files. We also need to compile the C++ code and load the functions into our working environment in R. We start by loading the necessary packages and the utility R functions. # Load TMB and optimization packages library(TMB) library(optimr) # Load the parameter transformation function source(&quot;functions/utils.R&quot;) Next, we need to compile the c++ code for computing the likelihood and its gradients. Once it is compiled, we can load the TMB likelihood object into R  making it available from R. # Run the `C++` file containing the TMB code TMB::compile(&quot;code/poi_hmm.cpp&quot;) # Load it dyn.load(dynlib(&quot;code/poi_hmm&quot;)) The data are part of a large data set collected with the Track Your Tinnitus (TYT) mobile application, a detailed description of which is presented in (Pryss, Reichert, Herrmann, et al. 2015; Pryss, Reichert, Langguth, et al. 2015). We analyze 87 successive days of the arousal variable, which is measured on a discrete scale. Higher values correspond to a higher degree of excitement, lower values to a more calm emotional state (for details, see Probst et al. 2016). The data can be loaded by a simple call. load(&quot;data/tinnitus.RData&quot;) Figure 4.1 present the raw data in Table 5.1, which are also available for download at data/tinnitus.RData. TYT data. Observations collected by the TYT app on 87 successive days (from left to right). 6 5 3 6 4 3 5 6 6 6 4 6 6 4 6 6 6 6 6 4 6 5 6 7 6 5 5 5 7 6 5 6 5 6 6 6 5 6 7 7 6 7 6 6 6 6 5 7 6 1 6 0 2 1 6 7 6 6 6 5 5 6 6 2 5 0 1 1 1 2 3 1 3 1 3 0 1 1 1 4 1 4 1 2 2 2 0 Figure 4.1: Plot of observations from TYT app data for 87 succesive days. 4.3 Modeling Initialization of the number of states and starting (or initial) values for the optimization First, the number of states needs to be determined. As explained by (Pohle et al. 2017b), (Pohle et al. 2017a), and (Zucchini, MacDonald, and Langrock 2016 Section 6) (to name only a few), usually one would first fit models with a different number of states. Then, these models are evaluated e.g. by means of model selection criteria (as carried out by Leroux and Puterman 1992) or prediction performance (Celeux and Durand 2008). Since the results reported by (Leroux and Puterman 1992) show that a two-state model is preferred by the BIC, we focus on this model only here - although other choices would be possible, e.g. the AIC selects a three-state model. The list object TMB_data contains the data and the number of states. # Model with 2 states m &lt;- 2 TMB_data &lt;- list(x = tinn_data, m = m) Secondly, initial values for the optimization procedure need to be defined. Although we will apply unconstrained optimization, we initialize the natural parameters, because this is much more intuitive and practical than handling the working parameters. # Generate initial set of parameters for optimization lambda &lt;- c(1, 3) gamma &lt;- matrix(c(0.8, 0.2, 0.2, 0.8), byrow = TRUE, nrow = m) Transformation from natural to working parameters The previously created initial values are transformed and stored in the list parameters for the optimization procedure. # Turn them into working parameters parameters &lt;- pois.HMM.pn2pw(m, lambda, gamma) Creation of the TMB negative log-likelihood function with its derivatives This object, stored as obj_tmb requires the data, the initial values, and the previously created DLL as input. Setting argument silent = TRUE disables tracing information and is only used here to avoid excessive output. obj_tmb &lt;- MakeADFun(TMB_data, parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE) This object also contains the previously defined initial values as a vector (par) rather than a list. The negative log-likelihood (fn), its gradient (gr), and Hessian (he) are functions of the parameters (in vector form) while the data are considered fixed: obj_tmb$par ## tlambda tlambda tgamma tgamma ## 0.000000 1.098612 -1.386294 -1.386294 obj_tmb$fn(obj_tmb$par) ## [1] 228.3552 obj_tmb$gr(obj_tmb$par) ## [,1] [,2] [,3] [,4] ## [1,] -3.60306 -146.0336 10.52832 -1.031706 obj_tmb$he(obj_tmb$par) ## [,1] [,2] [,3] [,4] ## [1,] 1.902009 -5.877900 -1.3799682 2.4054017 ## [2,] -5.877900 188.088247 -4.8501589 2.3434284 ## [3,] -1.379968 -4.850159 9.6066700 -0.8410438 ## [4,] 2.405402 2.343428 -0.8410438 0.7984216 Execution of the optimization For this step we rely again on the optimizer implemented in the nlminb function. The arguments, i.e.~ initial values for the parameters and the function to be optimized, are extracted from the previously created TMB object. mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn) # Check that it converged successfully mod_tmb$convergence == 0 ## [1] TRUE It is noteworthy that various alternatives to nlminb exist. Nevertheless, we focus on this established optimization routine because of its high speed of convergence. Obtaining ML estimates Obtaining the ML estimates of the natural parameters together with their standard errors is possible by using the previously introduced command sdreport. Recall that this requires the parameters of interest to be treated by the ADREPORT statement in the C++ part. It should be noted that the presentation of the set of parameters gamma below results from a column-wise representation of the TPM. summary(sdreport(obj_tmb, par.fixed = mod_tmb$par), &quot;report&quot;) ## Estimate Std. Error ## lambda 1.63641070 0.27758294 ## lambda 5.53309626 0.31876141 ## gamma 0.94980192 0.04374682 ## gamma 0.02592209 0.02088689 ## gamma 0.05019808 0.04374682 ## gamma 0.97407791 0.02088689 ## delta 0.34054163 0.23056401 ## delta 0.65945837 0.23056401 Note that the table above also contains estimation results for \\({\\boldsymbol\\delta}\\) and accompanying standard errors, although \\({\\boldsymbol\\delta}\\) is not estimated, but derived from \\({\\boldsymbol\\Gamma}\\). We provide further details on this aspect in Confidence intervals. The value of the nll function in the minimum found by the optimizer can also be extracted directly from the object mod_tmb by accessing the list element objective: mod_tmb$objective ## [1] 168.5361 Use exact gradient and Hessian In the optimization above we already benefited from an increased speed due to the evaluation of the nll in C++ compared to the forward algorithm being executed entirely in R. However, the use of TMB also permits to introduce the gradient and/or the Hessian computed by TMB into the optimization procedure. This is in general advisable, because TMB provides an exact value of both gradient and Hessian up to machine precision, which is superior to approximations used by optimizing procedure. Similar to the nll, both quantities can be extracted directly from the TMB object obj_tmb: # The negative log-likelihood is accessed by the objective # attribute of the optimized object mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn, gradient = obj_tmb$gr, hessian = obj_tmb$he) mod_tmb$objective ## [1] 168.5361 Note that passing the exact gradient and Hessian as provided by TMB to nlminb leads to the same minimum, i.e. value of the nll function, here. Is it noteworthy that inconsistencies can happen in the estimates due to computer approximations. The stationary distribution is a vector of probabilities and should sum to 1. However, it doesnt always behave as expected. adrep &lt;- summary(sdreport(obj_tmb), &quot;report&quot;) estimate_delta &lt;- adrep[rownames(adrep) == &quot;delta&quot;, &quot;Estimate&quot;] sum(estimate_delta) ## [1] 1 sum(estimate_delta) == 1 # The sum is displayed as 1 but is not 1 ## [1] FALSE As noted on (Zucchini, MacDonald, and Langrock 2016, 15960), ``the row sums of \\({\\boldsymbol\\Gamma}\\) will only approximately equal 1, and the components of the vector \\({\\boldsymbol\\delta}\\) will only approximately total 1. This can be remedied by scaling the vector \\({\\boldsymbol\\delta}\\) and each row of \\({\\boldsymbol\\Gamma}\\) to total 1. We do not remedy this issue because it provides no benefit to us, but this may lead to surprising results when checking equality. This is likely due to machine approximations when numbers far apart from each other interact together. In R, a small number is not 0 but is treated as 0 when added to a much larger number. This can result in incoherent findings when checking equality between 2 numbers. 1e-100 == 0 # Small numbers are 0 ## [1] FALSE (1 + 1e-100) == 1 ## [1] TRUE 4.4 Nested models 4.4.1 Principle As a first step in building a nested model, we arbitrarily fix \\(\\lambda_1\\) to the value 1. # Get the previous values, and fix some fixed_par_lambda &lt;- lambda fixed_par_lambda[1] &lt;- 1 # Transform them into working parameters new_parameters &lt;- pois.HMM.pn2pw(m = m, lambda = fixed_par_lambda, gamma = gamma) Then, we instruct TMB to read these custom parameters. We indicate fixed values by mapping them to NA values, whereas the variable values need to be mapped to different factor variables. Lastly, we specify this mapping with the map argument when making the TMB object. map &lt;- list(tlambda = as.factor(c(NA, 1)), tgamma = as.factor(c(2, 3))) fixed_par_obj_tmb &lt;- MakeADFun(TMB_data, new_parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE, map = map) Estimating of the model and displaying the results is performed as usual fixed_par_mod_tmb &lt;- nlminb(start = fixed_par_obj_tmb$par, objective = fixed_par_obj_tmb$fn, gradient = fixed_par_obj_tmb$gr, hessian = fixed_par_obj_tmb$he) summary(sdreport(fixed_par_obj_tmb), &quot;report&quot;) ## Estimate Std. Error ## lambda 1.00000000 0.00000000 ## lambda 5.50164872 0.30963641 ## gamma 0.94561055 0.04791050 ## gamma 0.02655944 0.02133283 ## gamma 0.05438945 0.04791050 ## gamma 0.97344056 0.02133283 ## delta 0.32810136 0.22314460 ## delta 0.67189864 0.22314460 Note that the standard error of \\(\\lambda_1\\) is zero, because it is no longer considered a variable parameter and does not enter the optimization procedure. 4.4.2 Limits This method cannot work in general for working parameters which are not linked to a single natural parameter. This is because only working parameters can be fixed with this method, but the working parameters of the TPM are not each linked to a single natural parameter. As an example, fixing the natural parameter \\(\\gamma_{11}\\) is not equivalent to fixing any working parameter \\(\\tau_{ij}\\). Hence, the TPM cannot in general be fixed. However, if conditions on the natural parameters can be translated to conditions on the working parameters, then there should not be any issue. We will show in the next section that a type of general constraints on the TPM can be carried out. In addition, overcoming this restriction should be possible via constrained optimization. 4.4.3 Parameter equality constraints It is noteworthy that more complex constraints are possible as well For example, to impose equality constraints (such as \\(\\gamma_{11} = \\gamma_{22}\\)), the corresponding factor level has to be identical for the concerned entries. As a reminder, we defined the working parameters via \\[ \\tau_{ij} = \\log\\left(\\frac{\\gamma_{ij}}{1 - \\sum_{k \\neq i} \\gamma_{ik}}\\right) = \\log(\\gamma_{ij}/\\gamma_{ii}), \\text{ for } i \\neq j \\] With a 2 state HMM, the constraint \\(\\gamma_{11} = \\gamma_{22}\\) is equivalent to \\(\\gamma_{12} = \\gamma_{21}\\). Thus, the constraint can be transformed into \\(\\tau_{12} = \\log(\\gamma_{12}/\\gamma_{11}) = \\log(\\gamma_{21}/\\gamma_{22}) = \\tau_{21}\\). The mapping parameters must be set to a common factor to be forced equal. map &lt;- list(tlambda = as.factor(c(1, 2)), tgamma = as.factor(c(3, 3))) fixed_par_obj_tmb &lt;- MakeADFun(TMB_data, parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE, map = map) fixed_par_mod_tmb &lt;- nlminb(start = fixed_par_obj_tmb$par, objective = fixed_par_obj_tmb$fn, gradient = fixed_par_obj_tmb$gr, hessian = fixed_par_obj_tmb$he) # Results + check that the constraint is respected results &lt;- summary(sdreport(fixed_par_obj_tmb), &quot;report&quot;) tpm &lt;- matrix(results[rownames(results) == &quot;gamma&quot;, &quot;Estimate&quot;], nrow = m, ncol = m, byrow = FALSE) # Transformations are column-wise by default, be careful! tpm ## [,1] [,2] ## [1,] 0.96759216 0.03240784 ## [2,] 0.03240784 0.96759216 tpm[1, 1] == tpm[2, 2] ## [1] TRUE Similar complex constraints on the TPM can also be setup for HMMs with over two states. However, it appears that it can only be done in general when the constraint involves natural parameters of the same row with the exception of a 2 state model. We have not found a way to easily implement equality constraints between natural TPM parameters of different rows. Possible solutions are constrained optimization and different parametrization. As an example, we will look at a three-state HMM with the constraint \\(\\gamma_{12} = \\gamma_{13}\\). # Model with 2 states m &lt;- 3 TMB_data &lt;- list(x = tinn_data, m = m) # Initial set of parameters lambda &lt;- c(1, 3, 5) gamma &lt;- matrix(c(0.8, 0.1, 0.1, 0.1, 0.8, 0.1, 0.1, 0.1, 0.8), byrow = TRUE, nrow = m) # Turn them into working parameters parameters &lt;- pois.HMM.pn2pw(m, lambda, gamma) # Build the TMB object obj_tmb &lt;- MakeADFun(TMB_data, parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE) # Optimize mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn, gradient = obj_tmb$gr, hessian = obj_tmb$he) # Check convergence mod_tmb$convergence == 0 ## [1] TRUE # Results summary(sdreport(obj_tmb), &quot;report&quot;) ## Estimate Std. Error ## lambda 8.281290e-01 5.418824e-01 ## lambda 1.705082e+00 2.949769e-01 ## lambda 5.514886e+00 3.080238e-01 ## gamma 5.516919e-01 3.132490e-01 ## gamma 4.596623e-09 5.115944e-05 ## gamma 2.749570e-02 2.164849e-02 ## gamma 1.989160e-01 2.698088e-01 ## gamma 9.772963e-01 3.436716e-02 ## gamma 2.453591e-10 2.730433e-06 ## gamma 2.493922e-01 2.546636e-01 ## gamma 2.270369e-02 3.436717e-02 ## gamma 9.725043e-01 2.164849e-02 ## delta 3.836408e-02 3.757898e-02 ## delta 3.361227e-01 3.144598e-01 ## delta 6.255132e-01 3.063220e-01 The transformed constraint becomes \\(\\tau_{12} = \\log(\\gamma_{12}/\\gamma_{11}) = \\log(\\gamma_{13}/\\gamma_{11}) = \\tau_{13}\\). We need to be careful how we specify the constraint, because the vector tgamma will be converted into a matrix column-wise since this is Rs default way to handle matrix-vector conversions. The tgamma matrix looks naturally like \\[\\begin{pmatrix} &amp;\\tau_{12}&amp;\\tau_{13}\\\\ \\tau_{21}&amp; &amp;\\tau_{23}\\\\ \\tau_{31}&amp;\\tau_{32}&amp; \\end{pmatrix}\\] As a vector in R, it becomes \\(\\left(\\tau_{21}, \\tau_{31}, \\tau_{12}, \\tau_{32}, \\tau_{13}, \\tau_{23}\\right)\\). Therefore, the constraint needs to be placed on the \\(3^{rd}\\) and \\(5^{th}\\) vector parameter. map &lt;- list(tlambda = as.factor(c(1, 2, 3)), tgamma = as.factor(c(4, 5, 6, 7, 6, 8))) fixed_par_obj_tmb &lt;- MakeADFun(TMB_data, parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE, map = map) fixed_par_mod_tmb &lt;- nlminb(start = fixed_par_obj_tmb$par, objective = fixed_par_obj_tmb$fn, gradient = fixed_par_obj_tmb$gr, hessian = fixed_par_obj_tmb$he) # Results + check that the constraint is respected results &lt;- summary(sdreport(fixed_par_obj_tmb), &quot;report&quot;) tpm &lt;- matrix(results[rownames(results) == &quot;gamma&quot;, &quot;Estimate&quot;], nrow = m, ncol = m, byrow = FALSE) # Transformations are column-wise by default, be careful! tpm ## [,1] [,2] [,3] ## [1,] 5.447127e-01 2.276437e-01 0.22764365 ## [2,] 2.718007e-09 9.753263e-01 0.02467374 ## [3,] 2.710078e-02 2.005408e-10 0.97289922 tpm[1, 2] == tpm[1, 3] ## [1] TRUE Equality constraints involving a diagonal member of the TPM are simpler to specify: the constraint \\(\\gamma_{i,j} = \\gamma_{i,i}\\) becomes transformed to \\(\\tau_{i,j} = 1\\) and this can be specified in the same way the Poisson mean \\(\\lambda_1\\) was fixed. References "],["confidence-intervals.html", "Chapter 5 Confidence intervals 5.1 Wald-type 5.2 Parametric bootstrap 5.3 Negative log-likelihood profile", " Chapter 5 Confidence intervals From the parameters ML estimates, we generate new data and re-estimate the parameters times. From that list of new estimates we can get the 2.5th and 97.5th percentiles and get 95% confidence intervals for the parameters. We show below how to derive confidence intervals using TMB, a likelihood profile based method, and parametric bootstrap, based on the 2 state Poisson HMM estimates. For all three methods, we require a model, so we generate a 2-state Poisson HMM based on the TYT dataset set.seed(123) library(TMB) TMB::compile(&quot;code/poi_hmm.cpp&quot;) ## [1] 0 dyn.load(dynlib(&quot;code/poi_hmm&quot;)) ## Warning: 31 external pointers will be removed source(&quot;functions/utils.R&quot;) m &lt;- 2 load(&quot;data/tinnitus.RData&quot;) TMB_data &lt;- list(x = tinn_data, m = m) # Initial set of parameters lambda_init &lt;- c(1, 3) gamma_init &lt;- matrix(c(0.8, 0.2, 0.2, 0.8), byrow = TRUE, nrow = m) # Turn them into working parameters parameters &lt;- pois.HMM.pn2pw(m, lambda_init, gamma_init) # Build the TMB object obj_tmb &lt;- MakeADFun(TMB_data, parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE) # Optimize mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn, gradient = obj_tmb$gr, hessian = obj_tmb$he) # MLE ML_working_estimates &lt;- mod_tmb$par # obj_tmb$env$last.par.best is also possible ML_natural_estimates &lt;- obj_tmb$report(ML_working_estimates) lambda &lt;- ML_natural_estimates$lambda gamma &lt;- ML_natural_estimates$gamma delta &lt;- ML_natural_estimates$delta 5.1 Wald-type Now that we have a model estimated via TMB, we can derive Wald-type (Wald 1943) confidence intervals. For example, the \\((1 - \\alpha) \\%\\) CI for \\(\\lambda_1\\) is given by \\(\\lambda_1 \\pm z_{1-\\alpha/2} * \\sigma_{\\lambda_1}\\) where \\(z_{x}\\) is the \\(x\\)-percentile of the standard normal distribution, and \\(\\sigma_{\\lambda_1}\\) is the standard error of \\(\\lambda_1\\) obtained via the delta method. First, we require the standard errors. We can retrieve them from the MakeADFun object. The standard errors of the working parameters tlambda and tgamma can be retrieved without needing to add ADREPORT in the C++ file. However, it is usually more interesting to access the standard errors of the natural parameters lambda, gamma and delta. This requires adding a few lines to the C++ file to produce these standard errors, as detailed in [Getting started with a linear regression]. Be careful: adrep lists gamma column-wise adrep &lt;- summary(sdreport(obj_tmb), &quot;report&quot;) adrep ## Estimate Std. Error ## lambda 1.63641100 0.27758296 ## lambda 5.53309576 0.31876147 ## gamma 0.94980209 0.04374676 ## gamma 0.02592204 0.02088688 ## gamma 0.05019791 0.04374676 ## gamma 0.97407796 0.02088688 ## delta 0.34054200 0.23056437 ## delta 0.65945800 0.23056437 Standard errors for \\(\\hat{{\\boldsymbol\\lambda}}\\) rows &lt;- rownames(adrep) == &quot;lambda&quot; lambda &lt;- adrep[rows, &quot;Estimate&quot;] lambda_std_error &lt;- adrep[rows, &quot;Std. Error&quot;] lambda ## lambda lambda ## 1.636411 5.533096 lambda_std_error ## lambda lambda ## 0.2775830 0.3187615 Standard errors for \\(\\hat{{\\boldsymbol\\Gamma}}\\) rows &lt;- rownames(adrep) == &quot;gamma&quot; gamma &lt;- adrep[rows, &quot;Estimate&quot;] gamma &lt;- matrix(gamma, ncol = m) gamma_std_error &lt;- adrep[rows, &quot;Std. Error&quot;] gamma_std_error &lt;- matrix(gamma_std_error, nrow = m, ncol = m) gamma ## [,1] [,2] ## [1,] 0.94980209 0.05019791 ## [2,] 0.02592204 0.97407796 gamma_std_error ## [,1] [,2] ## [1,] 0.04374676 0.04374676 ## [2,] 0.02088688 0.02088688 Standard errors for \\(\\hat{{\\boldsymbol\\delta}}\\) rows &lt;- rownames(adrep) == &quot;delta&quot; delta &lt;- adrep[rows, &quot;Estimate&quot;] delta_std_error &lt;- adrep[rows, &quot;Std. Error&quot;] delta ## delta delta ## 0.340542 0.659458 delta_std_error ## delta delta ## 0.2305644 0.2305644 5.2 Parametric bootstrap 5.2.1 Generating data In order to perform a parametric bootstrap, we need to be able to generate data from a set of parameters. For readability and code maintenance, it is conventional to store procedures that will be used more than once into functions. The data-generating function is defined in (Zucchini, MacDonald, and Langrock 2016, Section A.1.5 p.333). pois.HMM.generate.sample &lt;- function(ns,mod) { mvect &lt;- 1:mod$m state &lt;- numeric(ns) state[1] &lt;- sample(mvect, 1, prob = mod$delta) for (i in 2:ns) { state[i] &lt;- sample(mvect, 1, prob = mod$gamma[state[i - 1], ]) } x &lt;- rpois(ns, lambda = mod$lambda[state]) return(x) } Further, one can retrieve the state sequence used to generate the data by performing an adjustment: # Generate a random sample from a HMM pois.HMM.generate.sample &lt;- function(ns, mod) { mvect &lt;- 1:mod$m state &lt;- numeric(ns) state[1] &lt;- sample(mvect, 1, prob = mod$delta) for (i in 2:ns) { state[i] &lt;- sample(mvect, 1, prob = mod$gamma[state[i - 1], ]) } x &lt;- rpois(ns, lambda = mod$lambda[state]) return(list(data = x, state = state)) } The results are returned in a list to simplify usage, as the intuitive way (c(x, state)) would append the state sequence to the data. In practice, HMMs sometimes cannot be estimated on generated samples. To deal with this, we can generate a new sample as long as HMMs cannot be estimated on it with the help of this more robust function which can easily be adapted to different needs. Natural parameter estimates are returned for convenience. The argument test_marqLevAlg decides if convergence of marqLevAlg is required. The argument std_error decides if standard errors are returned along with TMBs estimates. This function relies on the custom function TMB.estimate which is a wrapper of nlminb made for Poisson HMM estimation via TMB. # Generate a random sample from a HMM pois.HMM.generate.estimable.sample &lt;- function(ns, mod, testing_params, params_names = PARAMS_NAMES, test_marqLevAlg = FALSE, std_error = FALSE, label_switch = FALSE) { if(anyNA(c(ns, mod, testing_params))) { stop(&quot;Some parameters are missing in pois.HMM.generate.estimable.sample&quot;) } # Count occurrences for each error failure &lt;- c(&quot;state_number&quot; = 0, &quot;TMB_null&quot; = 0, &quot;TMB_converge&quot; = 0, &quot;TMB_G_null&quot; = 0, &quot;TMB_G_converge&quot; = 0, &quot;TMB_H_null&quot; = 0, &quot;TMB_H_converge&quot; = 0, &quot;TMG_GH_null&quot; = 0, &quot;TMG_GH_converge&quot; = 0, &quot;marqLevAlg_null&quot; = 0, &quot;marqLevAlg_converge&quot; = 0, &quot;NA_value&quot; = 0) m &lt;- mod$m # Loop as long as there is an issue with nlminb repeat { mod_temp &lt;- NULL #simulate the data new_data &lt;- pois.HMM.generate.sample(ns = ns, mod = mod) # If the number of states generated is different from m, discard the data if (length(unique(new_data$state)) != m) { failure[&quot;state_number&quot;] &lt;- failure[&quot;state_number&quot;] + 1 next } TMB_benchmark_data &lt;- list(x = new_data$data, m = m) testing_w_params &lt;- pois.HMM.pn2pw(m = m, lambda = testing_params$lambda, gamma = testing_params$gamma, delta = testing_params$delta) # Test TMB suppressWarnings(mod_temp &lt;- TMB.estimate(TMB_data = TMB_benchmark_data, parameters = testing_w_params, std_error = std_error)) # If nlminb doesn&#39;t reach any result, discard the data if (is.null(mod_temp)) { failure[&quot;TMB_null&quot;] &lt;- failure[&quot;TMB_null&quot;] + 1 next } # If nlminb doesn&#39;t converge successfully, discard the data if (mod_temp$convergence != 0) { failure[&quot;TMB_converge&quot;] &lt;- failure[&quot;TMB_converge&quot;] + 1 next } # Test TMB_G suppressWarnings(mod_temp &lt;- TMB.estimate(TMB_data = TMB_benchmark_data, parameters = testing_w_params, gradient = TRUE, std_error = std_error)) # If nlminb doesn&#39;t reach any result, discard the data if (is.null(mod_temp)) { failure[&quot;TMB_G_null&quot;] &lt;- failure[&quot;TMB_G_null&quot;] + 1 next } # If nlminb doesn&#39;t converge successfully, discard the data if (mod_temp$convergence != 0) { failure[&quot;TMB_G_converge&quot;] &lt;- failure[&quot;TMB_G_converge&quot;] + 1 next } # Test TMB_H suppressWarnings(mod_temp &lt;- TMB.estimate(TMB_data = TMB_benchmark_data, parameters = testing_w_params, hessian = TRUE, std_error = std_error)) # If nlminb doesn&#39;t reach any result, discard the data if (is.null(mod_temp)) { failure[&quot;TMB_H_null&quot;] &lt;- failure[&quot;TMB_H_null&quot;] + 1 next } # If nlminb doesn&#39;t converge successfully, discard the data if (mod_temp$convergence != 0) { failure[&quot;TMB_H_converge&quot;] &lt;- failure[&quot;TMB_H_converge&quot;] + 1 next } # Test TMB_GH suppressWarnings(mod_temp &lt;- TMB.estimate(TMB_data = TMB_benchmark_data, parameters = testing_w_params, gradient = TRUE, hessian = TRUE, std_error = std_error)) # If nlminb doesn&#39;t reach any result, discard the data if (is.null(mod_temp)) { failure[&quot;TMB_GH_null&quot;] &lt;- failure[&quot;TMB_GH_null&quot;] + 1 next } # If nlminb doesn&#39;t converge successfully, discard the data if (mod_temp$convergence != 0) { failure[&quot;TMB_GH_converge&quot;] &lt;- failure[&quot;TMB_GH_converge&quot;] + 1 next } # Test marqLevAlg # marqLevAlg sometimes doesn&#39;t converge either, discard the data in these cases if (test_marqLevAlg == TRUE) { testing_w_params &lt;- unlist(testing_w_params) marq &lt;- tryCatch({ marqLevAlg(b = testing_w_params, fn = mod_temp$obj$fn, gr = mod_temp$obj$gr, hess = mod_temp$obj$he, maxiter = 10000) }, error = function(e) { return() }) # If nlminb doesn&#39;t reach any result, discard the data if (is.null(marq)) { failure[&quot;marqLevAlg_null&quot;] &lt;- failure[&quot;marqLevAlg_null&quot;] + 1 next } # If marqLevAlg doesn&#39;t converge successfully, discard the data if (marq$istop != 1) { failure[&quot;marqLevAlg_converge&quot;] &lt;- failure[&quot;marqLevAlg_converge&quot;] + 1 next } } if (label_switch == TRUE) { # Label switching # In practice, we don&#39;t use it for the tests because it doesn&#39;t sort mod$obj # Sorting them could lead to bugs and errors, and although it is better in principle, # there is little benefit in practice. The benefit would be to infer profile likelihood CIs # more easily when calculating coverage probabilities. natural_parameters &lt;- pois.HMM.label.order(m = m, lambda = mod_temp$lambda, gamma = mod_temp$gamma, delta = mod_temp$delta, lambda_std_error = mod_temp$lambda_std_error, gamma_std_error = mod_temp$gamma_std_error, delta_std_error = mod_temp$delta_std_error) } else { natural_parameters &lt;- list(m = m, lambda = mod_temp$lambda, gamma = mod_temp$gamma, delta = mod_temp$delta, lambda_std_error = mod_temp$lambda_std_error, gamma_std_error = mod_temp$gamma_std_error, delta_std_error = mod_temp$delta_std_error) } # If some parameters are NA for some reason, discard the data if (anyNA(natural_parameters[params_names], recursive = TRUE)) { failure[&quot;NA_value&quot;] &lt;- failure[&quot;NA_value&quot;] + 1 next } # If everything went well, end the &quot;repeat&quot; loop break } return(c(data = list(new_data$data), natural_parameters = list(natural_parameters), mod = list(mod_temp), failure = list(failure))) } 5.2.2 Bootstrap set.seed(123) library(TMB) TMB::compile(&quot;code/poi_hmm.cpp&quot;) dyn.load(dynlib(&quot;code/poi_hmm&quot;)) source(&quot;functions/utils.R&quot;) m &lt;- 2 load(&quot;data/tinnitus.RData&quot;) TMB_data &lt;- list(x = tinn_data, m = m) # Initial set of parameters lambda_init &lt;- c(1, 3) gamma_init &lt;- matrix(c(0.8, 0.2, 0.2, 0.8), byrow = TRUE, nrow = m) # Turn them into working parameters parameters &lt;- pois.HMM.pn2pw(m, lambda_init, gamma_init) # Build the TMB object obj_tmb &lt;- MakeADFun(TMB_data, parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE) # Optimize mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn, gradient = obj_tmb$gr, hessian = obj_tmb$he) # Bootstrap procedure bootstrap_estimates &lt;- data.frame() DATA_SIZE &lt;- length(tinn_data) # Set how many parametric bootstrap samples we create BOOTSTRAP_SAMPLES &lt;- 10 # MLE ML_working_estimates &lt;- obj_tmb$par ML_natural_estimates &lt;- obj_tmb$report(ML_working_estimates) lambda &lt;- ML_natural_estimates$lambda gamma &lt;- ML_natural_estimates$gamma delta &lt;- ML_natural_estimates$delta PARAMS_NAMES &lt;- c(&quot;lambda&quot;, &quot;gamma&quot;, &quot;delta&quot;) for (idx_sample in 1:BOOTSTRAP_SAMPLES) { # Generate a sample based on mod, and ensure a HMM can be estimated on it # with testing_params as initial parameters temp &lt;- pois.HMM.generate.estimable.sample(ns = DATA_SIZE, mod = list(m = m, lambda = lambda, gamma = gamma), testing_params = list(m = m, lambda = lambda_init, gamma = gamma_init))$natural_parameters # The values from gamma are taken columnwise natural_parameters &lt;- unlist(temp[PARAMS_NAMES]) len_par &lt;- length(natural_parameters) bootstrap_estimates[idx_sample, 1:len_par] &lt;- natural_parameters } # Lower and upper (2.5% and 97.5%) bounds q &lt;- apply(bootstrap_estimates, 2, function(par_estimate) { quantile(par_estimate, probs = c(0.025, 0.975)) }) PARAMS_NAMES &lt;- paste0(rep(&quot;lambda&quot;, m), 1:m) # Get row and column indexes for gamma instead of the default # columnwise index: the default indexes are 1:m for the 1st column, # then (m + 1):(2 * m) for the 2nd, etc... for (gamma_idx in 1:m ^ 2) { row &lt;- (gamma_idx - 1) %% m + 1 col &lt;- (gamma_idx - 1) %/% m + 1 row_col_idx &lt;- c(row, col) PARAMS_NAMES &lt;- c(PARAMS_NAMES, paste0(&quot;gamma&quot;, paste0(row_col_idx, collapse = &quot;&quot;))) } PARAMS_NAMES &lt;- c(PARAMS_NAMES, paste0(rep(&quot;delta&quot;, m), 1:m)) bootstrap_CI &lt;- data.frame(&quot;Parameter&quot; = PARAMS_NAMES, &quot;Estimate&quot; = c(lambda, gamma, delta), &quot;Lower bound&quot; = q[1, ], &quot;Upper bound&quot; = q[2, ]) print(bootstrap_CI, row.names = FALSE) It should be noted that some bootstrap estimates can be very large or very small. One possible reason is that the randomly generated bootstrap sample might contain long chains of the same values, thus causing some probabilities in the TPM to be near the boundary 0 or 1. However, a large number of bootstrap samples lowers that risk since we retrieve a 95% CI. It is important for the bootstrap procedure to take into account the fact that estimates may not necessarily all be in the same order. For example, the first bootstrap may evaluate \\((\\lambda_1, \\lambda_2) = (1.1, 3.1)\\) while the second may evaluate to \\((\\lambda_1, \\lambda_2) = (3.11, 1.11)\\). Since we are looking to aggregate these estimates in order to derive CI through their 95% quantiles, it is necessary to impose an order on these estimates, to avoid grouping some \\(\\lambda_1\\) with some \\(\\lambda_2\\). The first that comes to mind is the ascending order. To ensure that \\({\\boldsymbol\\hat{\\lambda}}\\) are kept in ascending order, we refer to our Section Label Switching for a solution to this issue. 5.3 Negative log-likelihood profile Our nll function is parametrized in terms of and optimized with respect to the working parameters. In practice, this aspect is easy to deal with. Once a profile CI for the working parameter (here \\(\\eta_2\\)) has been obtained following the procedure above, the corresponding CI for the natural parameter \\(\\lambda_2\\) results directly from transforming the upper and lower boundary of the CI for \\(\\eta_2\\) by the one-to-one transformation \\(\\lambda_2 = \\exp(\\eta_2)\\). For further details on the invariance of likelihood-based CIs to parameter transformations, we refer to (Meeker and Escobar 1995). Profiling \\(\\eta_2\\) (the working parameter corresponding to \\(\\lambda_2\\)) with TMB can be done with profile &lt;- tmbprofile(obj = obj_tmb, name = 2, trace = FALSE) head(profile) ## tlambda value ## 67 1.589247 170.5801 ## 66 1.592447 170.4783 ## 65 1.595647 170.3790 ## 64 1.598847 170.2821 ## 63 1.602047 170.1877 ## 62 1.605247 170.0957 A plot allows for a visual representation of the profile # par(mgp = c(2, 0.5, 0), mar = c(3, 3, 2.5, 1), # cex.lab = 1.5) plot(profile, level = 0.95, xlab = expression(eta[2]), ylab = &quot;nll&quot;) Then we can infer \\(\\eta_2\\)s confidence interval, and hence \\(\\lambda_2\\)s confidence interval confint(profile) ## lower upper ## tlambda 1.593141 1.820641 exp(confint(profile)) ## lower upper ## tlambda 4.919178 6.175815 Further, profiling the TPM is done similarly. However, since individual natural TPM parameters cannot be deduced from single working parameters, we need to profile the entire working TPM then transform it back to a natural TPM. profile3 &lt;- tmbprofile(obj = obj_tmb, name = 3, trace = FALSE) profile4 &lt;- tmbprofile(obj = obj_tmb, name = 4, trace = FALSE) # Obtain confidence intervals for working parameters tgamma_3_confint &lt;- confint(profile3) tgamma_4_confint &lt;- confint(profile4) # Group lower bounds and upper bounds lower &lt;- c(tgamma_3_confint[1], tgamma_4_confint[1]) upper &lt;- c(tgamma_3_confint[2], tgamma_4_confint[2]) # Infer bounds on natural parameters gamma_1 &lt;- gamma.w2n(m, lower) gamma_2 &lt;- gamma.w2n(m, upper) # Display unsorted lower and upper bounds gamma_1 ## [,1] [,2] ## [1,] 0.99800287 0.001997133 ## [2,] 0.00154545 0.998454550 gamma_2 ## [,1] [,2] ## [1,] 0.81653046 0.1834695 ## [2,] 0.08801416 0.9119858 # Sorted confidence interval for gamma_11 sort(c(gamma_1[1, 1], gamma_2[1, 1])) ## [1] 0.8165305 0.9980029 It is noteworthy that gamma_1 is not necessarily lower than gamma_2, because only the working confidence intervals are automatically sorted. Also note that linear combinations of parameters can be profiled by passing the lincomb argument. More details are available by executing ??TMB::tmbprofile to access the tmbprofiles help. The name parameter should be the index of a parameter to profile. While the functions help mentions it can be a parameters name, the first two parameters are both named tlambda as can be seen here mod_tmb$par ## tlambda tlambda tgamma tgamma ## 0.4925054 1.7107475 -3.6263979 -2.9402803 References "],["application-to-different-data-sets.html", "Chapter 6 Application to different data sets 6.1 TYT dataset 6.2 Simulated dataset 6.3 Lamb data", " Chapter 6 Application to different data sets 6.1 TYT dataset We detail here the code used to estimate a 2-state Poisson HMM based on the tinnitus dataset available in data/tinnitus.RData. Set a seed for randomness, and load files set.seed(123) library(TMB) TMB::compile(&quot;code/poi_hmm.cpp&quot;) ## [1] 0 dyn.load(dynlib(&quot;code/poi_hmm&quot;)) ## Warning: 3 external pointers will be removed source(&quot;functions/utils.R&quot;) load(&quot;data/tinnitus.RData&quot;) Set initial parameters # Parameters and covariates m &lt;- 2 gamma &lt;- matrix(c(0.8, 0.2, 0.2, 0.8), nrow = m, ncol = m) lambda &lt;- seq(quantile(tinn_data, 0.1), quantile(tinn_data, 0.9), length.out = m) delta &lt;- stat.dist(gamma) Transform them into working parameters working_params &lt;- pois.HMM.pn2pw(m, lambda, gamma) TMB_data &lt;- list(x = tinn_data, m = m) Estimate the parameters via a function # Build the TMB object obj_tmb &lt;- MakeADFun(TMB_data, working_params, DLL = &quot;poi_hmm&quot;, silent = TRUE) # Optimize mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn, gradient = obj_tmb$gr, hessian = obj_tmb$he) # Check convergence mod_tmb$convergence == 0 ## [1] TRUE # Results summary(sdreport(obj_tmb), &quot;report&quot;) ## Estimate Std. Error ## lambda 1.63641100 0.27758296 ## lambda 5.53309576 0.31876147 ## gamma 0.94980209 0.04374676 ## gamma 0.02592204 0.02088688 ## gamma 0.05019791 0.04374676 ## gamma 0.97407796 0.02088688 ## delta 0.34054200 0.23056437 ## delta 0.65945800 0.23056437 For the code used to generate coverage probabilities and acceleration results, please take a look at code/poi_hmm_tinn.R. 6.2 Simulated dataset We detail here the code used to simulate two datasets from 2-states Poisson HMMs, one of size and one of size . Then, using the same procedure as above, we estimate a model using different initial parameters. Set initial parameters (data size and HMM parameters) DATA_SIZE_SIMU &lt;- 2000 m &lt;- 2 # Generate parameters lambda &lt;- seq(10, 14, length.out = m) # Create the transition probability matrix with 0.8 on its diagonal gamma &lt;- matrix(c(0.8, 0.2, 0.2, 0.8), nrow = m, ncol = m) delta &lt;- stat.dist(gamma) The stat.dist function computes the stationary distribution. Generate data with one of the functions defined in Generating data simu_data &lt;- pois.HMM.generate.sample(ns = DATA_SIZE_SIMU, mod = list(m = m, lambda = lambda, gamma = gamma, delta = delta))$data Set initial parameters # Parameters and covariates m &lt;- 2 gamma &lt;- matrix(c(0.6, 0.4, 0.4, 0.6), nrow = m, ncol = m) lambda &lt;- seq(quantile(simu_data, 0.1), quantile(simu_data, 0.9), length.out = m) delta &lt;- stat.dist(gamma) # Display Poisson means lambda ## [1] 7 17 Transform them into working parameters working_params &lt;- pois.HMM.pn2pw(m, lambda, gamma) TMB_data &lt;- list(x = simu_data, m = m) Estimate the parameters via a function # Build the TMB object obj_tmb &lt;- MakeADFun(TMB_data, working_params, DLL = &quot;poi_hmm&quot;, silent = TRUE) # Optimize mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn, gradient = obj_tmb$gr, hessian = obj_tmb$he) # Check convergence mod_tmb$convergence == 0 ## [1] TRUE # Results summary(sdreport(obj_tmb), &quot;report&quot;) ## Estimate Std. Error ## lambda 9.7345726 0.23644285 ## lambda 13.8098676 0.22273984 ## gamma 0.8068975 0.03331223 ## gamma 0.1564875 0.02671332 ## gamma 0.1931025 0.03331223 ## gamma 0.8435125 0.02671332 ## delta 0.4476315 0.05201103 ## delta 0.5523685 0.05201103 For the code used to generate coverage probabilities and acceleration results, please take a look at code/poi_hmm_simu1.R and code/poi_hmm_simu2.R. 6.3 Lamb data We detail here the code used to estimate a 2-state Poisson HMM based on the lamb dataset available in INSERT GITHUB LINK Set a seed for randomness, and load files set.seed(123) library(TMB) TMB::compile(&quot;code/poi_hmm.cpp&quot;) ## [1] 0 dyn.load(dynlib(&quot;code/poi_hmm&quot;)) ## Warning: 12 external pointers will be removed source(&quot;functions/utils.R&quot;) load(&quot;data/fetal-lamb.RData&quot;) lamb_data &lt;- lamb rm(lamb) Set initial parameters # Parameters and covariates m &lt;- 2 gamma &lt;- matrix(c(0.8, 0.2, 0.2, 0.8), nrow = m, ncol = m) lambda &lt;- seq(0.3, 4, length.out = m) delta &lt;- stat.dist(gamma) Transform them into working parameters working_params &lt;- pois.HMM.pn2pw(m, lambda, gamma) TMB_data &lt;- list(x = lamb_data, m = m) Estimate the parameters via a function # Build the TMB object obj_tmb &lt;- MakeADFun(TMB_data, working_params, DLL = &quot;poi_hmm&quot;, silent = TRUE) # Optimize mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn, gradient = obj_tmb$gr, hessian = obj_tmb$he) # Check convergence mod_tmb$convergence == 0 ## [1] TRUE # Results summary(sdreport(obj_tmb), &quot;report&quot;) ## Estimate Std. Error ## lambda 0.25636541 0.04016445 ## lambda 3.11475432 1.02131182 ## gamma 0.98872128 0.01063571 ## gamma 0.31033853 0.18468648 ## gamma 0.01127872 0.01063571 ## gamma 0.68966147 0.18468648 ## delta 0.96493123 0.03181445 ## delta 0.03506877 0.03181445 On a minor note, when comparing our estimation results to those reported by (Leroux and Puterman 1992), some non-negligible differences can be noted. The reasons for this are difficult to determine, but some likely explanations are given in the following. First, differences in the parameter estimates may result e.g. from the optimizing algorithms used and related setting (e.g. convergence criterion, number of steps, optimization routines used in 1992,). Moreover, (Leroux and Puterman 1992) seem to base their calculations on an altered likelihood, which is reduced by removing the constant term \\(\\sum_{i=1}^{T} \\log(x_{i}!)\\) from the log-likelihood. This modification may also possess an impact on the behavior of the optimization algorithm, as e.g. relative convergence criteria and step size could be affected. The altered likelihood becomes apparent when computing it on a one-state HMM. A one-state Poisson HMM is a Poisson regression model, for which the log-likelihood has the expression \\[\\begin{align*} l(\\lambda) &amp;= \\log \\left(\\prod_{i=1}^{T} \\frac{\\lambda^{x_i} e^{-\\lambda}}{x_{i}!} \\right)\\\\ &amp;= - T \\lambda + \\log(\\lambda) \\left( \\sum_{i=1}^{T} x_i \\right) - \\sum_{i=1}^{T} \\log(x_{i}!). \\end{align*}\\] The authors find a ML estimate \\(\\lambda = 0.3583\\) and a log-likelihood of -174.26. In contrast, calculating the log-likelihood explicitly shows a different result. x &lt;- lamb_data # We use n instead of T in R code n &lt;- length(x) l &lt;- 0.3583 - n * l + log(l) * sum(x) - sum(log(factorial(x))) ## [1] -201.0436 The log-likelihood is different, but when the constant \\(- \\sum_{i=1}^{T} \\log(x_{i}!)\\) is removed, it matches our result. - n * l + log(l) * sum(x) ## [1] -174.2611 References "],["github.html", "Chapter 7 GitHub 7.1 Directory structure", " Chapter 7 GitHub For reading more easily, we recommend opening the file Data supplements.Rproj with R-Studio, which lets the user have the correct working path set up automatically. Other files can be opened directly by double-clicking on them, or via the Files tab in R-Studio. Most of the code can be folded/collapsed into sections easily by clicking in the menu Edit-&gt;Collapse All Output. Unfolding can be done by clicking on the arrows to the left of the folded sections, or in the menu Edit-&gt;Expand All Output. 7.1 Directory structure For readability, folders are displayed with a dash at the end. The folder code will be at the section code/. 7.1.1 code/ Files required to compute the main results of the article: acceleration factors and coverage probabilities. 7.1.2 code/linreg.cpp Specifies the function computing the negative log-likelihood of a linear regression in C++. Heavily inspired by https://github.com/kaskr/adcomp/blob/master/tmb_examples/linreg.cpp. 7.1.3 code/linreg.dll Generated automatically by compiling code/linreg.cpp. Library that contains code and data that can be used by more than one program at the same time. Used to compute negative log-likelihoods in R with TMB. 7.1.4 code/linreg.o Generated by compiling code/linreg.cpp. Used to compute negative log-likelihoods in R with TMB. 7.1.5 code/linreg_extended.cpp Similar to code/linreg.cpp, with some more complex code added to serve as an example. Used to compute negative log-likelihoods in R with TMB. 7.1.6 code/linreg_extended.dll Generated automatically by compiling code/linreg_extended.cpp. Similar to code/linreg.cpp. Used to compute negative log-likelihoods in R with TMB. 7.1.7 code/linreg_extended.o Generated by compiling code/linreg.cpp. Similar to code/linreg.o. Used to compute negative log-likelihoods in R with TMB. 7.1.8 code/main.R Used to run procedures related to timing and comparisons presented in the article. By default, it will load results instead. The first chunk of code it runs can be found in the file code/setup_parameters.R. Afterwards, either computations are run and their results are stored in data/, or results are loaded to the current environment. The computations are written in code/poi_hmm_tinn.R, code/poi_hmm_lamb.R, code/poi_hmm_simu1.R, and code/poi_hmm_simu2.R. NOTE: on an Intel(R) Core(TM) i7-8700 processor running under Windows 10 Enterprise version 1809, the entire computation took 6 days and 7 hours. More details at individual files. 7.1.9 code/packages.R Automatically install/load necessary packages for running code/main.R. 7.1.10 code/poi_hmm.cpp Specifies the function computing the negative log-likelihood of a m-state Poisson Hidden Markov Model in C++. 7.1.11 code/poi_hmm.dll Generated automatically by compiling code/poi_hmm.cpp. Library that contains code and data that can be used by more than one program at the same time. Used to compute negative log-likelihoods in R with TMB. 7.1.12 code/poi_hmm.o Generated by compiling code/linreg.cpp. Used to compute negative log-likelihoods in R with TMB. 7.1.13 code/poi_hmm_lamb.R 7.1.14 code/poi_hmm_simu1.R 7.1.15 code/poi_hmm_simu2.R 7.1.16 code/poi_hmm_tinn.R Most of their code is common: Parameters and covariates defines parameters and covariates in natural form. The simulation files generate data there. Parameters &amp; covariates for TMB transforms parameters and covariates to their working form. Estimation computes estimates with and without TMB for further comparison. The estimates are stored in the data-frames conf_int_*. Creating variables for the CIs creates necessary variables for to compute confidence intervals (CIs). Benchmarks uses the microbenchmark package to accurately time: the \\(TMB_0\\) \\(TMB_G\\) \\(TMB_H\\) and \\(TMB_{GH}\\) procedures. Their times are stored in the data-frames estim_benchmarks_df_* the negative log-likelihood computation with and without TMB. Their times are stored in the data-frames mllk_times_df_*. multiple estimation procedures: BFGS, L-BFGS-B, nlm, nlminb, hjn, marqLevAlg and TMB. Their times are stored in the data-frames method_comparison_df_*. Profiling the likelihood uses the TMB packages tmbprofile function to determine a profile of the likelihood, then determines a CI based on it for the corresponding working parameter. Eventually, CIs are derived when possible for the natural parameters \\(\\hat{{\\boldsymbol\\lambda}}\\) and \\(\\hat{{\\boldsymbol\\Gamma}}\\). The CIs are stored in the data-frames conf_int_*. Bootstrap derives a parametric bootstrap CI from the dataset, while checking that the generated data can be estimated without errors, after which we apply our label switching function to ensure that estimates of a parameter are only aggregated with estimates of the same parameter. The CIs are stored in the data-frames conf_int_*. TMB confidence intervals computes CIs based on TMB-derived standard errors. The CIs are stored in the data-frames conf_int_*. Coverage probabilities of the 3 CI methods simulates coverage samples based on true values when available or on estimates otherwise, then derives CIs using the three CI generation methods described above. The coverage probabilities are stored in the data-frames conf_int_*. Fixes uses the label-switching algorithm on the estimates in the conf_int_* variable to ensure they are correctly ordered, and then reorders \\(\\hat{{\\boldsymbol\\Gamma}}\\) in the conf_int_* variable to a row-wise order instead of the default column-wise order. In short this applies two fixes on the conf_int_* variable to make it easier to read. At the end, we reorder the profile likelihood based CIs in case they are not in ascending order. A unique randomness seed is set in each of these four files before all time-consuming tasks involving some randomness. code/setup_parameters.R defines the random number generators version. code/poi_hmm_tinn.R contains an extra section: Benchmark the same dataset many times to check the benchmark durations has low variance. As written in its title, its role is to time estimation with and without TMB. However, unlike in the section Benchmarks, estimation is done on the same dataset everytime. If everything goes well, the times should have a negligible variance. This allows us to check if normal background activity on the computer (e.g. the user opening a window or moving the mouse) affects estimation time in any noticeable way. If it affects estimation noticeably, then we would have to apply much stricter control on background processes in order to reliably compare estimation speeds with different optimizers. Luckily, a low variance was observed, making the acceleration evidenced in this project significant. NOTE: to avoid wasting time, some code is accelerated via parallelization: In code/poi_hmm_simu1.R, bootstrapping is parallelized both in Bootstrap (accelerated from 83 seconds per sample to 29 seconds) and in Coverage probabilities of the 3 CI methods (likelihood profiling was slower with parallelization, likely due to loading TMB each time and the procedure being already fairly quick: from 2.8 seconds in total without to 3.5 seconds in total with). In code/poi_hmm_simu2.R, bootstrapping and likelihood profiling are both parallelized in Bootstrap (accelerated from 64 seconds per bootstrap sample to 21), Profiling the likelihood, and in Coverage probabilities of the 3 CI methods. Parallelization is handled with the (Revolution Analytics and Weston, n.d.) package. 7.1.17 code/setup_parameters.R Sets up multiple variables either to define some global constants or to store useful results for easier maintenance, and compiles code/linreg.cpp and code/poi_hmm.cpp. 7.1.18 data/ Files containing both datasets and the computational results. [main.R] stores important results in this folder, and only loads them by default. Speed results are available after some post-processing that we leave to the reader. 7.1.19 data/fetal-lamb.RData Contains lamb: an integer vector of with 240 data from (Leroux and Puterman 1992). 7.1.20 data/results_lamb.RData 7.1.21 data/results_simu1.RData 7.1.22 data/results_simu2.RData 7.1.23 data/results_tinn.RData Contain results from computations and timings run in code/main.R. 7.1.24 data/tinnitus.RData Tinnitus data collected with the Track Your Tinnitus (TYT) mobile application on 87 successive days, provided by the University of Regensburg and European School for Interdisciplinary Tinnitus Research (ESIT), of which a detailed description is presented in Pryss, Reichert, Herrmann, et al. (2015) and Pryss, Reichert, Langguth, et al. (2015). A plot is available in Figure 4.1. 7.1.25 functions/ Utility functions used both in C++ files and in R files. 7.1.26 functions/utils.cpp Utility functions used in code/poi_hmm.cpp to transform working parameters to their natural form, and to compute the hidden Markov chains stationary distribution when needed. 7.1.27 functions/utils.R Useful functions to estimate HMMs with and without TMB and perform various pre-processing and post-processing. 7.1.28 functions/utils_linreg_extended.cpp Contains the useless example function References "],["references.html", "References Authors github account License", " References Contributors: Timothee Bacri1, Geir Drage Berentsen2, Jan Bulla1 and Sondre Hølleland3. University of Bergen, Norway. Norwegian Buisness school, Norway. Institute of Marine Research, Norway. Correspondance to: jan.bulla@uib.no Biometrical Journal paper can be found here. This repository contains the code to produce the R-bookdown website found here. Authors github account Timothee Bacri - timothee-bacri Sondre Hølleland - holleland License This project is licensed under the xxxx License - see LICENSE.md for details. "]]
