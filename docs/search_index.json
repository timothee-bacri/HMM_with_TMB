[["using-tmb.html", "Chapter 4 Using TMB 4.1 Likelihood function 4.2 Preparing data and functions 4.3 Modeling 4.4 Nested models", " Chapter 4 Using TMB 4.1 Likelihood function The function is defined in poi_hmm.cpp #include &lt;TMB.hpp&gt; #include &quot;../functions/utils.cpp&quot; // Likelihood for a poisson hidden markov model. template&lt;class Type&gt; Type objective_function&lt;Type&gt;::operator() () { // Data DATA_VECTOR(x); // timeseries vector DATA_INTEGER(m); // Number of states m // Parameters PARAMETER_VECTOR(tlambda); // conditional log_lambdas&#39;s PARAMETER_VECTOR(tgamma); // m(m-1) working parameters of TPM // Uncomment only when using a non-stationary distribution //PARAMETER_VECTOR(tdelta); // transformed stationary distribution, // Transform working parameters to natural parameters: vector&lt;Type&gt; lambda = tlambda.exp(); matrix&lt;Type&gt; gamma = Gamma_w2n(m, tgamma); // Construct stationary distribution vector&lt;Type&gt; delta = Stat_dist(m, gamma); // If using a non-stationary distribution, use this instead //vector&lt;Type&gt; delta = Delta_w2n(m, tdelta); // Get number of timesteps (n) int n = x.size(); // Evaluate conditional distribution: Put conditional // probabilities of observed x in n times m matrix // (one column for each state, one row for each datapoint): matrix&lt;Type&gt; emission_probs(n, m); matrix&lt;Type&gt; row1vec(1, m); row1vec.setOnes(); for (int i = 0; i &lt; n; i++) { if (x[i] != x[i]) { // f != f returns true if and only if f is NaN. // Replace missing values (NA in R, NaN in C++) with 1 emission_probs.row(i) = row1vec; } else { emission_probs.row(i) = dpois(x[i], lambda, false); } } // Corresponds to Zucchini&#39;s book page 333 matrix&lt;Type&gt; foo, P; Type mllk, sumfoo, lscale; if (m == 1) { mllk = - emission_probs.col(0).array().log().sum(); // Use adreport on variables we are interested in: ADREPORT(lambda); ADREPORT(gamma); ADREPORT(delta); // Things we need for local decoding REPORT(lambda); REPORT(gamma); REPORT(delta); return mllk; } foo = (delta * vector&lt;Type&gt;(emission_probs.row(0))).matrix(); sumfoo = foo.sum(); lscale = log(sumfoo); foo.transposeInPlace(); foo /= sumfoo; for (int i = 2; i &lt;= n; i++) { P = emission_probs.row(i - 1); foo = ((foo * gamma).array() * P.array()).matrix(); sumfoo = foo.sum(); lscale += log(sumfoo); foo /= sumfoo; } mllk = -lscale; // Use adreport on variables for which we want standard errors ADREPORT(lambda); ADREPORT(gamma); ADREPORT(delta); // Variables we need for local decoding and in a convenient format REPORT(lambda); REPORT(gamma); REPORT(delta); REPORT(n); REPORT(emission_probs); REPORT(mllk); return mllk; } 4.2 Preparing data and functions Before we can fit the model, we need to load some necessary packages and data files. We also need to compile the c++ code and load the functions into our working environment in R. We start by loading the necessary packages and the utility R functions. # Load TMB and optimization packages library(TMB) library(optimr) # Load the parameter transformation function source(&quot;functions/utils.R&quot;) Next, we need to compile the c++ code for computing the likelihood and its gradients. Once it is compiled, we can load the TMB likelihood object into R  making it available from R. # Run the `C++` file containing the TMB code TMB::compile(&quot;code/poi_hmm.cpp&quot;) # Load it dyn.load(dynlib(&quot;code/poi_hmm&quot;)) The data are part of a large data set collected with the Track Your Tinnitus (TYT) mobile application, a detailed description of which is presented in (R. Pryss, Reichert, Herrmann, et al. 2015; R. Pryss, Reichert, Langguth, et al. 2015). We analyze 87 successive days of the arousal variable, which is measured on a discrete scale. Higher values correspond to a higher degree of excitement, lower values to a more calm emotional state (for details, see Probst et al. 2016). The data is stored in the file data/tinnitus.Rdata and can be loaded by a simple call. load(&quot;data/tinnitus.RData&quot;) @ref(table:tinnitus_data) presents the raw data, which are also available for download at INSERT GITHUB LINK. TYT data. Observations collected by the TYT app on 87 successive days (from left to right). 6 5 3 6 4 3 5 6 6 6 4 6 6 4 6 6 6 6 6 4 6 5 6 7 6 5 5 5 7 6 5 6 5 6 6 6 5 6 7 7 6 7 6 6 6 6 5 7 6 1 6 0 2 1 6 7 6 6 6 5 5 6 6 2 5 0 1 1 1 2 3 1 3 1 3 0 1 1 1 4 1 4 1 2 2 2 0 Table 4.1 and Figure 4.1 presents the raw data, which are also available for download in the data folder on github.com/timothee-bacri/HMM_with_TMB/. Table 4.1: TYT data. Observations collected by the TYT app on 87 successive days (from left to right). 6 5 3 6 4 3 5 6 6 6 4 6 6 4 6 6 6 6 6 4 6 5 6 7 6 5 5 5 7 6 5 6 5 6 6 6 5 6 7 7 6 7 6 6 6 6 5 7 6 1 6 0 2 1 6 7 6 6 6 5 5 6 6 2 5 0 1 1 1 2 3 1 3 1 3 0 1 1 1 4 1 4 1 2 2 2 0 Figure 4.1: Plot of observations from TYT app data for 87 succesive days. 4.3 Modeling Initialization of the number of states and starting (or initial) values for the optimization First, the number of states needs to be determined. As explained by (Pohle, Langrock, van Beest, et al. 2017b), (Pohle, Langrock, van Beest, et al. 2017a), and (Zucchini, MacDonald, and Langrock 2016 Section 6) (to name only a few), usually one would first fit models with a different number of states. Then, these models are evaluated e.g. by means of model selection criteria (as carried out by Leroux and Puterman 1992) or prediction performance (Celeux and Durand 2008). Since the results reported by (Leroux and Puterman 1992) show that a two-state model is preferred by the BIC, we focus on this model only here - although other choices would be possible, e.g. the AIC selects a three-state model. The list object TMB_data contains the data and the number of states. # Model with 2 states m &lt;- 2 TMB_data &lt;- list(x = tinn_data, m = m) Secondly, initial values for the optimization procedure need to be defined. Although we will apply unconstrained optimization, we initialize the natural parameters, because this is much more intuitive and practical than handling the working parameters. # Generate initial set of parameters for optimization lambda &lt;- c(1, 3) gamma &lt;- matrix(c(0.8, 0.2, 0.2, 0.8), byrow = TRUE, nrow = m) Transformation from natural to working parameters The previously created initial values are transformed and stored in the list parameters for the optimization procedure. # Turn them into working parameters parameters &lt;- pois.HMM.pn2pw(m, lambda, gamma) Creation of the TMB negative log-likelihood function with its derivatives This object, stored as obj_tmb requires the data, the initial values, and the previously created DLL as input. Setting argument silent = TRUE disables tracing information and is only used here to avoid excessive output. obj_tmb &lt;- MakeADFun(TMB_data, parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE) This object also contains the previously defined initial values as a vector (par) rather than a list. The negative log-likelihood (fn), its gradient (gr), and Hessian (he) are functions of the parameters (in vector form) while the data are considered fixed: obj_tmb$par ## tlambda tlambda tgamma tgamma ## 0.000000 1.098612 -1.386294 -1.386294 obj_tmb$fn(obj_tmb$par) ## [1] 228.3552 obj_tmb$gr(obj_tmb$par) ## [,1] [,2] [,3] [,4] ## [1,] -3.60306 -146.0336 10.52832 -1.031706 obj_tmb$he(obj_tmb$par) ## [,1] [,2] [,3] [,4] ## [1,] 1.902009 -5.877900 -1.3799682 2.4054017 ## [2,] -5.877900 188.088247 -4.8501589 2.3434284 ## [3,] -1.379968 -4.850159 9.6066700 -0.8410438 ## [4,] 2.405402 2.343428 -0.8410438 0.7984216 Execution of the optimization For this step we rely again on the optimizer implemented in the nlminb function. The arguments, i.e.~ initial values for the parameters and the function to be optimized, are extracted from the previously created TMB object. mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn) # Check that it converged successfully mod_tmb$convergence == 0 ## [1] TRUE It is noteworthy that various alternatives to nlminb exist. Nevertheless, we focus on this established optimization routine because of its high speed of convergence. Obtaining ML estimates Obtaining the ML estimates of the natural parameters together with their standard errors is possible by using the previously introduced command sdreport. Recall that this requires the parameters of interest to be treated by the ADREPORT statement in the C++ part. It should be noted that the presentation of the set of parameters gamma below results from a column-wise representation of the TPM. summary(sdreport(obj_tmb, par.fixed = mod_tmb$par), &quot;report&quot;) ## Estimate Std. Error ## lambda 1.63641070 0.27758294 ## lambda 5.53309626 0.31876141 ## gamma 0.94980192 0.04374682 ## gamma 0.02592209 0.02088689 ## gamma 0.05019808 0.04374682 ## gamma 0.97407791 0.02088689 ## delta 0.34054163 0.23056401 ## delta 0.65945837 0.23056401 Note that the table above also contains estimation results for \\({\\boldsymbol\\delta}\\) and accompanying standard errors, although \\({\\boldsymbol\\delta}\\) is not estimated, but derived from \\({\\boldsymbol\\Gamma}\\). We provide further details on this aspect in Confidence intervals. The value of the nll function in the minimum found by the optimizer can also be extracted directly from the object mod_tmb by accessing the list element objective: mod_tmb$objective ## [1] 168.5361 Use exact gradient and Hessian In the optimization above we already benefited from an increased speed due to the evaluation of the nll in C++ compared to the forward algorithm being executed entirely in R. However, the use of TMB also permits to introduce the gradient and/or the Hessian computed by TMB into the optimization procedure. This is in general advisable, because TMB provides an exact value of both gradient and Hessian up to machine precision, which is superior to approximations used by optimizing procedure. Similar to the nll, both quantities can be extracted directly from the TMB object obj_tmb: # The negative log-likelihood is accessed by the objective # attribute of the optimized object mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn, gradient = obj_tmb$gr, hessian = obj_tmb$he) mod_tmb$objective ## [1] 168.5361 Note that passing the exact gradient and Hessian as provided by TMB to nlminb leads to the same minimum, i.e. value of the nll function, here. On a minor note, when comparing our estimation results to those reported by (Leroux and Puterman 1992), some non-negligible differences can be noted. The reasons for this are difficult to determine, but some likely explanations are given in the following. First, differences in the parameter estimates may result e.g. from the optimizing algorithms used and related setting (e.g. convergence criterion, number of steps, optimization routines used in 1992,). Moreover, (Leroux and Puterman 1992) seem to base their calculations on an altered likelihood, which is reduced by removing the constant term \\(\\sum_{i=1}^{T} \\log(x_{i}!)\\) from the log-likelihood. This modification may also possess an impact on the behavior of the optimization algorithm, as e.g.~relative convergence criteria and step size could be affected. 4.4 Nested models 4.4.1 Principle In order to build a nested model, we decide to fix \\(\\lambda_1\\) to the value 1. # Get the previous values, and fix some fixed_par_lambda &lt;- lambda fixed_par_lambda[1] &lt;- 1 # Transform them into working parameters new_parameters &lt;- pois.HMM.pn2pw(m = m, lambda = fixed_par_lambda, gamma = gamma) Then, we instruct TMB to read these custom parameters. We indicate fixed values by mapping them to NA values, whereas the variable values need to be mapped to different factor variables. Lastly, we specify this mapping with the map argument when making the TMB object. map &lt;- list(tlambda = as.factor(c(NA, 1)), tgamma = as.factor(c(2, 3))) fixed_par_obj_tmb &lt;- MakeADFun(TMB_data, new_parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE, map = map) Estimating of the model and displaying the results is performed as usual fixed_par_mod_tmb &lt;- nlminb(start = fixed_par_obj_tmb$par, objective = fixed_par_obj_tmb$fn, gradient = fixed_par_obj_tmb$gr, hessian = fixed_par_obj_tmb$he) summary(sdreport(fixed_par_obj_tmb), &quot;report&quot;) ## Estimate Std. Error ## lambda 1.00000000 0.00000000 ## lambda 5.50164872 0.30963641 ## gamma 0.94561055 0.04791050 ## gamma 0.02655944 0.02133283 ## gamma 0.05438945 0.04791050 ## gamma 0.97344056 0.02133283 ## delta 0.32810136 0.22314460 ## delta 0.67189864 0.22314460 Note that the standard error of \\(\\lambda_1\\) is zero, because it is no longer considered a variable parameter and does not enter the optimization procedure. 4.4.2 Limit This method cannot work in general for working parameters which are not linked to a single natural parameter. This is because only working parameters can be fixed with this method, but the working parameters of the TPM are not each linked to a single natural parameter. As an example, fixing the natural parameter \\(gamma_{11}\\) is not equivalent to fixing any working parameter \\(\\tau{ij}\\). Hence, the TPM cannot in general be fixed. However, if conditions on the natural parameters can be translated to conditions on the working parameters, then there should not be any issue. We will show in the next section that a type of general constraints on the TPM can be carried out. In addition, overcoming this restriction should be possible via constrained optimization. 4.4.3 Parameter equality constraints It is noteworthy that more complex constraints are possible as well For example, to impose equality constraints (such as \\(\\gamma_{11} = \\gamma_{22}\\)), the corresponding factor level has to be identical for the concerned entries. As a reminder, we defined the working parameters via \\[ \\tau_{ij} = \\log\\left(\\frac{\\gamma_{ij}}{1 - \\sum_{k \\neq i} \\gamma_{ik}}\\right) = \\log(\\gamma_{ij}/\\gamma_{ii}), \\text{ for } i \\neq j \\] With a 2 state HMM, the constraint \\(\\gamma_{11} = \\gamma_{22}\\) is equivalent to \\(\\gamma_{12} = \\gamma_{21}\\). Thus, the constraint can be transformed into \\(\\tau_{12} = \\log(\\gamma_{12}/\\gamma_{11}) = \\log(\\gamma_{21}/\\gamma_{22}) = \\tau_{21}\\). The mapping parameters must be set to a common factor to be forced equal. map &lt;- list(tlambda = as.factor(c(1, 2)), tgamma = as.factor(c(3, 3))) fixed_par_obj_tmb &lt;- MakeADFun(TMB_data, parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE, map = map) fixed_par_mod_tmb &lt;- nlminb(start = fixed_par_obj_tmb$par, objective = fixed_par_obj_tmb$fn, gradient = fixed_par_obj_tmb$gr, hessian = fixed_par_obj_tmb$he) # Results + check that the constraint is respected results &lt;- summary(sdreport(fixed_par_obj_tmb), &quot;report&quot;) tpm &lt;- matrix(results[rownames(results) == &quot;gamma&quot;, &quot;Estimate&quot;], nrow = m, ncol = m, byrow = FALSE) # Transformations are column-wise by default, be careful! tpm ## [,1] [,2] ## [1,] 0.96759216 0.03240784 ## [2,] 0.03240784 0.96759216 tpm[1, 1] == tpm[2, 2] ## [1] TRUE Similar complex constraints on the TPM can also be setup for HMMs with over two states. However, it appears that it can only be done in general when the constraint involves natural parameters of the same row with the exception of a 2 state model. We have not found a way to easily implement equality constraints between natural TPM parameters of different rows. Possible solutions are constrained optimization and different parametrization. As an example, we will look at a three-state HMM with the constraint \\(\\gamma_{12} = \\gamma_{13}\\). # Model with 2 states m &lt;- 3 TMB_data &lt;- list(x = tinn_data, m = m) # Initial set of parameters lambda &lt;- c(1, 3, 5) gamma &lt;- matrix(c(0.8, 0.1, 0.1, 0.1, 0.8, 0.1, 0.1, 0.1, 0.8), byrow = TRUE, nrow = m) # Turn them into working parameters parameters &lt;- pois.HMM.pn2pw(m, lambda, gamma) # Build the TMB object obj_tmb &lt;- MakeADFun(TMB_data, parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE) # Optimize mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn, gradient = obj_tmb$gr, hessian = obj_tmb$he) # Check convergence mod_tmb$convergence == 0 ## [1] TRUE # Results summary(sdreport(obj_tmb), &quot;report&quot;) ## Estimate Std. Error ## lambda 8.281290e-01 5.418824e-01 ## lambda 1.705082e+00 2.949769e-01 ## lambda 5.514886e+00 3.080238e-01 ## gamma 5.516919e-01 3.132490e-01 ## gamma 4.596623e-09 5.115944e-05 ## gamma 2.749570e-02 2.164849e-02 ## gamma 1.989160e-01 2.698088e-01 ## gamma 9.772963e-01 3.436716e-02 ## gamma 2.453591e-10 2.730433e-06 ## gamma 2.493922e-01 2.546636e-01 ## gamma 2.270369e-02 3.436717e-02 ## gamma 9.725043e-01 2.164849e-02 ## delta 3.836408e-02 3.757898e-02 ## delta 3.361227e-01 3.144598e-01 ## delta 6.255132e-01 3.063220e-01 The transformed constraint becomes \\(\\tau_{12} = \\log(\\gamma_{12}/\\gamma_{11}) = \\log(\\gamma_{13}/\\gamma_{11}) = \\tau_{13}\\). We need to be careful how we specify the constraint, because the vector tgamma will be converted into a matrix column-wise since this is Rs default way to handle matrix-vector conversions. The tgamma matrix looks naturally like \\[\\begin{pmatrix} &amp;\\tau_{12}&amp;\\tau_{13}\\\\ \\tau_{21}&amp; &amp;\\tau_{23}\\\\ \\tau_{31}&amp;\\tau_{32}&amp; \\end{pmatrix}\\] As a vector in R, it becomes \\(\\left(\\tau_{21}, \\tau_{31}, \\tau_{12}, \\tau_{32}, \\tau_{13}, \\tau_{23}\\right)\\). Therefore, the constraint needs to be placed on the \\(3^{rd}\\) and \\(6^{th}\\) vector parameter. map &lt;- list(tlambda = as.factor(c(1, 2, 3)), tgamma = as.factor(c(4, 5, 6, 7, 6, 8))) fixed_par_obj_tmb &lt;- MakeADFun(TMB_data, parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE, map = map) fixed_par_mod_tmb &lt;- nlminb(start = fixed_par_obj_tmb$par, objective = fixed_par_obj_tmb$fn, gradient = fixed_par_obj_tmb$gr, hessian = fixed_par_obj_tmb$he) # Results + check that the constraint is respected results &lt;- summary(sdreport(fixed_par_obj_tmb), &quot;report&quot;) tpm &lt;- matrix(results[rownames(results) == &quot;gamma&quot;, &quot;Estimate&quot;], nrow = m, ncol = m, byrow = FALSE) # Transformations are column-wise by default, be careful! tpm ## [,1] [,2] [,3] ## [1,] 5.447127e-01 2.276437e-01 0.22764365 ## [2,] 2.718007e-09 9.753263e-01 0.02467374 ## [3,] 2.710078e-02 2.005408e-10 0.97289922 tpm[1, 2] == tpm[1, 3] ## [1] TRUE Equality constraints involving a diagonal member of the TPM are simpler to specify: the constraint \\(\\gamma_{i,j} = \\gamma_{i,i}\\) becomes transformed to \\(\\tau_{i,j} = 1\\) and this can be specified in the same way the Poisson mean \\(\\lambda_1\\) was fixed. References "]]
