[["index.html", "A gentle tutorial of accelerated parameter and confidence interval estimation for Hidden Markov Models using Template Model Builder 1 Introduction", " A gentle tutorial of accelerated parameter and confidence interval estimation for Hidden Markov Models using Template Model Builder Timothée Bacri timothee.bacri@uib.no Jan Bulla jan.bulla@uib.no Geir Berentsen geir.berentsen@nhh.no Sondre Hølleland sondre.hoelleland@hi.no 2022-05-06 1 Introduction Welcome ! This website aims to accompany the reader of [Link to official article] The files used in this repository are available in https://github.com/timothee-bacri/HMM_with_TMB. For a description of the files and the directory structure, please read Directory Structure. Note that only the folders code/, functions/, and data/ contain files used in the article. The other folders and files relate to this Gitbook and are not described in Directory Structure. For a description of the functions in functions/utils.R and their parameters and return objects, please take a look at functions/utils.R-explanation.R. "],["principles-of-using-tmb-for-mle.html", " 2 Principles of using TMB for MLE 2.1 Setup 2.2 Linear regression example 2.3 Extending the C++ nll", " 2 Principles of using TMB for MLE 2.1 Setup In order to run the scripts and example code, you first need to set up TMB by going through the following steps: Install Rtools Install the R-package TMB (Kristensen 2022) by executing the following code in R: install.packages(&quot;TMB&quot;) (Optional) Setup error debugging in RStudio by running the command TMB:::setupRstudio() Advanced use is detailed in https://kaskr.github.io/adcomp/_book/Tutorial.html 2.2 Linear regression example Let \\({\\boldsymbol x}\\) and \\({\\boldsymbol y}\\) denote the predictor and response vector, respectively, both of length \\(n\\). For a simple linear regression model with intercept \\(a\\) and slope \\(b\\), the negative log-likelihood equals \\[\\begin{equation*} - \\log L(a, b, \\sigma^2) = - \\sum_{i=1}^n \\log(\\phi(y_i; a + bx_i, \\sigma^2)), \\end{equation*}\\] where \\(\\phi(\\cdot; \\mu, \\sigma^2)\\) corresponds to the density function of the univariate normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\). The use of TMB requires the (negative) log-likelihood function to be coded in C++ under a specific template, which is then loaded into R. The minimization of this function and other post-processing procedures are all carried out in R. Therefore, we require two files. The first file, named code/linreg.cpp, is written in C++ and defines the objective function, i.e. the negative log-likelihood (nll) function of the linear model. #include &lt;TMB.hpp&gt; //import the TMB template template&lt;class Type&gt; Type objective_function&lt;Type&gt;::operator() () { DATA_VECTOR(y); // Data vector y passed from R DATA_VECTOR(x); // Data vector x passed from R PARAMETER(a); // Parameter a passed from R PARAMETER(b); // Parameter b passed from R PARAMETER(tsigma); // Parameter sigma (transformed, on log-scale) // passed from R // Transform tsigma back to natural scale Type sigma = exp(tsigma); // Declare negative log-likelihood Type nll = - sum(dnorm(y, a + b * x, sigma, true)); // Necessary for inference on sigma, not only tsigma ADREPORT(sigma); return nll; } The second file needed is written in R and serves for compiling the nll function defined above and carrying out the estimation procedure by numerical optimization of the nll function. The .R file (shown below) carries out the compilation of the C++ file and minimization of the nll function: # Loading TMB package library(TMB) # Compilation. The compiler returns 0 if the compilation of # the cpp file was successful TMB::compile(&quot;code/linreg.cpp&quot;) ## [1] 0 # Dynamic loading of the compiled cpp file dyn.load(dynlib(&quot;code/linreg&quot;)) # Generate the data for our test sample set.seed(123) data &lt;- list(y = rnorm(20) + 1:20, x = 1:20) parameters &lt;- list(a = 0, b = 0, tsigma = 0) # Instruct TMB to create the likelihood function obj_linreg &lt;- MakeADFun(data, parameters, DLL = &quot;linreg&quot;, silent = TRUE) # Optimization of the objective function with nlminb mod_linreg &lt;- nlminb(obj_linreg$par, obj_linreg$fn, obj_linreg$gr, obj_linreg$he) mod_linreg$par ## a b tsigma ## 0.31009251 0.98395536 -0.05814649 It is noteworthy that if one estimates multiple models (for example on the same data with different initial values of parameters), then mod_linreg$par will output the latest optimized parameters. If one wishes to access the best set of optimized parameters (in terms of likelihood) among the ones that were estimated, then obj_tmb$env$last.par.best will output these. Now that the optimization is taken care of, we can display the estimates with standard errors using the sdreport function. sdreport(obj_linreg) ## sdreport(.) result ## Estimate Std. Error ## a 0.31009251 0.43829087 ## b 0.98395536 0.03658782 ## tsigma -0.05814649 0.15811383 ## Maximum gradient component: 1.300261e-10 If we use summary on this object, we also get the variables we have passed to ADREPORT in the code/linreg.cpp file. In this example, this is only the residual standard deviation; sigma. summary(sdreport(obj_linreg)) ## Estimate Std. Error ## a 0.31009251 0.43829087 ## b 0.98395536 0.03658782 ## tsigma -0.05814649 0.15811383 ## sigma 0.94351172 0.14918225 The select argument restricts the output to variables passed by ADREPORT(variable_name); in the cpp file. As we will see in the Section Wald-type confidence intervals based on the Hessian, this lets us derive confidence intervals for these natural parameters easily. summary(sdreport(obj_linreg), select = &quot;report&quot;) ## Estimate Std. Error ## sigma 0.9435117 0.1491823 Certainly, you would not build a TMB model to fit a linear regression. We can use standard R functions for that. Therefore, we use stats::lm on the same data and compare the estimates to those obtained by TMB. rbind( &quot;lm&quot; = lm(y ~ x, data = data)$coef, # linear regression using R &quot;TMB&quot; = mod_linreg$par[1:2] # intercept and slope from TMB fit ) ## (Intercept) x ## lm 0.3100925 0.9839554 ## TMB 0.3100925 0.9839554 As we can see, the parameter estimates are exactly the same. 2.3 Extending the C++ nll Sometimes it is useful to write subroutines as a separate function that can be used within your TMB likelihood function. This can increase readability of your code and reduce the number of lines of code in your main cpp file. Writing extra files to define functions compatible with TMB requires some care, as these must follow TMB’s template. To illustrate how this works, we add a separate function to the code/linreg.cpp example. The following function does not do anything meaningful, but is just meant to show you how you can add write an additional function and load it into your C++. We start by writing the subroutine function as a separate file called functions/utils_linreg_extended.cpp. template&lt;class Type&gt; matrix&lt;Type&gt; function_example(matrix&lt;Type&gt; mat_example) { // This function doesn&#39;t do anything meaningful matrix&lt;Type&gt; mat(2, 3); mat.setOnes(); mat.row(1) &lt;&lt; 5, 5, 5; mat(0, 2) = mat.row(1).sum(); return mat; } template&lt;class Type&gt; Type logistic(Type tsigma) { return 1 / (1 + exp (-tsigma)); } We then import it into code/linreg_extended.cpp. #include &lt;TMB.hpp&gt; //import the TMB template #include &quot;../functions/utils_linreg_extended.cpp&quot; template&lt;class Type&gt; Type objective_function&lt;Type&gt;::operator() () { DATA_VECTOR(y); // Data vector y passed from R DATA_VECTOR(x); // Data vector x passed from R PARAMETER(a); // Parameter a passed from R PARAMETER(b); // Parameter b passed from R PARAMETER(tsigma); // Parameter sigma (transformed by logistic function) // constrained on [0, 1] // passed from R // Transform tsigma back to natural scale with our external function Type sigma = logistic(tsigma); // Declare negative log-likelihood Type nll = - sum(dnorm(y, a + b * x, sigma, true)); // Necessary for inference on sigma, not only tsigma ADREPORT(sigma); /* This is a useless example to show how to manipulate matrices * in C++ * This creates a matrix of 2 rows and 3 columns. * The Eigen library is used to manipulate vectors, arrays, matrices... */ matrix&lt;Type&gt; mat_example(2, 3); mat_example &lt;&lt; 1, 2, 3, 4, 5, 6; mat_example.setOnes(); mat_example(1, 2) = sigma; matrix&lt;Type&gt; mat_example2 = function_example(mat_example); // This lets us retrieve any variables in a nice format REPORT(mat_example); REPORT(mat_example2); return nll; } And eventually we can use it in R, as shown in this minimal example. # Loading TMB package library(TMB) # Compilation. The compiler returns 0 if the compilation of # the cpp file was successful TMB::compile(&quot;code/linreg_extended.cpp&quot;) ## [1] 0 # Dynamic loading of the compiled cpp file dyn.load(dynlib(&quot;code/linreg_extended&quot;)) # Generate the data for our test sample set.seed(123) sigma &lt;- 0.6 data &lt;- list(y = rnorm(20, sd = sigma) + 1:20, x = 1:20) tsigma &lt;- log(sigma / (1 - sigma)) # Logit transform parameters &lt;- list(a = 0, b = 0, tsigma = 0.1) # Instruct TMB to create the likelihood function obj_linreg &lt;- MakeADFun(data, parameters, DLL = &quot;linreg_extended&quot;, silent = TRUE) # Optimization of the objective function with nlminb mod_linreg &lt;- nlminb(obj_linreg$par, obj_linreg$fn, obj_linreg$gr, obj_linreg$he) # Objects returned by ADREPORT() in C++ summary(sdreport(obj_linreg), select = &quot;report&quot;) ## Estimate Std. Error ## sigma 0.566107 0.08950934 # Object obj_linreg$report() ## $mat_example ## [,1] [,2] [,3] ## [1,] 1 1 1.0000000 ## [2,] 1 1 0.5658614 ## ## $mat_example2 ## [,1] [,2] [,3] ## [1,] 1 1 15 ## [2,] 5 5 5 Note that when you are writing the C++ nll file, compiling the file again may lead to an error. This is because the dynamic library (the .dll file) is already loaded, and cannot be overwritten. To prevent this, it is useful either to manually unload the file via the code dyn.unload(dynlib(&quot;code/linreg_extended&quot;)) or to restart the R session via the menu Session-&gt;Restart R (shortcut Ctrl+Shift+F10 on Windows). Note that restarting the session unloads all packages from the session, and will require you to load the TMB package again. "],["parameter-estimation-techniques-for-hmms.html", " 3 Parameter estimation techniquesfor HMMs 3.1 Basic notation and model setup 3.2 The likelihood function of an HMM 3.3 Forward algorithm andbackward algorithm 3.4 Reparameterization of thelikelihood function", " 3 Parameter estimation techniquesfor HMMs In this section we recall basic concepts underlying parameter estimation for HMMs via direct numerical optimization of the likelihood. In terms of notation, we stay as close as possible to Zucchini, MacDonald, and Langrock (2016), where a more detailed presentation is available. 3.1 Basic notation and model setup The conditional distribution densities can be called in R as well as in C++ (provided that the TMB template is respected) via dpois(x = 1, lambda = 5) # Poisson ## [1] 0.03368973 dnorm(x = 1, mean = 0, sd = 1) # Normal ## [1] 0.2419707 Other distributions are available. We let TPM denote the transition probability matrix, and reference it with the variable \\({\\boldsymbol\\Gamma}\\). 3.2 The likelihood function of an HMM The Poisson HMM negative log-likelihood function in C++ can be written the following way using TMB’s template. #include &lt;TMB.hpp&gt; #include &quot;../functions/utils.cpp&quot; // Likelihood for a poisson hidden markov model. template&lt;class Type&gt; Type objective_function&lt;Type&gt;::operator() () { // Data DATA_VECTOR(x); // timeseries vector DATA_INTEGER(m); // Number of states m // Parameters PARAMETER_VECTOR(tlambda); // conditional log_lambdas&#39;s PARAMETER_VECTOR(tgamma); // m(m-1) working parameters of TPM // Uncomment only when using a non-stationary distribution //PARAMETER_VECTOR(tdelta); // transformed stationary distribution, // Transform working parameters to natural parameters: vector&lt;Type&gt; lambda = tlambda.exp(); matrix&lt;Type&gt; gamma = gamma_w2n(m, tgamma); // Construct stationary distribution vector&lt;Type&gt; delta = stat_dist(m, gamma); // If using a non-stationary distribution, use this instead //vector&lt;Type&gt; delta = delta_w2n(m, tdelta); // Get number of timesteps (n) int n = x.size(); // Evaluate conditional distribution: Put conditional // probabilities of observed x in n times m matrix // (one column for each state, one row for each datapoint): matrix&lt;Type&gt; emission_probs(n, m); matrix&lt;Type&gt; row1vec(1, m); row1vec.setOnes(); for (int i = 0; i &lt; n; i++) { if (x[i] != x[i]) { // f != f returns true if and only if f is NaN. // Replace missing values (NA in R, NaN in C++) with 1 emission_probs.row(i) = row1vec; } else { emission_probs.row(i) = dpois(x[i], lambda, false); } } // Corresponds to (Zucchini et al., 2016, p 333) matrix&lt;Type&gt; foo, P; Type mllk, sumfoo, lscale; foo = (delta * vector&lt;Type&gt;(emission_probs.row(0))).matrix(); sumfoo = foo.sum(); lscale = log(sumfoo); foo.transposeInPlace(); foo /= sumfoo; for (int i = 2; i &lt;= n; i++) { P = emission_probs.row(i - 1); foo = ((foo * gamma).array() * P.array()).matrix(); sumfoo = foo.sum(); lscale += log(sumfoo); foo /= sumfoo; } mllk = -lscale; // Use adreport on variables for which we want standard errors ADREPORT(lambda); ADREPORT(gamma); ADREPORT(delta); // Variables we need for local decoding and in a convenient format REPORT(lambda); REPORT(gamma); REPORT(delta); REPORT(n); REPORT(emission_probs); REPORT(mllk); return mllk; } The case where \\(m = 1\\) doesn’t involve a hidden state, and thus is a Poisson regression instead of a Poisson HMM. Nonetheless, TMB also accelerates its estimation and may be useful to the reader. 3.3 Forward algorithm andbackward algorithm 3.3.1 Setup 3.3.1.1 Prepare the model library(TMB) source(&quot;functions/utils.R&quot;) load(&quot;data/fetal-lamb.RData&quot;) TMB::compile(&quot;code/poi_hmm.cpp&quot;) dyn.load(dynlib(&quot;code/poi_hmm&quot;)) lamb_data &lt;- lamb m &lt;- 2 TMB_data &lt;- list(x = lamb_data, m = m) lambda &lt;- c(1, 3) gamma &lt;- matrix(c(0.8, 0.2, 0.2, 0.8), byrow = TRUE, nrow = m) parameters &lt;- pois.HMM.pn2pw(m, lambda, gamma) obj_tmb &lt;- MakeADFun(TMB_data, parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE) mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn, gradient = obj_tmb$gr, hessian = obj_tmb$he) 3.3.1.2 Define variables Given an optimized MakeADFun object obj, we need to setup some variables to compute the probabilities detailed below. # Retrieve the objects at ML value adrep &lt;- obj_tmb$report(obj_tmb$env$last.par.best) delta &lt;- adrep$delta gamma &lt;- adrep$gamma emission_probs &lt;- adrep$emission_probs n &lt;- adrep$n m &lt;- length(delta) mllk &lt;- adrep$mllk Note that \\({\\boldsymbol\\lambda}\\) is not needed as we already have access to the emission probabilities. 3.3.1.3 Emission/output probabilities Emission probabilities (also called output probabilities) are the conditional probabilities of the data given a state. Using the notation in the article Section 3.1, emission probabilities are the conditional distributions \\(p_i(x) = \\text{P}(X_t = x \\vert C_t = i) = \\frac{e^{-\\lambda_i} \\lambda_i^x}{x!}\\). We store all these in a data frame where each row and column represent a data and a state respectively. More formally, we call \\(\\begin{pmatrix} p_1(x_1) &amp; p_2(x_1) &amp; \\ldots &amp; p_m(x_1)\\\\ p_1(x_2) &amp; p_2(x_2) &amp; \\ldots &amp; p_m(x_2)\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ p_1(x_n) &amp; p_2(x_n) &amp; \\ldots &amp; p_m(x_n)\\\\ \\end{pmatrix}\\) the emission probability matrix. We compute these probabilities in C++ rather than in R because it is faster and not more complicated to write // Evaluate conditional distribution: Put conditional // probabilities of observed x in n times m matrix // (one column for each state, one row for each datapoint): matrix&lt;Type&gt; emission_probs(n, m); matrix&lt;Type&gt; row1vec(1, m); row1vec.setOnes(); for (int i = 0; i &lt; n; i++) { if (x[i] != x[i]) { // f != f returns true if and only if f is NaN. // Replace missing values (NA in R, NaN in C++) with 1 emission_probs.row(i) = row1vec; } else { emission_probs.row(i) = dpois(x[i], lambda, false); } } Nevertheless, we also need to compute them in R to Forecast # Calculate emission probabilities get.emission.probs &lt;- function(data, lambda) { n &lt;- length(data) m &lt;- length(lambda) emission_probs &lt;- matrix(0, nrow = n, ncol = m) for (i in 1:n) { if (is.na(data[i])) { emission_probs[i, ] &lt;- rep(1, m) } else { emission_probs[i, ] &lt;- dpois(data[i], lambda) } } return(emission_probs) } 3.3.2 Log-forward probabilities We show here a way to compute the log of the forward probabilities, using a scaling scheme defined by Zucchini, MacDonald, and Langrock (2016, 66 and p.334). # Compute log-forward probabilities (scaling used) lalpha &lt;- matrix(NA, m, n) foo &lt;- delta * emission_probs[1, ] sumfoo &lt;- sum(foo) lscale &lt;- log(sumfoo) foo &lt;- foo / sumfoo lalpha[, 1] &lt;- log(foo) + lscale for (i in 2:n) { foo &lt;- foo %*% gamma * emission_probs[i, ] sumfoo &lt;- sum(foo) lscale &lt;- lscale + log(sumfoo) foo &lt;- foo / sumfoo lalpha[, i] &lt;- log(foo) + lscale } # lalpha contains n=240 columns, so we only display 5 for readability lalpha[, 1:5] ## [,1] [,2] [,3] [,4] [,5] ## [1,] -0.2920639 -0.5591179 -0.8265948 -1.094088 -1.361583 ## [2,] -6.4651986 -7.7716769 -8.1146145 -8.385232 -8.652850 3.3.3 Log-backward probabilities We show here a way to compute the log of the backward probabilities with a scaling scheme defined by Zucchini, MacDonald, and Langrock (2016, 67 and p.334). # Compute log-backwards probabilities (scaling used) lbeta &lt;- matrix(NA, m, n) lbeta[, n] &lt;- rep(0, m) foo &lt;- rep (1 / m, m) lscale &lt;- log(m) for (i in (n - 1):1) { foo &lt;- gamma %*% (emission_probs[i + 1, ] * foo) lbeta[, i] &lt;- log(foo) + lscale sumfoo &lt;- sum(foo) foo &lt;- foo / sumfoo lscale &lt;- lscale + log(sumfoo) } # lbeta contains n=240 columns, so we only display 4 for readability lbeta[, 1:4] ## [,1] [,2] [,3] [,4] ## [1,] -177.2275 -176.9600 -176.6925 -176.4250 ## [2,] -178.3456 -178.0781 -177.8099 -177.5253 We refer to the Section State Inference for an application of this code for state inference and forecasting. 3.4 Reparameterization of thelikelihood function 3.4.1 Utility functions in TMB Defining the negative log-likelihood function requires transforming the working parameters into their natural form. We define the function gamma.w2n to perform this transformation. The function stat.dist handles computing the stationary distribution, while delta.w2n can be used if a non-stationary distribution is provided. They are defined in functions/utils.cpp // Function transforming working parameters in initial distribution // to natural parameters template&lt;class Type&gt; vector&lt;Type&gt; delta_w2n(int m, vector&lt;Type&gt; tdelta) { vector&lt;Type&gt; delta(m); vector&lt;Type&gt; foo(m); if (m == 1) return Type(1); // set first element to one. // Fill in the last m - 1 elements with working parameters // and take exponential foo &lt;&lt; Type(1), tdelta.exp(); // normalize delta = foo / foo.sum(); return delta; } // Function transforming the working parameters in TPM to // natural parameters (w2n) template&lt;class Type&gt; matrix&lt;Type&gt; gamma_w2n(int m, vector&lt;Type&gt; tgamma) { // Construct m x m identity matrix matrix&lt;Type&gt; gamma(m, m); gamma.setIdentity(); if (m == 1) return gamma; // Fill offdiagonal elements with working parameters column-wise: int idx = 0; for (int i = 0; i &lt; m; i++){ for (int j = 0; j &lt; m; j++){ if (j != i){ // Fill gamma according to mapping and take exponential gamma(j, i) = tgamma.exp()(idx); idx++; } } } // Normalize each row: vector&lt;Type&gt; cs = gamma.rowwise().sum(); for (int i = 0; i &lt; m; i++) gamma.row(i) /= cs[i]; return gamma; } // Function computing the stationary distribution of a Markov chain template&lt;class Type&gt; vector&lt;Type&gt; stat_dist(int m, matrix&lt;Type&gt; gamma) { // Construct stationary distribution matrix&lt;Type&gt; I(m, m); matrix&lt;Type&gt; U(m, m); matrix&lt;Type&gt; row1vec(1, m); U = U.setOnes(); I = I.setIdentity(); row1vec.setOnes(); matrix&lt;Type&gt; A = I - gamma + U; matrix&lt;Type&gt; Ainv = A.inverse(); matrix&lt;Type&gt; deltamat = row1vec * Ainv; vector&lt;Type&gt; delta = deltamat.row(0); return delta; } Transforming the Poisson means into their natural form can be done simply with the exp function and doesn’t require a dedicated function. 3.4.2 Utility functions in R While TMB requires functions to transform working parameters to their natural form, pre-processing in R requires the inverse transformation. Functions to achieve this are available in Zucchini, MacDonald, and Langrock (2016, 51) and are displayed here with explaining comments for convenience. # Transform Poisson natural parameters to working parameters pois.HMM.pn2pw &lt;- function(m, lambda, gamma, delta = NULL, stationary = TRUE) { tlambda &lt;- log(lambda) foo &lt;- log(gamma / diag(gamma)) tgamma &lt;- as.vector(foo[!diag(m)]) if (stationary) { # If tdelta is set to NULL and returned in the list, # it will cause issues when optimizing with TMB return(list(tlambda = tlambda, tgamma = tgamma)) } else { tdelta &lt;- log(delta[- 1] / delta[1]) # TMB requires a list return(list(tlambda = tlambda, tgamma = tgamma, tdelta = tdelta)) } } This can be broken down into sub-functions if necessary. For the \\({\\boldsymbol\\Gamma}\\) part, we introduce gamma.n2w below. # Function to transform natural parameters to working ones gamma.n2w &lt;- function(m, gamma) { foo &lt;- log(gamma / diag(gamma)) tgamma &lt;- as.vector(foo[!diag(m)]) return(tgamma) } In the case where a non-stationary distribution is specified, transforming \\({\\boldsymbol\\delta}\\) is necessary. For this we use the delta.n2w function. # Function to transform natural parameters to working ones delta.n2w &lt;- function(m, delta) { tdelta &lt;- log(delta[- 1] / delta[1]) return(tdelta) } When assuming a stationary distribution, computing it can be done via the following stat.dist function. # Compute the stationary distribution of a Markov chain # with transition probability gamma stat.dist &lt;- function(gamma) { # The code from Zucchini can crash when dealing with computationally small numbers. # This is likely an approximation error. # For some reason, the result may be a complex number with imaginary part equal to 0. # We can just ignore the imaginary part because the eigenvectors of a real eigenvalue must be real. # In the cases where it happened, solve(t(diag(m) - gamma + 1), rep(1, m)) produced the same result without the imaginary part. first_eigen_row &lt;- Re(eigen(t(gamma))$vectors[, 1]) return(first_eigen_row / sum(first_eigen_row)) } Zucchini, MacDonald, and Langrock (2016, 51) shows that calculating the stationary distribution can be achieved by solving the equation below for \\({\\boldsymbol\\delta}\\), where \\({\\boldsymbol I}_m\\) is the \\(m*m\\) identity matrix, \\({\\boldsymbol U}\\) is a \\(m*m\\) matrix of ones, and \\({\\boldsymbol 1}\\) is a row vector of ones. \\[ {\\boldsymbol\\delta}({\\boldsymbol I}_m - {\\boldsymbol\\Gamma} + {\\boldsymbol U}) = {\\boldsymbol 1} \\] 3.4.2.1 Label switching As the model gets estimated each time, no order is imposed on the states by default. If one needs to aggregate estimates, this can lead to the label switching problem, where states aren’t ordered the same way in each model and are grouped incorrectly. The issue can be relevant when comparing results of different optimizers, initial parameters, or classes of models. To address this, we reorder the states by ascending Poisson means after estimation. Sorting the means is direct, however re-ordering the TPM is not as straightforward. To do so, we take the permutations of the states given by the sorted Poisson means, and permute each row index and column index to its new value. The function pois.HMM.label.order solves the issue and is presented below. # Relabel states by increasing Poisson means pois.HMM.label.order &lt;- function(m, lambda, gamma, delta = NULL, lambda_std_error = NULL, gamma_std_error = NULL, delta_std_error = NULL) { # gamma_vector_indices is used to calculate the indices of the reordered TPM gamma as # a vector for reordering the rows of the complete CI data.frame used for the article. gamma_vector_indices &lt;- 1:(m ^ 2) gamma_vector_matrix &lt;- matrix(gamma_vector_indices, nrow = m, ncol = m) ordered_gamma_vector_matrix &lt;- matrix(0, nrow = m, ncol = m) # Get the indices of the sorted states # according to ascending lambda # sorted_lambda contains the permutations needed ordered_lambda_indices &lt;- order(lambda) ordered_lambda &lt;- lambda[ordered_lambda_indices] names(ordered_lambda) &lt;- NULL # Reorder the TPM according to the switched states # in the sorted lambda ordered_gamma &lt;- matrix(0, nrow = m, ncol = m) for (col in 1:m) { new_col &lt;- which(ordered_lambda_indices == col) for (row in 1:m) { new_row &lt;- which(ordered_lambda_indices == row) ordered_gamma[row, col] &lt;- gamma[new_row, new_col] # Reorder the vector TPM ordered_gamma_vector_matrix[row, col] &lt;- gamma_vector_matrix[new_row, new_col] } } # Same for the TPM&#39;s standard errors ordered_gamma_std_error &lt;- NULL if (!is.null(gamma_std_error)) { ordered_gamma_std_error &lt;- matrix(0, nrow = m, ncol = m) for (col in 1:m) { new_col &lt;- which(ordered_lambda_indices == col) for (row in 1:m) { new_row &lt;- which(ordered_lambda_indices == row) ordered_gamma_std_error[row, col] &lt;- gamma_std_error[new_row, new_col] } } } # Reorder the stationary distribution if it is provided # Generate it otherwise if (is.null(delta)) { ordered_delta &lt;- stat.dist(ordered_gamma) } else { ordered_delta &lt;- delta[ordered_lambda_indices] } # Reorder the standard errors ordered_lambda_std_error &lt;- lambda_std_error[ordered_lambda_indices] ordered_delta_std_error &lt;- delta_std_error[ordered_lambda_indices] # The vector is assumed filled column-wise instead of row-wise, # because column-wise is the default way R handles matrix to vector conversion. # Change to row-wise if needed by replacing ordered_gamma_vector_matrix with # t(ordered_gamma_vector_matrix), or add byrow=TRUE # to &quot;ordered_gamma_vector_matrix &lt;- matrix(0, nrow = m, ncol = m)&quot; # We don&#39;t use it in case there is a bug, but it makes logical sense that it should work ordered_gamma_vector_matrix &lt;- as.numeric(ordered_gamma_vector_matrix) result &lt;- list(lambda = ordered_lambda, gamma = ordered_gamma, delta = ordered_delta, lambda_std_error = ordered_lambda_std_error, gamma_std_error = ordered_gamma_std_error, delta_std_error = ordered_delta_std_error, ordered_lambda_indices = ordered_lambda_indices, ordered_gamma_vector_indices = ordered_gamma_vector_matrix, # delta and lambda are the same size, so they are ordered the same way ordered_delta_indices = ordered_lambda_indices) # Remove the NULL elements result[sapply(result, is.null)] &lt;- NULL return(result) } We will go through an example to better understand the process. For readability, the TPM is filled with standard row and column indices instead of probabilities. lambda &lt;- c(3, 1, 2) gamma &lt;- matrix(c(11, 12, 13, 21, 22, 23, 31, 32, 33), byrow = TRUE, ncol = 3) pois.HMM.label.order(m = 3, lambda, gamma) ## $lambda ## [1] 1 2 3 ## ## $gamma ## [,1] [,2] [,3] ## [1,] 33 31 32 ## [2,] 13 11 12 ## [3,] 23 21 22 ## ## $delta ## [1] 0.3482817 0.3183850 0.3333333 ## ## $ordered_lambda_indices ## [1] 2 3 1 ## ## $ordered_gamma_vector_indices ## [1] 9 7 8 3 1 2 6 4 5 ## ## $ordered_delta_indices ## [1] 2 3 1 State 1 has been relabeled state 3, state 3 became state 2, and state 2 became state 1. Another way to address this issue is by parameterizing in terms of non-negative increments \\(\\lambda_j - \\lambda_{j-1}\\) with \\(\\lambda_0 \\equiv 0\\), as explained by Zucchini, MacDonald, and Langrock (2016, sec. 7.1.1 p. 112). However, Bulla and Berzel (2008, sec. 3.2 p. 7) shows this can impose optimization issues: “over all series, the simplest parameterization, i.e., the use of log-transformed state-dependent parameters, leads to the best results as regards the number of failures and the convergence to the global maximum”. These utility functions or subroutines are not complicated, but as you can see, they would cloud up your main code. Therefore, we put them in functions we can call from our main program. "],["using-tmb.html", " 4 Using TMB 4.1 Likelihood function 4.2 Optimization 4.3 Basic nested modelspecification 4.4 State inference and forecasting 4.5 Appendix", " 4 Using TMB We are now ready to present the main TMB code for calculating the negative log-likelihood of an HMM. 4.1 Likelihood function The function is defined in code/poi_hmm.cpp and imports the utility functions defined in functions/utils.cpp. #include &lt;TMB.hpp&gt; #include &quot;../functions/utils.cpp&quot; // Likelihood for a poisson hidden markov model. template&lt;class Type&gt; Type objective_function&lt;Type&gt;::operator() () { // Data DATA_VECTOR(x); // timeseries vector DATA_INTEGER(m); // Number of states m // Parameters PARAMETER_VECTOR(tlambda); // conditional log_lambdas&#39;s PARAMETER_VECTOR(tgamma); // m(m-1) working parameters of TPM // Uncomment only when using a non-stationary distribution //PARAMETER_VECTOR(tdelta); // transformed stationary distribution, // Transform working parameters to natural parameters: vector&lt;Type&gt; lambda = tlambda.exp(); matrix&lt;Type&gt; gamma = gamma_w2n(m, tgamma); // Construct stationary distribution vector&lt;Type&gt; delta = stat_dist(m, gamma); // If using a non-stationary distribution, use this instead //vector&lt;Type&gt; delta = delta_w2n(m, tdelta); // Get number of timesteps (n) int n = x.size(); // Evaluate conditional distribution: Put conditional // probabilities of observed x in n times m matrix // (one column for each state, one row for each datapoint): matrix&lt;Type&gt; emission_probs(n, m); matrix&lt;Type&gt; row1vec(1, m); row1vec.setOnes(); for (int i = 0; i &lt; n; i++) { if (x[i] != x[i]) { // f != f returns true if and only if f is NaN. // Replace missing values (NA in R, NaN in C++) with 1 emission_probs.row(i) = row1vec; } else { emission_probs.row(i) = dpois(x[i], lambda, false); } } // Corresponds to (Zucchini et al., 2016, p 333) matrix&lt;Type&gt; foo, P; Type mllk, sumfoo, lscale; foo = (delta * vector&lt;Type&gt;(emission_probs.row(0))).matrix(); sumfoo = foo.sum(); lscale = log(sumfoo); foo.transposeInPlace(); foo /= sumfoo; for (int i = 2; i &lt;= n; i++) { P = emission_probs.row(i - 1); foo = ((foo * gamma).array() * P.array()).matrix(); sumfoo = foo.sum(); lscale += log(sumfoo); foo /= sumfoo; } mllk = -lscale; // Use adreport on variables for which we want standard errors ADREPORT(lambda); ADREPORT(gamma); ADREPORT(delta); // Variables we need for local decoding and in a convenient format REPORT(lambda); REPORT(gamma); REPORT(delta); REPORT(n); REPORT(emission_probs); REPORT(mllk); return mllk; } Note that we use similar names as to the TMB side. 4.2 Optimization Before we can fit the model, we need to load some necessary packages and data files. We also need to compile the C++ code and load the functions into our working environment in R. We start by compiling the C++ code for computing the likelihood and its gradients. Once it is compiled, we can load the TMB likelihood object into R – making it available from R. # Load TMB and optimization packages library(TMB) # Run the C++ file containing the TMB code TMB::compile(&quot;code/poi_hmm.cpp&quot;) # Load it dyn.load(dynlib(&quot;code/poi_hmm&quot;)) Next, we need to load the necessary packages and the utility R functions. # Load the parameter transformation function source(&quot;functions/utils.R&quot;) The data are part of a large data set collected with the “Track Your Tinnitus” (TYT) mobile application, a detailed description of which is presented in Pryss, Reichert, Langguth, et al. (2015) and Pryss, Reichert, Herrmann, et al. (2015). We analyze 87 successive days of the “arousal” variable, which is measured on a discrete scale. Higher values correspond to a higher degree of excitement, lower values to a more calm emotional state (for details, see Probst et al. 2016). The data can be loaded by a simple call. load(&quot;data/tinnitus.RData&quot;) Figure 4.1 presents the raw data in the Table below, which are also available for download at data/tinnitus.RData. TYT data. Observations collected by the TYT app on 87 successive days (from left to right). 6 5 3 6 4 3 5 6 6 6 4 6 6 4 6 6 6 6 6 4 6 5 6 7 6 5 5 5 7 6 5 6 5 6 6 6 5 6 7 7 6 7 6 6 6 6 5 7 6 1 6 0 2 1 6 7 6 6 6 5 5 6 6 2 5 0 1 1 1 2 3 1 3 1 3 0 1 1 1 4 1 4 1 2 2 2 0 Figure 4.1: Plot of observations from TYT app data for 87 succesive days. Initialization of the number of states and starting (or initial) values for the optimization. First, the number of states needs to be determined. As explained by Pohle et al. (2017a), Pohle et al. (2017b), and Zucchini, MacDonald, and Langrock (2016, sec. 6) (to name only a few), usually one would first fit models with a different number of states. Then, these models are evaluated e.g. by means of model selection criteria (as carried out by Leroux and Puterman 1992) or prediction performance (Celeux and Durand 2008). As shown in the appendix, the model selection procedure shows that both AIC and BIC prefer a two-state model over a Poisson regression and over a model with three or four states. Consequently, we focus on the two-state model in the following. The list object TMB_data contains the data and the number of states. # Model with 2 states m &lt;- 2 TMB_data &lt;- list(x = tinn_data, m = m) Secondly, initial values for the optimization procedure need to be defined. Although we will apply unconstrained optimization, we initialize the natural parameters, because this is much more intuitive and practical than handling the working parameters. # Generate initial set of parameters for optimization lambda &lt;- c(1, 3) gamma &lt;- matrix(c(0.8, 0.2, 0.2, 0.8), byrow = TRUE, nrow = m) Transformation from natural to working parameters The previously created initial values are transformed and stored in the list parameters for the optimization procedure. # Turn them into working parameters parameters &lt;- pois.HMM.pn2pw(m, lambda, gamma) Creation of the TMB negative log-likelihood function with its derivatives This object, stored as obj_tmb requires the data, the initial values, and the previously created DLL as input. Setting argument silent = TRUE disables tracing information and is only used here to avoid excessive output. obj_tmb &lt;- MakeADFun(TMB_data, parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE) This object also contains the previously defined initial values as a vector (par) rather than a list. The negative log-likelihood (fn), its gradient (gr), and Hessian (he) are functions of the parameters (in vector form) while the data are considered fixed. These functions are available to the user and can be evaluated for instance at the initial parameter values: obj_tmb$par ## tlambda tlambda tgamma tgamma ## 0.000000 1.098612 -1.386294 -1.386294 obj_tmb$fn(obj_tmb$par) ## [1] 228.3552 obj_tmb$gr(obj_tmb$par) ## [,1] [,2] [,3] [,4] ## [1,] -3.60306 -146.0336 10.52832 -1.031706 obj_tmb$he(obj_tmb$par) ## [,1] [,2] [,3] [,4] ## [1,] 1.902009 -5.877900 -1.3799682 2.4054017 ## [2,] -5.877900 188.088247 -4.8501589 2.3434284 ## [3,] -1.379968 -4.850159 9.6066700 -0.8410438 ## [4,] 2.405402 2.343428 -0.8410438 0.7984216 Execution of the optimization For this step we rely again on the optimizer implemented in the nlminb function. The arguments, i.e.~ initial values for the parameters and the function to be optimized, are extracted from the previously created TMB object. mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn) # Check that it converged successfully mod_tmb$convergence == 0 ## [1] TRUE It is noteworthy that various alternatives to nlminb exist. Nevertheless, we focus on this established optimization routine because of its high speed of convergence. Obtaining ML estimates Obtaining the ML estimates of the natural parameters together with their standard errors is possible by using the previously introduced command sdreport. Recall that this requires the parameters of interest to be treated by the ADREPORT statement in the C++ part. It should be noted that the presentation of the set of parameters gamma below results from a column-wise representation of the TPM. summary(sdreport(obj_tmb, par.fixed = mod_tmb$par), &quot;report&quot;) ## Estimate Std. Error ## lambda 1.63641070 0.27758294 ## lambda 5.53309626 0.31876141 ## gamma 0.94980192 0.04374682 ## gamma 0.02592209 0.02088689 ## gamma 0.05019808 0.04374682 ## gamma 0.97407791 0.02088689 ## delta 0.34054163 0.23056401 ## delta 0.65945837 0.23056401 Note that the table above also contains estimation results for \\({\\boldsymbol\\delta}\\) and accompanying standard errors, although \\({\\boldsymbol\\delta}\\) is not estimated, but derived from \\({\\boldsymbol\\Gamma}\\). We provide further details on this aspect in Confidence intervals. The value of the nll function in the minimum found by the optimizer can also be extracted directly from the object mod_tmb by accessing the list element objective: mod_tmb$objective ## [1] 168.5361 Use exact gradient and Hessian In the optimization above we already benefited from an increased speed due to the evaluation of the nll in C++ compared to the forward algorithm being executed entirely in R. However, since TMB computes the gradient and hessian of the likelihood, we can provide this information to the optimizer This is in general advisable, because TMB provides an exact value of both gradient and Hessian up to machine precision, which is superior to approximations used by the optimizing procedure. Similar to the nll, both quantities can be extracted directly from the TMB object obj_tmb: # The negative log-likelihood is accessed by the objective # attribute of the optimized object mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn, gradient = obj_tmb$gr, hessian = obj_tmb$he) mod_tmb$objective ## [1] 168.5361 Note that passing the exact gradient and Hessian as provided by TMB to nlminb leads to the same minimum, i.e. value of the nll function, here. Is it noteworthy that inconsistencies can happen in the estimates due to computer approximations. The stationary distribution is a vector of probabilities and should sum to 1. However, it doesn’t always behave as expected. adrep &lt;- summary(sdreport(obj_tmb), &quot;report&quot;) estimate_delta &lt;- adrep[rownames(adrep) == &quot;delta&quot;, &quot;Estimate&quot;] sum(estimate_delta) ## [1] 1 sum(estimate_delta) == 1 # The sum is displayed as 1 but is not 1 ## [1] FALSE As noted on Zucchini, MacDonald, and Langrock (2016, 159–60), ``the row sums of \\({\\boldsymbol\\Gamma}\\) will only approximately equal 1, and the components of the vector \\({\\boldsymbol\\delta}\\) will only approximately total 1. This can be remedied by scaling the vector \\({\\boldsymbol\\delta}\\) and each row of \\({\\boldsymbol\\Gamma}\\) to total 1’’. We do not remedy this issue because it provides no benefit to us, but this may lead to surprising results when checking equality. This is likely due to machine approximations when numbers far apart from each other interact together. In R, a small number is not 0 but is treated as 0 when added to a much larger number. This can result in incoherent findings when checking equality between 2 numbers. 1e-100 == 0 # Small numbers are 0 ## [1] FALSE (1 + 1e-100) == 1 ## [1] TRUE 4.3 Basic nested modelspecification 4.3.1 Principle As a first step in building a nested model, we arbitrarily fix \\(\\lambda_1\\) to the value 1. # Get the previous values, and fix some fixed_par_lambda &lt;- lambda fixed_par_lambda[1] &lt;- 1 # Transform them into working parameters new_parameters &lt;- pois.HMM.pn2pw(m = m, lambda = fixed_par_lambda, gamma = gamma) Then, we instruct TMB to read these custom parameters. We indicate fixed values by mapping them to NA values, whereas the variable values need to be mapped to different factor variables. Lastly, we specify this mapping with the map argument when making the TMB object. map &lt;- list(tlambda = as.factor(c(NA, 1)), tgamma = as.factor(c(2, 3))) fixed_par_obj_tmb &lt;- MakeADFun(TMB_data, new_parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE, map = map) Estimation of the model and displaying the results is performed as usual. fixed_par_mod_tmb &lt;- nlminb(start = fixed_par_obj_tmb$par, objective = fixed_par_obj_tmb$fn, gradient = fixed_par_obj_tmb$gr, hessian = fixed_par_obj_tmb$he) summary(sdreport(fixed_par_obj_tmb), &quot;report&quot;) ## Estimate Std. Error ## lambda 1.00000000 0.00000000 ## lambda 5.50164872 0.30963641 ## gamma 0.94561055 0.04791050 ## gamma 0.02655944 0.02133283 ## gamma 0.05438945 0.04791050 ## gamma 0.97344056 0.02133283 ## delta 0.32810136 0.22314460 ## delta 0.67189864 0.22314460 Note that the standard error of \\(\\lambda_1\\) is zero, because it is no longer considered a variable parameter and does not enter the optimization procedure. 4.3.2 Limits This method cannot work in general for working parameters which are not linked to a single natural parameter. This is because only working parameters can be fixed with this method, but the working parameters of the TPM are not each linked to a single natural parameter. As an example, fixing the natural parameter \\(\\gamma_{11}\\) is not equivalent to fixing any working parameter \\(\\tau_{ij}\\). Hence, the TPM cannot in general be fixed, except perhaps via constrained optimization. However, if conditions on the natural parameters can be translated to conditions on the working parameters, then there should not be any issue. We refer to the next section for more details. 4.3.3 Parameter equalityconstraints More complex constraints are also possible. For example, imposing equality constraints (such as \\(\\gamma_{11} = \\gamma_{22}\\)) requires the corresponding factor level to be identical for the concerned entries. As a reminder, we defined the working parameters via \\[ \\tau_{ij} = \\log(\\frac{\\gamma_{ij}}{1 - \\sum_{k \\neq i} \\gamma_{ik}}) = \\log(\\gamma_{ij}/\\gamma_{ii}), \\text{ for } i \\neq j \\] With a two-state Poisson HMM, the constraint \\(\\gamma_{11} = \\gamma_{22}\\) is equivalent to \\(\\gamma_{12} = \\gamma_{21}\\). Thus, the constraint can be transformed into \\(\\tau_{12} = \\log(\\gamma_{12}/\\gamma_{11}) = \\log(\\gamma_{21}/\\gamma_{22}) = \\tau_{21}\\). The mapping parameters must be set to a common factor to be forced equal. map &lt;- list(tlambda = as.factor(c(1, 2)), tgamma = as.factor(c(3, 3))) fixed_par_obj_tmb &lt;- MakeADFun(TMB_data, parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE, map = map) fixed_par_mod_tmb &lt;- nlminb(start = fixed_par_obj_tmb$par, objective = fixed_par_obj_tmb$fn, gradient = fixed_par_obj_tmb$gr, hessian = fixed_par_obj_tmb$he) # Results + check that the constraint is respected results &lt;- summary(sdreport(fixed_par_obj_tmb), &quot;report&quot;) tpm &lt;- matrix(results[rownames(results) == &quot;gamma&quot;, &quot;Estimate&quot;], nrow = m, ncol = m, byrow = FALSE) # Transformations are column-wise by default, be careful! tpm ## [,1] [,2] ## [1,] 0.96759216 0.03240784 ## [2,] 0.03240784 0.96759216 tpm[1, 1] == tpm[2, 2] ## [1] TRUE Similar complex constraints on the TPM can also be setup for HMMs with three or more states. However, it appears this can only be achieved in general when the constraint involves natural parameters of the same row (with the exception of a two-state model). We have not found a way to easily implement equality constraints between natural TPM parameters of different rows. A solution might be constrained optimization. As an example, we will look at a three-state HMM with the constraint \\(\\gamma_{12} = \\gamma_{13}\\). # Model with 2 states m &lt;- 3 TMB_data &lt;- list(x = tinn_data, m = m) # Initial set of parameters lambda &lt;- c(1, 3, 5) gamma &lt;- matrix(c(0.8, 0.1, 0.1, 0.1, 0.8, 0.1, 0.1, 0.1, 0.8), byrow = TRUE, nrow = m) # Turn them into working parameters parameters &lt;- pois.HMM.pn2pw(m, lambda, gamma) # Build the TMB object obj_tmb &lt;- MakeADFun(TMB_data, parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE) # Optimize mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn, gradient = obj_tmb$gr, hessian = obj_tmb$he) # Check convergence mod_tmb$convergence == 0 ## [1] TRUE # Results summary(sdreport(obj_tmb), &quot;report&quot;) ## Estimate Std. Error ## lambda 8.281290e-01 5.418824e-01 ## lambda 1.705082e+00 2.949769e-01 ## lambda 5.514886e+00 3.080238e-01 ## gamma 5.516919e-01 3.132490e-01 ## gamma 4.596623e-09 5.115944e-05 ## gamma 2.749570e-02 2.164849e-02 ## gamma 1.989160e-01 2.698088e-01 ## gamma 9.772963e-01 3.436716e-02 ## gamma 2.453591e-10 2.730433e-06 ## gamma 2.493922e-01 2.546636e-01 ## gamma 2.270369e-02 3.436717e-02 ## gamma 9.725043e-01 2.164849e-02 ## delta 3.836408e-02 3.757898e-02 ## delta 3.361227e-01 3.144598e-01 ## delta 6.255132e-01 3.063220e-01 The transformed constraint becomes \\(\\tau_{12} = \\log(\\gamma_{12}/\\gamma_{11}) = \\log(\\gamma_{13}/\\gamma_{11}) = \\tau_{13}\\). We need to be careful how we specify the constraint, because the vector tgamma will be converted into a matrix column-wise since this is R’s default way to handle matrix-vector conversions. The tgamma matrix looks naturally like \\[\\begin{pmatrix} &amp;\\tau_{12}&amp;\\tau_{13}\\\\ \\tau_{21}&amp; &amp;\\tau_{23}\\\\ \\tau_{31}&amp;\\tau_{32}&amp; \\end{pmatrix}\\] As a vector in R, it becomes \\(\\left(\\tau_{21}, \\tau_{31}, \\tau_{12}, \\tau_{32}, \\tau_{13}, \\tau_{23}\\right)\\). Therefore, the constraint needs to be placed on the \\(3^{rd}\\) and \\(5^{th}\\) vector parameter in the same way as we did when the Poisson mean \\(\\lambda_1\\) was fixed. map &lt;- list(tlambda = as.factor(c(1, 2, 3)), tgamma = as.factor(c(4, 5, 6, 7, 6, 8))) fixed_par_obj_tmb &lt;- MakeADFun(TMB_data, parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE, map = map) fixed_par_mod_tmb &lt;- nlminb(start = fixed_par_obj_tmb$par, objective = fixed_par_obj_tmb$fn, gradient = fixed_par_obj_tmb$gr, hessian = fixed_par_obj_tmb$he) # Results + check that the constraint is respected results &lt;- summary(sdreport(fixed_par_obj_tmb), &quot;report&quot;) tpm &lt;- matrix(results[rownames(results) == &quot;gamma&quot;, &quot;Estimate&quot;], nrow = m, ncol = m, byrow = FALSE) # Transformations are column-wise by default, be careful! tpm ## [,1] [,2] [,3] ## [1,] 5.447127e-01 2.276437e-01 0.22764365 ## [2,] 2.718007e-09 9.753263e-01 0.02467374 ## [3,] 2.710078e-02 2.005408e-10 0.97289922 tpm[1, 2] == tpm[1, 3] ## [1] TRUE Equality constraints involving a diagonal member of the TPM are simpler to specify: the constraint \\(\\gamma_{i,j} = \\gamma_{i,i}\\) becomes transformed to \\(\\tau_{i,j} = 1\\) and this can be specified in the same way the Poisson mean \\(\\lambda_1\\) was fixed. 4.4 State inference and forecasting The following code requires executing the code presented in the Section Forward algorithm and backward algorithm, as the log-forward and log-backward probabilities are needed. 4.4.1 Local decoding The smoothing probabilities are defined in Zucchini, MacDonald, and Langrock (2016, 87 and p.336) as \\[ \\text{P}(C_t = i \\vert X^{(n)} = x^{(n)}) = \\frac{\\alpha_t(i) \\beta_t(i)}{L_n} \\] # Compute conditional state probabilities, smoothing probabilities stateprobs &lt;- matrix(NA, ncol = n, nrow = m) llk &lt;- - mllk n &lt;- length(tinn_data) for(i in 1:n) { stateprobs[, i] &lt;- exp(lalpha[, i] + lbeta[, i] - llk) } Local decoding is a straightforward maximum of the smoothing probabilities. # Most probable states (local decoding) ldecode &lt;- rep(NA, n) for (i in 1:n) { ldecode[i] &lt;- which.max(stateprobs[, i]) } ldecode ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [85] 2 2 2 4.4.2 Forecast The forecast distribution or h-step-ahead-probabilities as well as its implementation in R is detailed in Zucchini, MacDonald, and Langrock (2016, 83 and p.337) Then, \\[ \\text{P}(X_{n+h} = x \\vert X^{(n)} = x^{(n)}) = \\frac{{\\boldsymbol\\alpha}_n {\\boldsymbol\\Gamma}^h \\textbf{P}(x) {\\boldsymbol 1}&#39;}{{\\boldsymbol\\alpha}_n {\\boldsymbol 1}&#39;} = {\\boldsymbol\\Phi}_n {\\boldsymbol\\Gamma}^h \\textbf{P}(x) {\\boldsymbol 1}&#39; \\] An implementation of this using a scaling scheme is # Number of steps h &lt;- 1 # Values for which we want the forecast probabilities xf &lt;- 0:50 nxf &lt;- length(xf) dxf &lt;- matrix(0, nrow = h, ncol = nxf) foo &lt;- delta * emission_probs[1, ] sumfoo &lt;- sum(foo) lscale &lt;- log(sumfoo) foo &lt;- foo / sumfoo for (i in 2:n) { foo &lt;- foo %*% gamma * emission_probs[i, ] sumfoo &lt;- sum( foo) lscale &lt;- lscale + log(sumfoo) foo &lt;- foo / sumfoo } emission_probs_xf &lt;- get.emission.probs(xf, lambda) for (i in 1:h) { foo &lt;- foo %*% gamma for (j in 1:m) { dxf[i, ] &lt;- dxf[i, ] + foo[j] * emission_probs_xf[, j] } } # dxf contains n=240 columns, so we only display 4 for readability dxf[, 1:4] ## [1] 0.1595358 0.2247547 0.2102057 0.1678969 4.4.3 Global decoding The Viterbi algorithm is detailed in Zucchini, MacDonald, and Langrock (2016, 88 and p.334). It calculates the sequence of states \\((i_1^*, \\ldots, i_T^*)\\) which maximizes the conditional probability of all states simultaneously, i.e. \\[ (i_1^*, \\ldots, i_n^*) = \\mathop{\\mathrm{argmax}}_{i_1, \\ldots, i_n \\in \\{1, \\ldots, m \\}} \\text{P}(C_1 = i_1, \\ldots, C_n = i_n \\vert X^{(n)} = x^{(n)}). \\] An implementation of it is xi &lt;- matrix(0, n, m) foo &lt;- delta * emission_probs[1, ] xi[1, ] &lt;- foo / sum(foo) for (i in 2:n) { foo &lt;- apply(xi[i - 1, ] * gamma, 2, max) * emission_probs[i, ] xi[i, ] &lt;- foo / sum(foo) } iv &lt;- numeric(n) iv[n] &lt;- which.max(xi[n, ]) for (i in (n - 1):1){ iv[i] &lt;- which.max(gamma[, iv[i + 1]] * xi[i, ]) } iv ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [85] 2 2 2 4.5 Appendix Model selection tinnitus via AIC and BIC (calculation defined in Zucchini, MacDonald, and Langrock (2016)). AIC and BIC of a Poisson regression m &lt;- 1 TMB_data &lt;- list(x = tinn_data, m = m) lambda &lt;- 2 gamma &lt;- 1 parameters &lt;- pois.HMM.pn2pw(m, lambda, gamma) obj_tmb &lt;- MakeADFun(TMB_data, parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE) mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn) mllk &lt;- mod_tmb$objective np &lt;- length(unlist(parameters)) AIC_1 &lt;- 2 * (mllk + np) n &lt;- sum(!is.na(TMB_data$x)) BIC_1 &lt;- 2 * mllk + np * log(n) mod_tmb$convergence == 0 ## [1] TRUE AIC_1 ## [1] 394.197 BIC_1 ## [1] 396.6629 AIC and BIC of a two-state Poisson HMM m &lt;- 2 TMB_data &lt;- list(x = tinn_data, m = m) lambda &lt;- seq(from = 1, to = 3, length.out = m) # 0.8 on the diagonal, and 0.2 split along the rest of each line, size m gamma &lt;- matrix(0.2 / (m - 1), nrow = m, ncol = m) diag(gamma) &lt;- 0.8 parameters &lt;- pois.HMM.pn2pw(m, lambda, gamma) obj_tmb &lt;- MakeADFun(TMB_data, parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE) mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn) mllk &lt;- mod_tmb$objective np &lt;- length(unlist(parameters)) AIC_2 &lt;- 2 * (mllk + np) n &lt;- sum(!is.na(TMB_data$x)) BIC_2 &lt;- 2 * mllk + np * log(n) mod_tmb$convergence == 0 ## [1] TRUE AIC_2 ## [1] 345.0721 BIC_2 ## [1] 354.9357 AIC and BIC of a three-state Poisson HMM m &lt;- 3 TMB_data &lt;- list(x = tinn_data, m = m) lambda &lt;- seq(from = 1, to = 3, length.out = m) # 0.8 on the diagonal, and 0.2 split along the rest of each line, size m gamma &lt;- matrix(0.2 / (m - 1), nrow = m, ncol = m) diag(gamma) &lt;- 0.8 parameters &lt;- pois.HMM.pn2pw(m, lambda, gamma) obj_tmb &lt;- MakeADFun(TMB_data, parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE) mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn) mllk &lt;- mod_tmb$objective np &lt;- length(unlist(parameters)) AIC_3 &lt;- 2 * (mllk + np) n &lt;- sum(!is.na(TMB_data$x)) BIC_3 &lt;- 2 * mllk + np * log(n) mod_tmb$convergence == 0 ## [1] TRUE AIC_3 ## [1] 353.0463 BIC_3 ## [1] 375.2395 AIC and BIC of a four-state Poisson HMM m &lt;- 4 TMB_data &lt;- list(x = tinn_data, m = m) lambda &lt;- seq(from = 1, to = 3, length.out = m) # 0.8 on the diagonal, and 0.2 split along the rest of each line, size m gamma &lt;- matrix(0.2 / (m - 1), nrow = m, ncol = m) diag(gamma) &lt;- 0.8 parameters &lt;- pois.HMM.pn2pw(m, lambda, gamma) obj_tmb &lt;- MakeADFun(TMB_data, parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE) mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn) mllk &lt;- mod_tmb$objective np &lt;- length(unlist(parameters)) AIC_4 &lt;- 2 * (mllk + np) n &lt;- sum(!is.na(TMB_data$x)) BIC_4 &lt;- 2 * mllk + np * log(n) mod_tmb$convergence == 0 ## [1] TRUE AIC_4 ## [1] 365.6644 BIC_4 ## [1] 405.1189 Summary Models AIC BIC Poisson regression 394.1969962 396.6629043 Two-state Poisson HMM 345.0721117 354.9357442 Three-state Poisson HMM 353.0463375 375.2395106 Four-state Poisson HMM 365.6643762 405.1189061 AIC and BIC prefer a two-state model over a Poisson regression and over three and four-state HMMs. "],["confidence-intervals.html", " 5 Confidence intervals 5.1 Wald-type confidenceintervals based on the Hessian 5.2 Likelihood profile basedconfidence intervals 5.3 Bootstrap-basedconfidence intervals", " 5 Confidence intervals From the parameters’ ML estimates, we generate new data and re-estimate the parameters 1000 times. From that list of new estimates we can get the 2.5th and 97.5th percentiles and get 95% confidence intervals for the parameters. We show below how to derive confidence intervals using TMB, a likelihood profile based method, and parametric bootstrap, based on the 2 state Poisson HMM estimates. For all three methods, we require a model, so we generate a 2-state Poisson HMM based on the TYT dataset # Load TMB and optimization packages library(TMB) # Run the C++ file containing the TMB code TMB::compile(&quot;code/poi_hmm.cpp&quot;) ## [1] 0 # Load it dyn.load(dynlib(&quot;code/poi_hmm&quot;)) # Load the parameter transformation function source(&quot;functions/utils.R&quot;) load(&quot;data/tinnitus.RData&quot;) # Model with 2 states m &lt;- 2 TMB_data &lt;- list(x = tinn_data, m = m) # Generate initial set of parameters for optimization lambda &lt;- c(1, 3) gamma &lt;- matrix(c(0.8, 0.2, 0.2, 0.8), byrow = TRUE, nrow = m) # Turn them into working parameters parameters &lt;- pois.HMM.pn2pw(m, lambda, gamma) obj_tmb &lt;- MakeADFun(TMB_data, parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE) # The negative log-likelihood is accessed by the objective # attribute of the optimized object mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn, gradient = obj_tmb$gr, hessian = obj_tmb$he) mod_tmb$objective ## [1] 168.5361 5.1 Wald-type confidenceintervals based on the Hessian Now that we have a model estimated via TMB, we can derive Wald-type (Wald 1943) confidence intervals. For example, the \\((1 - \\alpha) \\%\\) CI for \\(\\lambda_1\\) is given by \\(\\lambda_1 \\pm z_{1-\\alpha/2} * \\sigma_{\\lambda_1}\\) where \\(z_{x}\\) is the \\(x\\)-percentile of the standard normal distribution, and \\(\\sigma_{\\lambda_1}\\) is the standard error of \\(\\lambda_1\\) obtained via the delta method. First, we require the standard errors. We can retrieve them from the MakeADFun object. The standard errors of the working parameters tlambda and tgamma can be retrieved without needing to add ADREPORT in the C++ file. However, it is usually more interesting to access the standard errors of the natural parameters lambda, gamma and delta. This requires adding a few lines to the C++ file to produce these standard errors, as detailed in [Getting started with a linear regression]. Be careful: adrep lists gamma column-wise. adrep &lt;- summary(sdreport(obj_tmb), &quot;report&quot;) adrep ## Estimate Std. Error ## lambda 1.63641100 0.27758296 ## lambda 5.53309576 0.31876147 ## gamma 0.94980209 0.04374676 ## gamma 0.02592204 0.02088688 ## gamma 0.05019791 0.04374676 ## gamma 0.97407796 0.02088688 ## delta 0.34054200 0.23056437 ## delta 0.65945800 0.23056437 Standard errors for \\(\\hat{{\\boldsymbol\\lambda}}\\): rows &lt;- rownames(adrep) == &quot;lambda&quot; lambda &lt;- adrep[rows, &quot;Estimate&quot;] lambda_std_error &lt;- adrep[rows, &quot;Std. Error&quot;] lambda ## lambda lambda ## 1.636411 5.533096 lambda_std_error ## lambda lambda ## 0.2775830 0.3187615 Standard errors for \\(\\hat{{\\boldsymbol\\Gamma}}\\) rows &lt;- rownames(adrep) == &quot;gamma&quot; gamma &lt;- adrep[rows, &quot;Estimate&quot;] gamma &lt;- matrix(gamma, ncol = m) # Convert to matrix gamma_std_error &lt;- adrep[rows, &quot;Std. Error&quot;] gamma_std_error &lt;- matrix(gamma_std_error, nrow = m, ncol = m) gamma ## [,1] [,2] ## [1,] 0.94980209 0.05019791 ## [2,] 0.02592204 0.97407796 gamma_std_error ## [,1] [,2] ## [1,] 0.04374676 0.04374676 ## [2,] 0.02088688 0.02088688 Standard errors for \\(\\hat{{\\boldsymbol\\delta}}\\): rows &lt;- rownames(adrep) == &quot;delta&quot; delta &lt;- adrep[rows, &quot;Estimate&quot;] delta_std_error &lt;- adrep[rows, &quot;Std. Error&quot;] delta ## delta delta ## 0.340542 0.659458 delta_std_error ## delta delta ## 0.2305644 0.2305644 5.2 Likelihood profile basedconfidence intervals Our nll function is parametrized in terms of and optimized with respect to the working parameters. In practice, this aspect is easy to deal with. Once a profile CI for the working parameter (here \\(\\eta_2\\)) has been obtained following the procedure above, the corresponding CI for the natural parameter \\(\\lambda_2\\) results directly from transforming the upper and lower boundary of the CI for \\(\\eta_2\\) by the one-to-one transformation \\(\\lambda_2 = \\exp(\\eta_2)\\). For further details on the invariance of likelihood-based CIs to parameter transformations, we refer to Meeker and Escobar (1995). Profiling \\(\\eta_2\\) (the working parameter corresponding to \\(\\lambda_2\\)) with TMB can be done with profile &lt;- tmbprofile(obj = obj_tmb, name = 2, trace = FALSE) head(profile) ## tlambda value ## 67 1.589247 170.5801 ## 66 1.592447 170.4783 ## 65 1.595647 170.3790 ## 64 1.598847 170.2821 ## 63 1.602047 170.1877 ## 62 1.605247 170.0957 A plot allows for a visual representation of the profile # par(mgp = c(2, 0.5, 0), mar = c(3, 3, 2.5, 1), # cex.lab = 1.5) plot(profile, level = 0.95, xlab = expression(eta[2]), ylab = &quot;nll&quot;) Then we can infer \\(\\eta_2\\)’s confidence interval, and hence \\(\\lambda_2\\)’s confidence interval confint(profile) ## lower upper ## tlambda 1.593141 1.820641 exp(confint(profile)) ## lower upper ## tlambda 4.919178 6.175815 Further, profiling the TPM is done similarly. However, since individual natural TPM parameters cannot be deduced from single working parameters, we need to profile the entire working TPM then transform it back to a natural TPM. profile3 &lt;- tmbprofile(obj = obj_tmb, name = 3, trace = FALSE) profile4 &lt;- tmbprofile(obj = obj_tmb, name = 4, trace = FALSE) # Obtain confidence intervals for working parameters tgamma_3_confint &lt;- confint(profile3) tgamma_4_confint &lt;- confint(profile4) # Group lower bounds and upper bounds lower &lt;- c(tgamma_3_confint[1], tgamma_4_confint[1]) upper &lt;- c(tgamma_3_confint[2], tgamma_4_confint[2]) # Infer bounds on natural parameters gamma_1 &lt;- gamma.w2n(m, lower) gamma_2 &lt;- gamma.w2n(m, upper) # Display unsorted lower and upper bounds gamma_1 ## [,1] [,2] ## [1,] 0.99800287 0.001997133 ## [2,] 0.00154545 0.998454550 gamma_2 ## [,1] [,2] ## [1,] 0.81653046 0.1834695 ## [2,] 0.08801416 0.9119858 # Sorted confidence interval for gamma_11 sort(c(gamma_1[1, 1], gamma_2[1, 1])) ## [1] 0.8165305 0.9980029 It is noteworthy that gamma_1 is not necessarily lower than gamma_2, because only the working confidence intervals are automatically sorted. Also note that linear combinations of parameters can be profiled by passing the lincomb argument. More details are available by executing ??TMB::tmbprofile to access the tmbprofile’s help. The name parameter should be the index of a parameter to profile. While the function’s help mentions it can be a parameter’s name, the first two parameters are both named tlambda as can be seen here mod_tmb$par ## tlambda tlambda tgamma tgamma ## 0.4925054 1.7107475 -3.6263979 -2.9402803 5.3 Bootstrap-basedconfidence intervals 5.3.1 Generating data In order to perform a parametric bootstrap, we need to be able to generate data from a set of parameters. For readability and code maintenance, it is conventional to store procedures that will be used more than once into functions. The data-generating function is defined in Zucchini, MacDonald, and Langrock (2016, secs. A.1.5 p.333). pois.HMM.generate.sample &lt;- function(ns,mod) { mvect &lt;- 1:mod$m state &lt;- numeric(ns) state[1] &lt;- sample(mvect, 1, prob = mod$delta) for (i in 2:ns) { state[i] &lt;- sample(mvect, 1, prob = mod$gamma[state[i - 1], ]) } x &lt;- rpois(ns, lambda = mod$lambda[state]) return(x) } Further, one can retrieve the state sequence used to generate the data by performing an adjustment: # Generate a random sample from a HMM pois.HMM.generate.sample &lt;- function(ns, mod) { mvect &lt;- 1:mod$m state &lt;- numeric(ns) state[1] &lt;- sample(mvect, 1, prob = mod$delta) for (i in 2:ns) { state[i] &lt;- sample(mvect, 1, prob = mod$gamma[state[i - 1], ]) } x &lt;- rpois(ns, lambda = mod$lambda[state]) return(list(data = x, state = state)) } The results are returned in a list to simplify usage, as the intuitive way (c(x, state)) would append the state sequence to the data. In practice, HMMs sometimes cannot be estimated on generated samples. To deal with this, we can generate a new sample as long as HMMs cannot be estimated on it, with the help of this more resilient function which can easily be adapted to different needs. Natural parameter estimates are returned for convenience. The argument test_marqLevAlg decides if convergence of marqLevAlg is required. The argument std_error decides if standard errors are returned along with TMB’s estimates. This function relies on the custom function TMB.estimate which is a wrapper of nlminb made for Poisson HMM estimation via TMB. # Generate a random sample from a HMM pois.HMM.generate.estimable.sample &lt;- function(ns, mod, testing_params, params_names = PARAMS_NAMES, std_error = FALSE) { if(anyNA(c(ns, mod, testing_params))) { stop(&quot;Some parameters are missing in pois.HMM.generate.estimable.sample&quot;) } # Count occurrences for each error failure &lt;- c(&quot;state_number&quot; = 0, &quot;TMB_null&quot; = 0, &quot;TMB_converge&quot; = 0, &quot;TMB_G_null&quot; = 0, &quot;TMB_G_converge&quot; = 0, &quot;TMB_H_null&quot; = 0, &quot;TMB_H_converge&quot; = 0, &quot;TMG_GH_null&quot; = 0, &quot;TMG_GH_converge&quot; = 0, &quot;NA_value&quot; = 0) m &lt;- mod$m # Loop as long as there is an issue with nlminb repeat { mod_temp &lt;- NULL #simulate the data new_data &lt;- pois.HMM.generate.sample(ns = ns, mod = mod) # If the number of states generated is different from m, discard the data if (length(unique(new_data$state)) != m) { failure[&quot;state_number&quot;] &lt;- failure[&quot;state_number&quot;] + 1 next } TMB_new_data &lt;- list(x = new_data$data, m = m) testing_w_params &lt;- pois.HMM.pn2pw(m = m, lambda = testing_params$lambda, gamma = testing_params$gamma, delta = testing_params$delta) # Test TMB suppressWarnings(mod_temp &lt;- TMB.estimate(TMB_data = TMB_new_data, parameters = testing_w_params, std_error = std_error)) # If nlminb doesn&#39;t reach any result, discard the data if (is.null(mod_temp)) { failure[&quot;TMB_null&quot;] &lt;- failure[&quot;TMB_null&quot;] + 1 next } # If nlminb doesn&#39;t converge successfully, discard the data if (mod_temp$convergence != 0) { failure[&quot;TMB_converge&quot;] &lt;- failure[&quot;TMB_converge&quot;] + 1 next } # Test TMB_G suppressWarnings(mod_temp &lt;- TMB.estimate(TMB_data = TMB_new_data, parameters = testing_w_params, gradient = TRUE, std_error = std_error)) # If nlminb doesn&#39;t reach any result, discard the data if (is.null(mod_temp)) { failure[&quot;TMB_G_null&quot;] &lt;- failure[&quot;TMB_G_null&quot;] + 1 next } # If nlminb doesn&#39;t converge successfully, discard the data if (mod_temp$convergence != 0) { failure[&quot;TMB_G_converge&quot;] &lt;- failure[&quot;TMB_G_converge&quot;] + 1 next } # Test TMB_H suppressWarnings(mod_temp &lt;- TMB.estimate(TMB_data = TMB_new_data, parameters = testing_w_params, hessian = TRUE, std_error = std_error)) # If nlminb doesn&#39;t reach any result, discard the data if (is.null(mod_temp)) { failure[&quot;TMB_H_null&quot;] &lt;- failure[&quot;TMB_H_null&quot;] + 1 next } # If nlminb doesn&#39;t converge successfully, discard the data if (mod_temp$convergence != 0) { failure[&quot;TMB_H_converge&quot;] &lt;- failure[&quot;TMB_H_converge&quot;] + 1 next } # Test TMB_GH suppressWarnings(mod_temp &lt;- TMB.estimate(TMB_data = TMB_new_data, parameters = testing_w_params, gradient = TRUE, hessian = TRUE, std_error = std_error)) # If nlminb doesn&#39;t reach any result, discard the data if (is.null(mod_temp)) { failure[&quot;TMB_GH_null&quot;] &lt;- failure[&quot;TMB_GH_null&quot;] + 1 next } # If nlminb doesn&#39;t converge successfully, discard the data if (mod_temp$convergence != 0) { failure[&quot;TMB_GH_converge&quot;] &lt;- failure[&quot;TMB_GH_converge&quot;] + 1 next } natural_parameters &lt;- list(m = m, lambda = mod_temp$lambda, gamma = mod_temp$gamma, delta = mod_temp$delta, lambda_std_error = mod_temp$lambda_std_error, gamma_std_error = mod_temp$gamma_std_error, delta_std_error = mod_temp$delta_std_error) # If some parameters are NA for some reason, discard the data if (anyNA(natural_parameters[params_names], recursive = TRUE)) { failure[&quot;NA_value&quot;] &lt;- failure[&quot;NA_value&quot;] + 1 next } # If everything went well, end the &quot;repeat&quot; loop break } return(list(data = new_data$data, states = new_data$state, natural_parameters = natural_parameters, mod = mod_temp, failure = failure)) } 5.3.2 Bootstrap set.seed(123) library(TMB) TMB::compile(&quot;code/poi_hmm.cpp&quot;) dyn.load(dynlib(&quot;code/poi_hmm&quot;)) source(&quot;functions/utils.R&quot;) m &lt;- 2 load(&quot;data/tinnitus.RData&quot;) TMB_data &lt;- list(x = tinn_data, m = m) # Initial set of parameters lambda_init &lt;- c(1, 3) gamma_init &lt;- matrix(c(0.8, 0.2, 0.2, 0.8), byrow = TRUE, nrow = m) # Turn them into working parameters parameters &lt;- pois.HMM.pn2pw(m, lambda_init, gamma_init) # Build the TMB object obj_tmb &lt;- MakeADFun(TMB_data, parameters, DLL = &quot;poi_hmm&quot;, silent = TRUE) # Optimize mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn, gradient = obj_tmb$gr, hessian = obj_tmb$he) # Bootstrap procedure bootstrap_estimates &lt;- data.frame() DATA_SIZE &lt;- length(tinn_data) # Set how many parametric bootstrap samples we create BOOTSTRAP_SAMPLES &lt;- 10 # MLE ML_working_estimates &lt;- obj_tmb$par ML_natural_estimates &lt;- obj_tmb$report(ML_working_estimates) lambda &lt;- ML_natural_estimates$lambda gamma &lt;- ML_natural_estimates$gamma delta &lt;- ML_natural_estimates$delta PARAMS_NAMES &lt;- c(&quot;lambda&quot;, &quot;gamma&quot;, &quot;delta&quot;) for (idx_sample in 1:BOOTSTRAP_SAMPLES) { # Generate a sample based on mod, and ensure a HMM can be estimated on it # with testing_params as initial parameters temp &lt;- pois.HMM.generate.estimable.sample(ns = DATA_SIZE, mod = list(m = m, lambda = lambda, gamma = gamma), testing_params = list(m = m, lambda = lambda_init, gamma = gamma_init))$natural_parameters # The values from gamma are taken columnwise natural_parameters &lt;- unlist(temp[PARAMS_NAMES]) len_par &lt;- length(natural_parameters) bootstrap_estimates[idx_sample, 1:len_par] &lt;- natural_parameters } # Lower and upper (2.5% and 97.5%) bounds q &lt;- apply(bootstrap_estimates, 2, function(par_estimate) { quantile(par_estimate, probs = c(0.025, 0.975)) }) PARAMS_NAMES &lt;- paste0(rep(&quot;lambda&quot;, m), 1:m) # Get row and column indices for gamma instead of the default # columnwise index: the default indices are 1:m for the 1st column, # then (m + 1):(2 * m) for the 2nd, etc... for (gamma_idx in 1:m ^ 2) { row &lt;- (gamma_idx - 1) %% m + 1 col &lt;- (gamma_idx - 1) %/% m + 1 row_col_idx &lt;- c(row, col) PARAMS_NAMES &lt;- c(PARAMS_NAMES, paste0(&quot;gamma&quot;, paste0(row_col_idx, collapse = &quot;&quot;))) } PARAMS_NAMES &lt;- c(PARAMS_NAMES, paste0(rep(&quot;delta&quot;, m), 1:m)) bootstrap_CI &lt;- data.frame(&quot;Parameter&quot; = PARAMS_NAMES, &quot;Estimate&quot; = c(lambda, gamma, delta), &quot;Lower bound&quot; = q[1, ], &quot;Upper bound&quot; = q[2, ]) print(bootstrap_CI, row.names = FALSE) It should be noted that some bootstrap estimates can be very large or very small. One possible reason is that the randomly generated bootstrap sample might contain long chains of the same values, thus causing some probabilities in the TPM to be near the boundary 0 or 1. However, a large number of bootstrap samples lowers that risk since we retrieve a 95% CI. It is important for the bootstrap procedure to take into account the fact that estimates may not necessarily all be in the same order. For example, the first bootstrap may evaluate \\((\\lambda_1, \\lambda_2) = (1.1, 3.1)\\) while the second may evaluate to \\((\\lambda_1, \\lambda_2) = (3.11, 1.11)\\). Since we are looking to aggregate these estimates in order to derive CI through their 95% quantiles, it is necessary to impose an order on these estimates, to avoid grouping some \\(\\lambda_1\\) with some \\(\\lambda_2\\). The first that comes to mind is the ascending order. To ensure that \\(\\hat{{\\boldsymbol\\lambda}}\\) are kept in ascending order, we refer to our Section Label Switching for a solution. "],["application-to-different-data-sets.html", " 6 Application to different data sets 6.1 TYT data 6.2 Lamb data 6.3 Simulated data", " 6 Application to different data sets 6.1 TYT data We detail here the code used to estimate a two-state Poisson HMM based on the tinnitus dataset available in data/tinnitus.RData. Set a seed for randomness, and load files set.seed(123) library(TMB) TMB::compile(&quot;code/poi_hmm.cpp&quot;) ## [1] 0 dyn.load(dynlib(&quot;code/poi_hmm&quot;)) source(&quot;functions/utils.R&quot;) load(&quot;data/tinnitus.RData&quot;) Set initial parameters # Parameters and covariates m &lt;- 2 gamma &lt;- matrix(c(0.8, 0.2, 0.2, 0.8), nrow = m, ncol = m) lambda &lt;- seq(quantile(tinn_data, 0.1), quantile(tinn_data, 0.9), length.out = m) delta &lt;- stat.dist(gamma) Transform them into working parameters working_params &lt;- pois.HMM.pn2pw(m, lambda, gamma) TMB_data &lt;- list(x = tinn_data, m = m) Estimate the parameters via a function # Build the TMB object obj_tmb &lt;- MakeADFun(TMB_data, working_params, DLL = &quot;poi_hmm&quot;, silent = TRUE) # Optimize mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn, gradient = obj_tmb$gr, hessian = obj_tmb$he) # Check convergence mod_tmb$convergence == 0 ## [1] TRUE # Results summary(sdreport(obj_tmb), &quot;report&quot;) ## Estimate Std. Error ## lambda 1.63641100 0.27758296 ## lambda 5.53309576 0.31876147 ## gamma 0.94980209 0.04374676 ## gamma 0.02592204 0.02088688 ## gamma 0.05019791 0.04374676 ## gamma 0.97407796 0.02088688 ## delta 0.34054200 0.23056437 ## delta 0.65945800 0.23056437 For the code used to generate coverage probabilities and acceleration results, please take a look at code/poi_hmm_tinn.R. 6.2 Lamb data We detail here the code used to estimate a two-state Poisson HMM based on the lamb dataset available in data/fetal-lamb.RData Set a seed for randomness, and load files set.seed(123) library(TMB) TMB::compile(&quot;code/poi_hmm.cpp&quot;) ## [1] 0 dyn.load(dynlib(&quot;code/poi_hmm&quot;)) source(&quot;functions/utils.R&quot;) load(&quot;data/fetal-lamb.RData&quot;) lamb_data &lt;- lamb rm(lamb) Set initial parameters # Parameters and covariates m &lt;- 2 gamma &lt;- matrix(c(0.8, 0.2, 0.2, 0.8), nrow = m, ncol = m) lambda &lt;- seq(0.3, 4, length.out = m) delta &lt;- stat.dist(gamma) Transform them into working parameters working_params &lt;- pois.HMM.pn2pw(m, lambda, gamma) TMB_data &lt;- list(x = lamb_data, m = m) Estimate the parameters via a function # Build the TMB object obj_tmb &lt;- MakeADFun(TMB_data, working_params, DLL = &quot;poi_hmm&quot;, silent = TRUE) # Optimize mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn, gradient = obj_tmb$gr, hessian = obj_tmb$he) # Check convergence mod_tmb$convergence == 0 ## [1] TRUE # Results summary(sdreport(obj_tmb), &quot;report&quot;) ## Estimate Std. Error ## lambda 0.25636541 0.04016445 ## lambda 3.11475432 1.02131182 ## gamma 0.98872128 0.01063571 ## gamma 0.31033853 0.18468648 ## gamma 0.01127872 0.01063571 ## gamma 0.68966147 0.18468648 ## delta 0.96493123 0.03181445 ## delta 0.03506877 0.03181445 For the code used to generate coverage probabilities and acceleration results, please take a look at code/poi_hmm_lamb.R. On a minor note, when comparing our estimation results to those reported by Leroux and Puterman (1992), some non-negligible differences can be noted. The reasons for this are difficult to determine, but some likely explanations are given in the following. First, differences in the parameter estimates may result e.g. from the optimizing algorithms used and related setting (e.g. convergence criterion, number of steps, optimization routines used in 1992, etc). Moreover, Leroux and Puterman (1992) seem to base their calculations on an altered likelihood, which is reduced by removing the constant term \\(\\sum_{i=1}^{T} \\log(x_{i}!)\\) from the log-likelihood. This modification may also possess an impact on the behavior of the optimization algorithm, as e.g. relative convergence criteria and step size could be affected. The altered likelihood becomes apparent when computing it on a one-state HMM. A one-state Poisson HMM is a Poisson regression model, for which the log-likelihood has the expression \\[\\begin{align*} l(\\lambda) &amp;= \\log \\left(\\prod_{i=1}^{T} \\frac{\\lambda^{x_i} e^{-\\lambda}}{x_{i}!} \\right)\\\\ &amp;= - T \\lambda + \\log(\\lambda) \\left( \\sum_{i=1}^{T} x_i \\right) - \\sum_{i=1}^{T} \\log(x_{i}!). \\end{align*}\\] The authors find a ML estimate \\(\\lambda = 0.3583\\) and a log-likelihood of -174.26. In contrast, calculating the log-likelihood explicitly shows a different result. x &lt;- lamb_data # We use n instead of T in R code n &lt;- length(x) l &lt;- 0.3583 - n * l + log(l) * sum(x) - sum(log(factorial(x))) ## [1] -201.0436 The log-likelihood is different, but when the constant \\(- \\sum_{i=1}^{T} \\log(x_{i}!)\\) is removed, it matches our result. - n * l + log(l) * sum(x) ## [1] -174.2611 6.3 Simulated data We detail here the code used to simulate two datasets from two-states Poisson HMMs, one of size 2000 and one of size 5000. Then, using the same procedure as above, we estimate a model using different initial parameters. Set initial parameters (data size and HMM parameters) DATA_SIZE_SIMU &lt;- 2000 m &lt;- 2 # Generate parameters lambda &lt;- seq(10, 14, length.out = m) # Create the transition probability matrix with 0.8 on its diagonal gamma &lt;- matrix(c(0.8, 0.2, 0.2, 0.8), nrow = m, ncol = m) delta &lt;- stat.dist(gamma) The stat.dist function computes the stationary distribution. Generate data with one of the functions defined in Generating data simu_data &lt;- pois.HMM.generate.sample(ns = DATA_SIZE_SIMU, mod = list(m = m, lambda = lambda, gamma = gamma, delta = delta))$data Set initial parameters # Parameters and covariates m &lt;- 2 gamma &lt;- matrix(c(0.6, 0.4, 0.4, 0.6), nrow = m, ncol = m) lambda &lt;- seq(quantile(simu_data, 0.1), quantile(simu_data, 0.9), length.out = m) delta &lt;- stat.dist(gamma) # Display Poisson means lambda ## [1] 7 17 Transform them into working parameters working_params &lt;- pois.HMM.pn2pw(m, lambda, gamma) TMB_data &lt;- list(x = simu_data, m = m) Estimate the parameters via a function # Build the TMB object obj_tmb &lt;- MakeADFun(TMB_data, working_params, DLL = &quot;poi_hmm&quot;, silent = TRUE) # Optimize mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn, gradient = obj_tmb$gr, hessian = obj_tmb$he) # Check convergence mod_tmb$convergence == 0 ## [1] TRUE # Results summary(sdreport(obj_tmb), &quot;report&quot;) ## Estimate Std. Error ## lambda 9.7345726 0.23644285 ## lambda 13.8098676 0.22273984 ## gamma 0.8068975 0.03331223 ## gamma 0.1564875 0.02671332 ## gamma 0.1931025 0.03331223 ## gamma 0.8435125 0.02671332 ## delta 0.4476315 0.05201103 ## delta 0.5523685 0.05201103 For the code used to generate coverage probabilities and acceleration results, please take a look at code/poi_hmm_simu1.R, code/poi_hmm_simu2.R, code/poi_hmm_simu3.R and code/poi_hmm_simu4.R. "],["gaussian-hmm.html", " 7 Gaussian HMM 7.1 Dataset 7.2 Likelihood function 7.3 Estimation 7.4 Results", " 7 Gaussian HMM Univariate Gaussian HMMs with TMB are very similar to the previous Poisson HMM. The main changes are the distribution parameters being changed from \\({\\boldsymbol\\lambda}\\) to \\({\\boldsymbol\\mu}\\) and \\({\\boldsymbol\\sigma}\\). In turn, these cause the density function to change from dpois to dnorm (and from rpois to rnorm for simulations). Many functions available in functions/utils.R have been adapted for the Gaussian case in functions/norm_utils.R. We detail below an example of a Gaussian HMM with TMB. 7.1 Dataset The dataset we show in this example contains log-returns of the SP500 dataset pulled from Yahoo finance. These log-returns are generated with the following code. library(tidyverse) # Loading the sp500 dataset SP500 &lt;- read_csv(&quot;data/S&amp;P500.csv&quot;) %&gt;% mutate(lag = lag(`Adj Close`), lr = log(`Adj Close`/lag)) %&gt;% select(lr) %&gt;% drop_na() SP500 &lt;- 100 * SP500$lr save(SP500, file = &quot;data/SP500.RData&quot;) 7.2 Likelihood function The Gaussian negative log-likelihood in TMB can be coded as #include &lt;TMB.hpp&gt; #include &quot;../functions/norm_utils.cpp&quot; // Likelihood for a normal hidden markov model. template&lt;class Type&gt; Type objective_function&lt;Type&gt;::operator() () { // Data DATA_VECTOR(x); // timeseries vector DATA_INTEGER(m); // Number of states m // Parameters PARAMETER_VECTOR(tmu); // conditional means PARAMETER_VECTOR(tsigma); // conditional log_sd&#39;s PARAMETER_VECTOR(tgamma); // m(m-1) working parameters of TPM // PARAMETER_VECTOR(tdelta); // m-1 working parameters of initial distribution // Transform working parameters to natural parameters: vector&lt;Type&gt; mu = tmu; vector&lt;Type&gt; sigma = tsigma.exp(); matrix&lt;Type&gt; gamma = gamma_w2n(m, tgamma); // vector&lt;Type&gt; delta = delta_w2n(m, tdelta); // Construct stationary distribution vector&lt;Type&gt; delta = stat_dist(m, gamma); // Get number of timesteps (n) int n = x.size(); // Evaluate conditional distribution: Put conditional probabilities // of observed x in n times m matrix (one column for each state) matrix&lt;Type&gt; emission_probs(n, m); matrix&lt;Type&gt; row1vec(1, m); row1vec.setOnes(); for (int i = 0; i &lt; n; i++) { if (x[i] != x[i]) { // f != f returns true if and only if f is NaN. // Replace missing values (NA in R, NaN in C++) with 1 emission_probs.row(i) = row1vec; } else { emission_probs.row(i) = dnorm(x(i), mu, sigma, false); } } // Corresponds to (Zucchini et al., 2016, p 333) matrix&lt;Type&gt; foo, P; Type mllk, sumfoo, lscale; foo = (delta * vector&lt;Type&gt;(emission_probs.row(0))).matrix(); sumfoo = foo.sum(); lscale = log(sumfoo); foo.transposeInPlace(); foo /= sumfoo; for (int i = 2; i &lt;= n; i++) { P = emission_probs.row(i - 1); foo = ((foo * gamma).array() * P.array()).matrix(); sumfoo = foo.sum(); lscale += log(sumfoo); foo /= sumfoo; } mllk = -lscale; // Use adreport on variables for which we want standard errors ADREPORT(mu); ADREPORT(sigma); ADREPORT(gamma); ADREPORT(delta); // Variables we need for local decoding and in a convenient format REPORT(mu); REPORT(sigma); REPORT(gamma); REPORT(delta); REPORT(n); // REPORT(emission_probs); REPORT(mllk); return mllk; } and requires the following utility functions. // Function transforming working parameters in initial distribution // to natural parameters template&lt;class Type&gt; vector&lt;Type&gt; delta_w2n(int m, vector&lt;Type&gt; tdelta) { vector&lt;Type&gt; delta(m); vector&lt;Type&gt; foo(m); if (m == 1) return Type(1); // set first element to one. // Fill in the last m - 1 elements with working parameters // and take exponential foo &lt;&lt; Type(1), tdelta.exp(); // normalize delta = foo / foo.sum(); return delta; } // Function transforming the working parameters in TPM to // natural parameters (w2n) template&lt;class Type&gt; matrix&lt;Type&gt; gamma_w2n(int m, vector&lt;Type&gt; tgamma) { // Construct m x m identity matrix matrix&lt;Type&gt; gamma(m, m); gamma.setIdentity(); if (m == 1) return gamma; // Fill offdiagonal elements with working parameters column-wise: int idx = 0; for (int i = 0; i &lt; m; i++){ for (int j = 0; j &lt; m; j++){ if (j != i){ // Fill gamma according to mapping and take exponential gamma(j, i) = tgamma.exp()(idx); idx++; } } } // Normalize each row: vector&lt;Type&gt; cs = gamma.rowwise().sum(); for (int i = 0; i &lt; m; i++) gamma.row(i) /= cs[i]; return gamma; } // Function computing the stationary distribution of a Markov chain template&lt;class Type&gt; vector&lt;Type&gt; stat_dist(int m, matrix&lt;Type&gt; gamma) { // Construct stationary distribution matrix&lt;Type&gt; I(m, m); matrix&lt;Type&gt; U(m, m); matrix&lt;Type&gt; row1vec(1, m); U = U.setOnes(); I = I.setIdentity(); row1vec.setOnes(); matrix&lt;Type&gt; A = I - gamma + U; matrix&lt;Type&gt; Ainv = A.inverse(); matrix&lt;Type&gt; deltamat = row1vec * Ainv; vector&lt;Type&gt; delta = deltamat.row(0); return delta; } 7.3 Estimation The following R code illustrates how to compute the estimates and standard errors, using the objective function written above. # Load packages and utility functions source(&quot;code/packages.R&quot;) source(&quot;functions/norm_utils.R&quot;) # Compile and load the objective function for TMB TMB::compile(&quot;code/norm_hmm.cpp&quot;) ## [1] 0 dyn.load(dynlib(&quot;code/norm_hmm&quot;)) # Parameters and covariates load(&quot;data/SP500.RData&quot;) m &lt;- 2 # Means mu &lt;- c(-2 * mean(SP500), 2 * mean(SP500)) # Standard deviations sigma &lt;- c(0.5 * sd(SP500), 2 * sd(SP500)) # TPM gamma &lt;- matrix(c(0.9, 0.1, 0.1, 0.9), nrow = 2, byrow = TRUE) # Parameters &amp; covariates for TMB working_params &lt;- norm.HMM.pn2pw(m = m, mu = mu, sigma = sigma, gamma = gamma) TMB_data &lt;- list(x = SP500, m = m) obj &lt;- MakeADFun(TMB_data, working_params, DLL = &quot;norm_hmm&quot;, silent = TRUE) # Estimation obj_tmb &lt;- MakeADFun(data = TMB_data, parameters = working_params, DLL = &quot;norm_hmm&quot;, silent = TRUE) mod_tmb &lt;- nlminb(start = obj_tmb$par, objective = obj_tmb$fn, gradient = obj_tmb$gr, hessian = obj_tmb$he) Note that the script above makes a simple educated guess on the true parameters in order to choose initial parameters. 7.4 Results Estimates can be shown here along with their standard errors, as was done previously in the Poisson case. As before, the matrix results are displayed in a column-wise format. This can be seen by comparing estimates with a more readable view. summary(sdreport(obj_tmb), &quot;report&quot;) ## Estimate Std. Error ## mu 0.06116848 0.005401313 ## mu -0.12288814 0.032944675 ## sigma 0.69860750 0.005201810 ## sigma 2.24762988 0.029916757 ## gamma 0.98850975 0.001067611 ## gamma 0.04374985 0.004126772 ## gamma 0.01149025 0.001067611 ## gamma 0.95625015 0.004126772 ## delta 0.79199446 0.016439163 ## delta 0.20800554 0.016439163 # Readable estimates obj_tmb$report() ## $mu ## [1] 0.06116848 -0.12288814 ## ## $sigma ## [1] 0.6986075 2.2476299 ## ## $delta ## [1] 0.7921573 0.2078427 ## ## $n ## [1] 23349 ## ## $gamma ## [,1] [,2] ## [1,] 0.98852111 0.01147889 ## [2,] 0.04374985 0.95625015 ## ## $mllk ## [1] 31516.82 An intuitive interpretation of these results is that on average, market prices either grow slowly (positive low mean) but surely (low variance) as people invest carefully, or the market prices decrease (negative larger mean) as people sell in panic for fear of losing too much (higher variance). For more details on this, see e.g.  and get some information on bull and bear markets. Note that the TPM is displayed column-wise. "],["multivariate-gaussian-hmm.html", " 8 Multivariate Gaussian HMM 8.1 Dataset &amp; Parameters 8.2 Likelihood function 8.3 Estimation 8.4 Results 8.5 Bias", " 8 Multivariate Gaussian HMM Multivariate Gaussian HMMs with TMB is a direct generalization of the univariate case from the previous section. We focus on the main differences. Most notably, distribution parameters are changed from Normal mean vectors to matrices and we now have one covariance matrix for each state. For the implementation, the main change is for the density function to go from dnorm to dmvnorm (and the random data generation functions from rnorm to rmvnorm for simulations) from the mvtnorm (Genz et al. 2021) package. Many functions available in functions/norm_utils.R have been adapted with the changes described for the multivariate Gaussian case in functions/mvnorm_utils.R. As an example, the function to generate random data becomes # Generate a random sample from a HMM mvnorm.HMM.generate.sample &lt;- function(ns, mod) { mvect &lt;- 1:mod$m state &lt;- numeric(ns) state[1] &lt;- sample(mvect, 1, prob = mod$delta) for (i in 2:ns) { state[i] &lt;- sample(mvect, 1, prob = mod$gamma[state[i - 1], ]) } x &lt;- matrix(NA, nrow = ns, ncol = p) for (i in 1:ns) { x[i, ] &lt;- rmvnorm(n = 1, mean = mod$mu[state[i], ], sigma = mod$sigma[, , state[i]]) } names(x) &lt;- NULL return(list(data = x, state = state)) } We detail below an example of the estimation of a two-state trivariate Gaussian HMM with TMB. 8.1 Dataset &amp; Parameters To avoid issues regarding the choice of data, we simulate our own. Let two multivariate Gaussian distributions \\(N({\\boldsymbol\\mu_1}, {\\boldsymbol\\Sigma_1})\\) and \\(N({\\boldsymbol\\mu_2}, {\\boldsymbol\\Sigma_2})\\) be defined with (rather arbitrary) parameters as follows. \\[\\begin{equation*} {\\boldsymbol\\mu_1} = (6, 8, 9) \\quad \\text{and } {\\boldsymbol\\mu_2} = (1, 2, 3) \\end{equation*}\\] \\[\\begin{equation*} {\\boldsymbol\\Sigma_1} = \\begin{pmatrix} 1.0 &amp; 0.5 &amp; 0.5\\\\ 0.5 &amp; 1.0 &amp; 0.5\\\\ 0.5 &amp; 0.5 &amp; 1.0 \\end{pmatrix} \\quad \\text{ and } {\\boldsymbol\\Sigma_2} = \\begin{pmatrix} 2.0 &amp; 1.5 &amp; 1.5\\\\ 1.5 &amp; 2.0 &amp; 1.5\\\\ 1.5 &amp; 1.5 &amp; 2.0 \\end{pmatrix}. \\end{equation*}\\] Let us consider a stationary Markov chain with the following (also fairly random) TPM \\[\\begin{equation*} {\\boldsymbol\\Gamma} = \\begin{pmatrix} 0.95 &amp; 0.05\\\\ 0.15 &amp; 0.85 \\end{pmatrix}. \\end{equation*}\\] We generate 1000 data with the function above. 8.2 Likelihood function The Gaussian negative log-likelihood in TMB can be coded as #include &lt;TMB.hpp&gt; #include &quot;../functions/mvnorm_utils.cpp&quot; // Likelihood for a multivariate normal hidden markov model. template&lt;class Type&gt; Type objective_function&lt;Type&gt;::operator() () { // Data DATA_MATRIX(x); // timeseries matrix (n rows * p cols) DATA_INTEGER(m); // Number of states m // Parameters PARAMETER_MATRIX(tmu); // mp conditional mu&#39;s (matrix: m rows, p columns) PARAMETER_MATRIX(tsigma); // mp(p+1)/2 working parameters of covariance matrices (matrix: p(p+1)/2 rows, m columns) PARAMETER_VECTOR(tgamma); // m(m-1) working parameters of TPM (vector: m*m-m columns) // Uncomment only when using a non-stationary distribution //PARAMETER_VECTOR(tdelta); // m-1 working parameters (vector) // Load namespace which contains the multivariate distributions using namespace density; // Get number of covariates (p) int p = x.cols(); // Transform working parameters to natural parameters: matrix&lt;Type&gt; mu = tmu.transpose(); // We need rows (for the emission probability matrix) but TMB only supports easy retrieval of columns matrix&lt;Type&gt; gamma = gamma_w2n(m, tgamma); array&lt;Type&gt; sigma = sigma_w2n(m, p, tsigma); // Construct m matrices of size p x p (array: p x p x m) // Construct stationary distribution vector&lt;Type&gt; delta = stat_dist(m, gamma); // If using a non-stationary distribution, use this instead //vector&lt;Type&gt; delta = delta_w2n(m, tdelta); // Get number of timesteps (n) int n = x.rows(); // Evaluate conditional distribution: Put conditional // probabilities of observed x in n times m matrix // (one column for each state, one row for each datapoint): matrix&lt;Type&gt; emission_probs(n, m); matrix&lt;Type&gt; sigma_m(p, p); vector&lt;Type&gt; residual_vec(p); Type nll = 0; // Evaluate and store the conditional distributions column-wise bool NA_appears = false; for (int m_idx = 0; m_idx &lt; m; m_idx++) { MVNORM_t&lt;Type&gt; neg_log_dmvnorm(sigma.col(m_idx).matrix()); for (int i = 0; i &lt; n; i++) { // Replace missing values (NA in R, NaN in C++) with 1 NA_appears = false; for (int p_idx = 0; p_idx &lt; p; p_idx++) { if (x(i, p_idx) != x(i, p_idx)) { // f != f returns true if and only if f is NaN. NA_appears = true; } } if (NA_appears) { emission_probs(i, m_idx) = 1; } else { // Fill the emission probability matrix residual_vec = vector&lt;Type&gt;(x.row(i)); residual_vec -= vector&lt;Type&gt;(mu.col(m_idx)); nll = neg_log_dmvnorm(residual_vec); // MVNORM_t returns the negative log-likelihood, we only want the likelihood emission_probs(i, m_idx) = exp(-nll); } } } // Corresponds to (Zucchini et al., 2016, p 333) matrix&lt;Type&gt; foo, P; Type mllk, sumfoo, lscale; foo = (delta * vector&lt;Type&gt;(emission_probs.row(0))).matrix(); sumfoo = foo.sum(); lscale = log(sumfoo); foo.transposeInPlace(); foo /= sumfoo; for (int i = 2; i &lt;= n; i++) { P = emission_probs.row(i - 1); foo = ((foo * gamma).array() * P.array()).matrix(); sumfoo = foo.sum(); lscale += log(sumfoo); foo /= sumfoo; } mllk = -lscale; // Undo the transpose done at the beginning mu.transposeInPlace(); // Use adreport on variables for which we want standard errors ADREPORT(mu); ADREPORT(sigma); ADREPORT(gamma); ADREPORT(delta); // Variables we need for local decoding and in a convenient format REPORT(mu); REPORT(sigma); REPORT(gamma); REPORT(delta); REPORT(n); // REPORT(emission_probs); REPORT(mllk); return mllk; } and requires the following utility functions. // Function transforming working parameters in initial distribution // to natural parameters template&lt;class Type&gt; vector&lt;Type&gt; delta_w2n(int m, vector&lt;Type&gt; tdelta) { vector&lt;Type&gt; delta(m); vector&lt;Type&gt; foo(m); if (m == 1) return Type(1); // set first element to one. // Fill in the last m - 1 elements with working parameters // and take exponential foo &lt;&lt; Type(1), tdelta.exp(); // normalize delta = foo / foo.sum(); return delta; } // Function transforming the working parameters in TPM to // natural parameters (w2n) template&lt;class Type&gt; matrix&lt;Type&gt; gamma_w2n(int m, vector&lt;Type&gt; tgamma) { // Construct m x m identity matrix matrix&lt;Type&gt; gamma(m, m); gamma.setIdentity(); if (m == 1) return gamma; // Fill offdiagonal elements with working parameters column-wise: int idx = 0; for (int i = 0; i &lt; m; i++){ for (int j = 0; j &lt; m; j++){ if (j != i){ // Fill gamma according to mapping and take exponential gamma(j, i) = tgamma.exp()(idx); idx++; } } } // Normalize each row: vector&lt;Type&gt; cs = gamma.rowwise().sum(); for (int i = 0; i &lt; m; i++) gamma.row(i) /= cs[i]; return gamma; } // Function transforming the working parameters in the covariance matrices to // natural parameters (w2n) (from row-wise upper triangle as a vector to // the original covariance matrix) template&lt;class Type&gt; array&lt;Type&gt; sigma_w2n(int m, int p, matrix&lt;Type&gt; tsigma) { // Construct m matrices of size p x p (p x p x m array) array&lt;Type&gt; sigma_array(p, p, m); matrix&lt;Type&gt; temporary_matrix(p, p); matrix&lt;Type&gt; sigma_matrix(p, p); // Fill upper triangular elements with working parameters column-wise int idx; for (int m_idx = 0; m_idx &lt; m; m_idx++) { idx = 0; for (int j = 0; j &lt; p; j++) { // tsigma is filled column-wise from sigma for (int i = 0; i &lt;= j; i++) { // Fill sigma_array according to mapping, undo the log transformation if (i == j) { sigma_array(i, j, m_idx) = exp(tsigma(idx, m_idx)); idx++; } else if (i &lt; j) { // Fill sigma_array according to mapping sigma_array(i, j, m_idx) = tsigma(idx, m_idx); idx++; } else { sigma_array(i, j, m_idx) = 0; } } } // Undo the Cholesky transformation sigma_matrix = sigma_array.col(m_idx).matrix(); // col() selects the last index, row() doesn&#39;t exist temporary_matrix = sigma_matrix.transpose() * sigma_matrix; sigma_array.col(m_idx) = temporary_matrix.array(); } return sigma_array; } // Function computing the stationary distribution of a Markov chain template&lt;class Type&gt; vector&lt;Type&gt; stat_dist(int m, matrix&lt;Type&gt; gamma) { // Construct stationary distribution matrix&lt;Type&gt; I(m, m); matrix&lt;Type&gt; U(m, m); matrix&lt;Type&gt; row1vec(1, m); U = U.setOnes(); I = I.setIdentity(); row1vec.setOnes(); matrix&lt;Type&gt; A = I - gamma + U; matrix&lt;Type&gt; Ainv = A.inverse(); matrix&lt;Type&gt; deltamat = row1vec * Ainv; vector&lt;Type&gt; delta = deltamat.row(0); return delta; } 8.3 Estimation The estimation procedure is similar to before. # Load packages and utility functions source(&quot;code/packages.R&quot;) source(&quot;functions/mvnorm_utils.R&quot;) # Compile and load the objective function for TMB TMB::compile(&quot;code/mvnorm_hmm.cpp&quot;) ## [1] 0 dyn.load(dynlib(&quot;code/mvnorm_hmm&quot;)) set.seed(123) # Two states m &lt;- 2 # Trivariate Normal distribution p &lt;- 3 # One row of means per state, one column per dimension of the data mu &lt;- matrix(c(6, 8, 9, 1, 2, 3), nrow = m, ncol = p, byrow = TRUE) # Two covariance matrices sigma1 &lt;- matrix(c(1.0, 0.5, 0.5, 0.5, 1.0, 0.5, 0.5, 0.5, 1.0), nrow = p, ncol = p, byrow = TRUE) sigma2 &lt;- matrix(c(2.0, 1.5, 1.5, 1.5, 2.0, 1.5, 1.5, 1.5, 2.0), nrow = p, ncol = p, byrow = TRUE) # We store them in an array for convenience, making them # easily distinguishable at a glance when displayed sigma &lt;- array(c(sigma1, sigma2), dim = c(p, p, m)) # TPM gamma &lt;- matrix(c(0.95, 0.05, 0.15, 0.85), byrow = TRUE, nrow = m, ncol = m) # Similarly to the Poisson case, we can generate data. # Here, we generate a trivariate Gaussian sample of size 1000. mod &lt;- list(m = m, mu = mu, sigma = sigma, gamma = gamma) TMBdata &lt;- mvnorm.HMM.generate.sample(1000, mod) # Parameters &amp; covariates for TMB # TMB requires a list TMB_data &lt;- list(x = TMBdata$data, m = m) # TMB requires the parameters to be either vectors, matrices, or arrays. # For simplicity, we pass the parameters as a list of vectors and matrices. pw &lt;- mvnorm.HMM.pn2pw(m = m, mu = mu, sigma = sigma, gamma = gamma) # Estimation obj_tmb &lt;- MakeADFun(data = TMB_data, parameters = pw, DLL = &quot;mvnorm_hmm&quot;, silent = TRUE) nlminb(start = obj_tmb$par, objective = obj_tmb$fn, gradient = obj_tmb$gr, hessian = obj_tmb$he) ## $par ## tmu tmu tmu tmu tmu tmu tsigma ## 5.979331886 1.105791929 7.974662650 2.016963286 9.034963083 3.069442564 -0.005278076 ## tsigma tsigma tsigma tsigma tsigma tsigma tsigma ## 0.470025716 -0.161247055 0.455150038 0.312012980 -0.218999355 0.306537527 1.029377442 ## tsigma tsigma tsigma tsigma tgamma tgamma ## -0.038685928 0.977222514 0.440952661 -0.128765538 -1.852673335 -2.813032886 ## ## $objective ## [1] 4295.975 ## ## $convergence ## [1] 0 ## ## $iterations ## [1] 5 ## ## $evaluations ## function gradient ## 6 5 ## ## $message ## [1] &quot;both X-convergence and relative convergence (5)&quot; 8.4 Results We show estimates and their standard errors. Note that the matrix results (for \\({\\boldsymbol\\mu}\\), \\({\\boldsymbol\\delta}\\), \\({\\boldsymbol\\Gamma}\\), and \\({\\boldsymbol\\Sigma}\\)) are displayed in a column-wise format. This can be verified by checking the more easily readable view of these estimates. summary(sdreport(obj_tmb), &quot;report&quot;) ## Estimate Std. Error ## mu 5.97933189 0.037563682 ## mu 1.10579193 0.079577526 ## mu 7.97466265 0.036769886 ## mu 2.01696329 0.082568262 ## mu 9.03496308 0.036893647 ## mu 3.06944256 0.081165357 ## sigma 0.98949937 0.052850720 ## sigma 0.46755142 0.040689252 ## sigma 0.45275405 0.040488620 ## sigma 0.46755142 0.040689252 ## sigma 0.94526437 0.050668944 ## sigma 0.47948079 0.040495684 ## sigma 0.45275405 0.040488620 ## sigma 0.47948079 0.040495684 ## sigma 0.94984027 0.051242766 ## sigma 1.84609954 0.155927347 ## sigma 1.39862795 0.143048208 ## sigma 1.32776440 0.138042910 ## sigma 1.39862795 0.143048208 ## sigma 1.98516354 0.167983567 ## sigma 1.43015056 0.145538312 ## sigma 1.32776440 0.138042910 ## sigma 1.43015056 0.145538312 ## sigma 1.92236070 0.161913458 ## gamma 0.94337605 0.008707558 ## gamma 0.13555932 0.019818312 ## gamma 0.05662395 0.008707558 ## gamma 0.86444068 0.019818312 ## delta 0.70536482 0.043981080 ## delta 0.29463518 0.043981080 # More readable estimates report &lt;- obj_tmb$report() report ## $mu ## [,1] [,2] [,3] ## [1,] 5.979332 7.974663 9.034963 ## [2,] 1.105792 2.016963 3.069443 ## ## $sigma ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 0.9894994 0.4675514 0.4527541 ## [2,] 0.4675514 0.9452644 0.4794808 ## [3,] 0.4527541 0.4794808 0.9498403 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 1.846100 1.398628 1.327764 ## [2,] 1.398628 1.985164 1.430151 ## [3,] 1.327764 1.430151 1.922361 ## ## ## $delta ## [1] 0.7055608 0.2944392 ## ## $n ## [1] 1000 ## ## $gamma ## [,1] [,2] ## [1,] 0.9434294 0.05657056 ## [2,] 0.1355593 0.86444068 ## ## $mllk ## [1] 4295.975 8.5 Bias Since the data was simulated, the true parameters are known and the validity of the estimates can be checked. To do this, we look at the maximum absolute percentage difference between estimates and true values. (report$mu - mu) / mu ## [,1] [,2] [,3] ## [1,] -0.003444686 -0.003167169 0.003884787 ## [2,] 0.105791929 0.008481643 0.023147521 (report$sigma - sigma) / sigma ## , , 1 ## ## [,1] [,2] [,3] ## [1,] -0.01050063 -0.06489716 -0.09449190 ## [2,] -0.06489716 -0.05473563 -0.04103843 ## [3,] -0.09449190 -0.04103843 -0.05015973 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] -0.07695023 -0.067581365 -0.11482373 ## [2,] -0.06758137 -0.007418231 -0.04656629 ## [3,] -0.11482373 -0.046566292 -0.03881965 (report$gamma - gamma) / gamma ## [,1] [,2] ## [1,] -0.006916378 0.13141118 ## [2,] -0.096271185 0.01698903 Differences are at most in the order of 10% for values near 0. This is acceptable. "],["github.html", " 9 GitHub 9.1 Directory structure", " 9 GitHub For reading more easily, we recommend opening the file Data supplements.Rproj with R-Studio, which lets the user have the correct working path set up automatically. Other files can be opened directly by double-clicking on them, or via the Files tab in R-Studio. Most of the code can be folded/collapsed into sections easily by clicking in the menu Edit-&gt;Collapse All Output. Unfolding can be done by clicking on the arrows to the left of the folded sections, or in the menu Edit-&gt;Expand All Output. 9.1 Directory structure For readability, folders are displayed with a dash at the end. The folder code will be at the section code/. 9.1.1 code/ Files required to compute the main results of the article: acceleration factors and coverage probabilities. Folder on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/code 9.1.2 code/linreg.cpp Specifies the function computing the negative log-likelihood of a linear regression in C++. Heavily inspired by https://github.com/kaskr/adcomp/blob/master/tmb_examples/linreg.cpp. File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/code/linreg.cpp File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/code/linreg.cpp 9.1.3 code/linreg_extended.cpp Similar to code/linreg.cpp, with some more complex code added to serve as an example. Used to compute negative log-likelihoods in R with TMB. File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/code/linreg_extended.cpp File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/code/linreg_extended.cpp 9.1.4 code/main.R Used to run procedures related to timing and comparisons presented in the article. By default, it will load results instead. The first chunk of code it runs can be found in the file code/setup_parameters.R. Afterwards, either computations are run and their results are stored in data/, or results are loaded to the current environment. The computations are written in code/poi_hmm_tinn.R, code/poi_hmm_lamb.R, code/poi_hmm_simu1.R, code/poi_hmm_simu2.R, code/poi_hmm_simu3.R, and code/poi_hmm_simu4.R. NOTE: Our scripts were tested on a workstation with 4 Intel(R) Xeon(R) Gold 6134 processors (3.7 GHz) running under the Linux distribution Ubuntu 18.04.6 LTS (Bionic Beaver) with 384 GB, and took about a week of computing time. More details are available at individual files. File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/code/main.R File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/code/main.R 9.1.5 code/mvnorm_hmm.cpp Specifies the function computing the negative log-likelihood of a m-state multivariate Gaussian Hidden Markov Model in C++. File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/code/mvnorm_hmm.cpp File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/code/mvnorm_hmm.cpp 9.1.6 code/norm_hmm.cpp Specifies the function computing the negative log-likelihood of a m-state univariate Gaussian Hidden Markov Model in C++. File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/code/norm_hmm.cpp File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/code/norm_hmm.cpp 9.1.7 code/packages.R Automatically install/load necessary packages for running code/main.R with all simulations. File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/code/packages.R File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/code/packages.R 9.1.8 code/poi_hmm.cpp Specifies the function computing the negative log-likelihood of a m-state Poisson Hidden Markov Model in C++. File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/code/poi_hmm.cpp File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/code/poi_hmm.cpp 9.1.9 code/poi_hmm_lamb.R File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/code/poi_hmm_lamb.R File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/code/poi_hmm_lamb.R On our machine, this file took approximately 2h to compute. 9.1.10 code/poi_hmm_simu1.R File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/code/poi_hmm_simu1.R File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/code/poi_hmm_simu1.R On our machine, this file took approximately 3h30 to compute. 9.1.11 code/poi_hmm_simu2.R File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/code/poi_hmm_simu2.R File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/code/poi_hmm_simu2.R On our machine, this file took approximately 23h to compute. 9.1.12 code/poi_hmm_simu3.R Unlike in the other simulations, the coverage probabilities computation code does not ensure that profile CIs converge. This is solved in the fourth simulation with a larger dataset size, and shows that care must be taken when dealing with profile CIs because they can fail to produce results. File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/code/poi_hmm_simu3.R File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/code/poi_hmm_simu3.R On our machine, this file took approximately 26h30 to compute. 9.1.13 code/poi_hmm_simu4.R File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/code/poi_hmm_simu3.R File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/code/poi_hmm_simu3.R On our machine, this file took approximately 63h to compute. 9.1.14 code/poi_hmm_tinn.R Most of their code is common: Parameters and covariates defines parameters and covariates in natural form. The simulation files generate data there. Parameters &amp; covariates for TMB transforms parameters and covariates to their working form. Estimation computes estimates with and without TMB for further comparison. The estimates are stored in the data-frames conf_int_*. Creating variables for the CIs creates necessary variables for to compute confidence intervals (CIs). Benchmarks uses the microbenchmark (Mersmann 2021) package to accurately time the \\(TMB_0\\) \\(TMB_G\\) \\(TMB_H\\) and \\(TMB_{GH}\\) procedures. Their times are stored in the data-frames estim_benchmarks_df_* Profiling the likelihood uses the TMB package’s tmbprofile function to determine a profile of the likelihood, then determines a CI based on it for the corresponding working parameter. Eventually, CIs are derived when possible for the natural parameters \\(\\hat{{\\boldsymbol\\lambda}}\\) and \\(\\hat{{\\boldsymbol\\Gamma}}\\). The CIs are stored in the data-frames conf_int_*. Bootstrap derives a parametric bootstrap CI from the dataset, while checking that the generated data can be estimated without errors, after which we apply our label switching function to ensure that estimates of a parameter are only aggregated with estimates of the same parameter. The CIs are stored in the data-frames conf_int_*. TMB confidence intervals computes CIs based on TMB-derived standard errors. The CIs are stored in the data-frames conf_int_*. Coverage probabilities of the 3 CI methods simulates coverage samples based on true values when available or on estimates otherwise, then derives CIs using the three CI generation methods described above. The coverage probabilities are stored in the data-frames conf_int_*. Fixes uses the label-switching algorithm on the estimates in the conf_int_* variable to ensure they are correctly ordered, and then reorders \\(\\hat{{\\boldsymbol\\Gamma}}\\) in the conf_int_* variable to a row-wise order instead of the default column-wise order. In short this applies two fixes on the conf_int_* variable to make it easier to read. At the end, we reorder the profile likelihood based CIs in case they are not in ascending order. A unique randomness seed is set in each of these four files before all time-consuming tasks involving some randomness. code/setup_parameters.R defines the random number generator’s version. code/poi_hmm_tinn.R contains an extra section: Benchmark the same dataset many times to check the benchmark durations has low variance. As written in its title, its role is to time estimation with and without TMB. However, unlike in the section Benchmarks, estimation is done on the same dataset everytime. If everything goes well, the times should have a negligible variance. This allows us to check if normal background activity on the computer (e.g. the user opening a window or moving the mouse) affects estimation time in any noticeable way. If it affects estimation noticeably, then we would have to apply much stricter control on background processes in order to reliably compare estimation speeds with different optimizers. Luckily, a low variance was observed, making the acceleration evidenced in this project significant. NOTE: to be faster, some code is parallelized: In code/poi_hmm_simu1.R, bootstrapping is parallelized both in Bootstrap (accelerated from 83 seconds per sample to 29 seconds) and in Coverage probabilities of the 3 CI methods (likelihood profiling was slower with parallelization, likely due to loading TMB each time and the procedure being already fairly quick: from 2.8 seconds in total without to 3.5 seconds in total with). In code/poi_hmm_simu2.R, bootstrapping and likelihood profiling are both parallelized in Bootstrap (accelerated from 64 seconds per bootstrap sample to 21), Profiling the likelihood, and in Coverage probabilities of the 3 CI methods. code/poi_hmm_simu3.R and code/poi_hmm_simu4.R benefit from the same parallelization as code/poi_hmm_simu2.R. Parallelization is handled with the foreach (Revolution Analytics and Weston, n.d.) package and the doParallel (Corporation and Weston 2022) package as a backend. File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/code/poi_hmm_tinn.R File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/code/poi_hmm_tinn.R 9.1.15 code/setup_parameters.R Sets up multiple variables either to define some global constants or to store useful results for easier maintenance, and compiles code/linreg.cpp and code/poi_hmm.cpp. File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/code/setup_parameters.R File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/code/setup_parameters.R 9.1.16 data/ Files containing both datasets and the computational results. code/main.R stores important results in this folder, and only loads them by default. Speed results are available after some post-processing that we leave to the reader. Folder on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/data 9.1.17 data/fetal-lamb.RData Contains lamb: an integer vector of with 240 data from Leroux and Puterman (1992). File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/data/fetal-lamb.RData File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/data/fetal-lamb.RData 9.1.18 data/results_lamb.RData File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/data/results_lamb.RData File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/data/results_lamb.RData 9.1.19 data/results_simu1.RData File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/data/results_simu1.RData File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/data/results_simu1.RData 9.1.20 data/results_simu2.RData File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/data/results_simu2.RData File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/data/results_simu2.RData 9.1.21 data/results_simu3.RData File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/data/results_simu3.RData File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/data/results_simu3.RData 9.1.22 data/results_simu4.RData File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/data/results_simu4.RData File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/data/results_simu4.RData 9.1.23 data/results_tinn.RData Contains results from computations and timings run in code/main.R. File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/data/results_tinn.RData File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/data/results_tinn.RData 9.1.24 data/tinnitus.RData Tinnitus data collected with the “Track Your Tinnitus” (TYT) mobile application on 87 successive days, provided by the University of Regensburg and European School for Interdisciplinary Tinnitus Research (ESIT), of which a detailed description is presented in Pryss, Reichert, Langguth, et al. (2015) and Pryss, Reichert, Herrmann, et al. (2015). A plot is available in Figure 4.1. File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/data/tinnitus.RData File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/data/tinnitus.RData 9.1.25 functions/ Utility functions used both in C++ files and in R files. Folder on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/functions 9.1.26 functions/mvnorm_utils.cpp Utility functions used in Multivariate Gaussian HMM to transform working parameters to their natural form, and to compute the hidden Markov chain’s stationary distribution when needed. File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/functions/mvnorm_utils.cpp File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/functions/mvnorm_utils.cpp 9.1.27 functions/mvnorm_utils.R Useful functions to estimate multivariate Gaussian HMMs with and without TMB and perform various pre-processing and post-processing. File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/functions/mvnorm_utils.R File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/functions/mvnorm_utils.R 9.1.28 functions/norm_utils.cpp Utility functions used in Gaussian HMM to transform working parameters to their natural form, and to compute the hidden Markov chain’s stationary distribution when needed. File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/functions/norm_utils.cpp File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/functions/norm_utils.cpp 9.1.29 functions/norm_utils.R Useful functions to estimate Gaussian HMMs with and without TMB and perform various pre-processing and post-processing. File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/functions/norm_utils.R File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/functions/norm_utils.R 9.1.30 functions/utils.cpp Utility functions used in code/poi_hmm.cpp to transform working parameters to their natural form, and to compute the hidden Markov chain’s stationary distribution when needed. File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/functions/utils.cpp File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/functions/utils.cpp 9.1.31 functions/utils.R Useful functions to estimate Poisson HMMs with and without TMB and perform various pre-processing and post-processing. File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/functions/utils.R File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/functions/utils.R 9.1.32 functions/utils.R-explanation.R Description of the function parameters and return objects defined in functions/utils.R. The other utility files are adapted to the univariate and multivariate Gaussian setting but otherwise have identical functions. Note that the article focuses on Poisson HMMs. Hence, some utility functions in R present for the Poisson HMM are not adapted to the Gaussian HMMs. File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/functions/utils.R-explanation.R File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/functions/utils.R-explanation.R 9.1.33 functions/utils_linreg_extended.cpp Contains the useless example function. File on GitHub: https://github.com/timothee-bacri/HMM_with_TMB/blob/main/functions/utils_linreg_extended.cpp File: https://github.com/timothee-bacri/HMM_with_TMB/raw/main/functions/utils_linreg_extended.cpp "],["references.html", "References", " References "]]
