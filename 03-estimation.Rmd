# Parameter estimation techniques<br>for HMMs

In this section we recall basic concepts underlying parameter estimation for HMMs via direct numerical optimization of the likelihood. In terms of notation, we stay as close as possible to @zucchini, where a more detailed presentation is available.

## Basic notation and model setup

The conditional distribution densities can be called in `R` as well as in `C++` (provided that the `TMB` template is respected) via
```{r 2densities}
dpois(x = 1, lambda = 5) # Poisson
dnorm(x = 1, mean = 0, sd = 1) # Normal
```
Other distributions are available.

## The likelihood function of an HMM

The Poisson HMM negative log-likelihood function in `C++` can be written the following way using `TMB`'s template..
```{Rcpp 2poi_hmm.cpp, code=readLines("code/poi_hmm.cpp"), eval=FALSE}
```

The case where $m = 1$ doesn't involve a hidden state, and thus is a Poisson regression instead of a Poisson HMM.
Nonetheless, `TMB` also accelerates its estimation and may be useful to the reader.

## Forward algorithm and<br>backward algorithm {#forward-backward}
### Setup
#### Prepare the model
```{r 3prepare, results=FALSE}
library(TMB)
source("functions/utils.R")
load("data/fetal-lamb.RData")
TMB::compile("code/poi_hmm.cpp")
dyn.load(dynlib("code/poi_hmm"))
lamb_data <- lamb
m <- 2
TMB_data <- list(x = lamb_data, m = m)
lambda <- c(1, 3)
gamma <- matrix(c(0.8, 0.2,
                  0.2, 0.8), byrow = TRUE, nrow = m)
parameters <- pois.HMM.pn2pw(m, lambda, gamma)
obj_tmb <- MakeADFun(TMB_data, parameters,
                     DLL = "poi_hmm", silent = TRUE)
mod_tmb <- nlminb(start = obj_tmb$par, objective = obj_tmb$fn,
                  gradient = obj_tmb$gr, hessian = obj_tmb$he)
```

#### Define variables
Given an optimized `MakeADFun` object `obj`, we need to setup some variables to compute the probabilities detailed below.

```{r 3init-decoding}
# Retrieve the objects at ML value
adrep <- obj_tmb$report(obj_tmb$env$last.par.best)
delta <- adrep$delta
gamma <- adrep$gamma
emission_probs <- adrep$emission_probs
n <- adrep$n
m <- length(delta)
mllk <- adrep$mllk
```

Note that $\bs{\lambda}$ is not needed as we already have access to the emission probabilities.

#### Emission/output probabilities
Emission probabilities (also called output probabilities) are the conditional probabilities of the data given a state.

Using the notation in the article Section 3.1, emission probabilities are the conditional distributions $p_i(x) = \text{P}(X_t = x \vert C_t = i) = \frac{e^{-\lambda_i} \lambda_i^x}{x!}$.
We store all these in a data frame where each row and column represent a data and a state respectively.

More formally, we call
$\begin{pmatrix}
p_1(x_1)  & p_2(x_1) & \ldots & p_m(x_1)\\
p_1(x_2)  & p_2(x_2) & \ldots & p_m(x_2)\\
\vdots    &  \vdots  & \ddots & \vdots\\
p_1(x_n)  & p_2(x_n) & \ldots & p_m(x_n)\\
\end{pmatrix}$
the emission probability matrix.

We compute these probabilities in `C++` rather than in `R` because it is faster and not more complicated to write

```{Rcpp 1poi_hmm.cpp, code=readLines("code/poi_hmm.cpp")[32:46], eval=FALSE}
```

Nevertheless, we also need to compute them in `R` to [Forecast](#forecast)
```{r get.emission.probs}
```

### Log-forward probabilities {#log-forward}
We show here a way to compute the log of the forward probabilities, using a scaling scheme defined by @zucchini[p.66 and p.334].

```{r 3log-forward}
# Compute log-forward probabilities (scaling used)
lalpha <- matrix(NA, m, n)
foo <- delta * emission_probs[1, ]
sumfoo <- sum(foo)
lscale <- log(sumfoo)
foo <- foo / sumfoo
lalpha[, 1] <- log(foo) + lscale
for (i in 2:n) {
  foo <- foo %*% gamma * emission_probs[i, ]
  sumfoo <- sum(foo)
  lscale <- lscale + log(sumfoo)
  foo <- foo / sumfoo
  lalpha[, i] <- log(foo) + lscale
}
# lalpha contains n=240 columns, so we only display 5 for readability
lalpha[, 1:5]
```

### Log-backward probabilities {#log-backward}
The backward probabilities have been defined in the same section
@zucchini[p.67 and p.334].

```{r 3log-backward}
# Compute log-backwards probabilities (scaling used)
lbeta <- matrix(NA, m, n)
lbeta[, n] <- rep(0, m)
foo <- rep (1 / m, m)
lscale <- log(m)
for (i in (n - 1):1) {
  foo <- gamma %*% (emission_probs[i + 1, ] * foo)
  lbeta[, i] <- log(foo) + lscale
  sumfoo <- sum(foo)
  foo <- foo / sumfoo
  lscale <- lscale + log(sumfoo)
}
# lbeta contains n=240 columns, so we only display 4 for readability
lbeta[, 1:4]
```

We refer to the Section [State Inference](#state-inference) for an application of this code for state inference and forecasting.

## Reparameterization of the<br>likelihood function

### Utility functions in `TMB`

Defining the negative log-likelihood function requires transforming the working parameters into their natural form.
We define the function `gamma.w2n` to perform this transformation.

The function `stat.dist` handles computing the stationary distribution, while `delta.w2n` can be used if a non-stationary distribution is provided.

They are defined in *[functions/utils.cpp](functions/utils.cpp)*

```{Rcpp 2utils.cpp, code=readLines("functions/utils.cpp"), eval=FALSE}
```

Transforming the Poisson means into their natural form can be done simply with the `exp` function and doesn't require a dedicated function.

### Utility functions in `R`
While `TMB` requires functions to transform working parameters to their natural form, pre-processing in `R` requires the inverse transformation.

Functions to achieve this are available in @zucchini [p.51] and are displayed here with explaining comments for convenience.

```{r pois.HMM.pn2pw}
```

This can be broken down into sub-functions if necessary. For the $\bs{\Gamma}$ part, we introduce `gamma.n2w` below.
```{r gamma.n2w}
```

In the case where a non-stationary distribution is specified, transforming $\bs{\delta}$ is necessary. For this we use the `delta.n2w` function.
```{r delta.n2w}
```

When assuming a stationary distribution, computing it can be done via the following `stat.dist` function.
```{r stat.dist}
```

@zucchini[p.51] shows that calculating the stationary distribution can be achieved by solving the equation below for $\bs{\delta}$, where $\bs{I}_m$ is the $m*m$ identity matrix, $\bs{U}$ is a $m*m$ matrix of ones, and $\bs{1}$ is a row vector of ones.
\[
\bs{\delta}(\bs{I}_m - \bs{\Gamma} + \bs{U}) = \bs{1}
\]


#### Label switching {#label-switching}
As the model gets estimated each time, no order is imposed on the states by default.
If one needs to aggregate estimates, this can lead to the label switching problem, where states aren't ordered the same way in each model and are grouped incorrectly.
The issue can be relevant when comparing results of different optimizers, initial parameters, or classes of models. 

To address this, we reorder the states by ascending Poisson means after estimation.
Sorting the means is direct, however re-ordering the TPM is not as straightforward.
To do so, we take the permutations of the states given by the sorted Poisson means, and permute each row index and column index to its new value.
The function `pois.HMM.label.order` solves the issue and is presented below. 
```{r pois.HMM.label.order}
```

We will go through an example to better understand the process.
For readability, the TPM is filled with standard row and column indexes instead of probabilities.
```{r 2label-switch}
lambda <- c(3, 1, 2)
gamma <- matrix(c(11, 12, 13,
                  21, 22, 23,
                  31, 32, 33), byrow = TRUE, ncol = 3)
pois.HMM.label.order(m = 3, lambda, gamma)
```

State 1 has been relabeled state 3, state 3 became state 2, and state 2 became state 1.

Another way to address this issue is by parameterizing in terms of non-negative increments $\lambda_j - \lambda_{j-1}$ with $\lambda_0 \equiv 0$, as explained by @zucchini[ Section 7.1.1 p. 112].
However, @bulla[ Section 3.2 p. 7] shows this can impose optimization issues: "over all series, the simplest parameterization, i.e., the use of log-transformed state-dependent parameters, leads to the best results as regards the number of failures and the convergence to the global maximum".

These utility functions or subroutines are not complicated, but as you can see, they would cloud up your main code.
Therefore, we put them in functions we can call from our main program.

