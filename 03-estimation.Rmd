# Parameter estimation techniques for HMMs

In this section we recall basic concepts underlying parameter estimation for HMMs via direct numerical optimization of the likelihood. In terms of notation, we stay as close as possible to @zucchini, where a more detailed presentation is available.

## Basic notation and model setup

The conditional distribution densities can be called in `R` as well as in `C++` (provided that the `TMB` template is respected) via
```{r 2densities}
dpois(x = 1, lambda = 5) # Poisson
dnorm(x = 1, mean = 0, sd = 1) # Normal
```
Other distributions are available.

## The likelihood function of an HMM

The Poisson HMM negative log-likelihood function in `C++` can be written the following way using `TMB`'s template..
```{Rcpp 2poi_hmm.cpp, code=readLines("code/poi_hmm.cpp"), eval=FALSE}
```

The case where $m = 1$ doesn't involve a hidden state, and thus is a Poisson regression instead of a Poisson HMM.
Nonetheless, `TMB` also accelerates its estimation and may be useful to the reader.

## Forward algorithm and backward algorithm

We refer to the Section [State Inference](#state-inference).

## Reparameterization of the likelihood function

### Utility functions in `TMB`

Defining the negative log-likelihood function requires transforming the working parameters into their natural form.
We define the function `gamma.w2n` to perform this transformation.

The function `stat.dist` handles computing the stationary distribution, while `delta.w2n` can be used if a non-stationary distribution is provided.

They are defined in *[functions/utils.cpp](functions/utils.cpp)*

```{Rcpp 2utils.cpp, code=readLines("functions/utils.cpp"), eval=FALSE}
```

Transforming the Poisson means into their natural form can be done simply with the `exp` function and doesn't require a dedicated function.

### Utility functions in `R`
While `TMB` requires functions to transform working parameters to their natural form, pre-processing in `R` requires the inverse transformation.

Functions to achieve this are available in @zucchini [p.51] and are displayed here with explaining comments for convenience.

```{r pois.HMM.pn2pw}
```

This can be broken down into sub-functions if necessary. For the $\b{\Gamma}$ part, we introduce `gamma.n2w` below.
```{r gamma.n2w}
```

In the case where a non-stationary distribution is specified, transforming $\b{\delta}$ is necessary. For this we use the `delta.n2w` function.
```{r delta.n2w}
```

When assuming a stationary distribution, computing it can be done via the following `stat.dist` function.
```{r stat.dist}
```

@zucchini[p.51] shows that calculating the stationary distribution can be achieved by solving the equation below for $\b{\delta}$, where $\b{I}_m$ is the $m*m$ identity matrix, $\b{U}$ is a $m*m$ matrix of ones, and $\b{1}$ is a row vector of ones.
\[
\b{\delta}(\b{I}_m - \b{\Gamma} + \b{U}) = \b{1}
\]


#### Label switching {#label-switching}
As the model gets estimated each time, we do not impose by default an order for the states.
This can lead to the label switching problem, where states aren't ordered the same way in each model and are grouped incorrectly.
The issue can be relevant when comparing results of different optimizers, initial parameters, or classes of models. 

To address the problem, we reorder the states by ascending Poisson means.
Sorting the means is direct, however re-ordering the TPM is not as straightforward.
To do so, we take the permutations of the states given by the sorted Poisson means, and permute each row index and column index to its new value.
A function to achieve this is named `pois.HMM.label.order` and presented below. 
```{r pois.HMM.label.order}
```

We will go through an example to better understand the process.
For readability, the TPM is filled with standard row and column indexes instead of probabilities.
```{r 2label-switch}
lambda <- c(3, 1, 2)
gamma <- matrix(c(11, 12, 13,
                  21, 22, 23,
                  31, 32, 33), byrow = TRUE, ncol = 3)
pois.HMM.label.order(m = 3, lambda, gamma)
```

State 1 has been relabeled state 3, state 3 became state 2, and state 2 became state 1.

Another way to address this issue is by parameterizing in terms of non-negative increments $\lambda_j - \lambda_{j-1}$ with $\lambda_0 \equiv 0$, as explained by @zucchini[Section 7.1.1 p. 112].
However, @bulla [Section 3.2 p. 7] shows this can impose optimization issues: "over all series, the simplest parameterization, i.e., the use of log-transformed state-dependent parameters, leads to the best results as regards the number of failures and the convergence to the global maximum".

These utility functions or subroutines are not complicated, but as you can see, they would cloud up your main code.
Therefore, we put them in functions we can call from our main program.

